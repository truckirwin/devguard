[
    {
        "timestamp": "2025-05-07 16:13:17",
        "MODEL": "Time Begins",
        "INPUT": "",
        "COMMAND": "",
        "RESPONSE": "",
        "PARAMETERS": "Parameters not available."
    },
    {
        "timestamp": "2025-05-07 16:20:52",
        "MODEL": "Claude 35S",
        "COMMAND": "Read the doc in the INPUT.\n\nSome of the ideas below are not incorporated into the input yet.  Incorporate those ideas and details INTO the doc in the input.\n\nWrite your response with full markup for table formatting and text formatting.  \n\nStage 1: Project Initiation\nDefine Project Basics: User creates a new training project by entering core details:\nCourse title and code\nDescription and business need justification\nTarget audience and skill level requirements\nInstructional level (100-400)\nEstimated duration\nSelect AWS Services: User specifies which AWS services will be covered in the training\nSet Project Parameters: Version number, status tracking begins as \"in-progress\"\nData Storage: Project metadata is saved to DynamoDB in the TrainCraft_Projects table\nStage 2: Training Needs Analysis\nAudience Assessment: User defines detailed target audience characteristics\nKnowledge Gap Analysis: Document current vs. desired knowledge state\nBusiness Impact Measurement: Define how training success will be measured\nAI Assistance: May use Amazon Bedrock models to help generate audience profiles\nData Storage: Stage content saved to TrainCraft_StageContents with projectId reference\nStage 3: Design Workshop\nLearning Objectives: Define specific learning outcomes\nContent Structure: Outline modules, topics, and learning flow\nInteractive Elements: Plan hands-on activities, labs, assessments\nCollaboration: Multiple stakeholders may add comments via the Comments system\nReview Process: Design approval workflow using comments/status changes\nStage 4: Content Development\nCreate Course Materials: Develop detailed content for each module\nTechnical Writing: Document AWS service details, use cases, limitations\nAI Augmentation: Use Amazon Bedrock models to enhance content generation\nClaude 3.7 Sonnet for complex technical explanations\nNova Premier for use case narratives\nOther models for specific content tasks\nProgress Tracking: Content completeness tracked by stage/module\nStage 5: Lab Exercise Design\nTechnical Requirements: Define AWS resources needed for labs\nStep-by-Step Instructions: Create detailed lab procedures\nSuccess Criteria: Define completion requirements\nTesting: Validate lab exercises functionality\nResources: Attach supporting resources (code samples, templates) via Resources system\nStage 6: Quality Review\nTechnical Accuracy: Verify AWS service information is current and correct\nPedagogical Review: Ensure content meets learning objectives\nFormatting Standards: Check for consistency with AWS training templates\nComment System: Reviewers add feedback via Comments\nRevision Tracking: Changes logged with updatedAt timestamps\nStage 7: Final Edits\nAddress Review Feedback: Implement changes from Stage 6\nContent Polishing: Final language refinement and technical edits\nVisual Elements: Finalize diagrams, illustrations, presentations\nCross-references: Ensure consistency across all materials\nFinal Approval: Mark stage as completed when ready\nStage 8: Pre-Publication\nPackaging: Assemble all materials into deployment format\nAccess Controls: Set up appropriate permissions\nQuality Assurance: Final verification of all components\nRelease Notes: Document version and change information\nDeployment Plan: Schedule and logistics for rollout\nStage 9: Publication\nRelease: Deploy training content to final destination platforms\nAnnouncement: Notify stakeholders of availability\nAccess Management: Provide credentials to instructors/learners\nResource Tracking: Monitor usage metrics\nProject Status: Mark project as \"completed\" in system\nFinal Storage: All materials archived in connected AWS S3 storage\nThroughout this entire workflow:\nProgress Tracking: Each stage updates currentStage, completedAt timestamps\nCollaboration: Comments system allows for feedback at any stage\nResource Management: Training assets are attached and versioned\nAI Assistance: Amazon Bedrock integration provides AI support across stages\nData Persistence: All information stored in DynamoDB tables",
        "INPUT": "TrainCraft Technical Data Model and Workflow Specification\nDocument ID: TC-TECH-001\nVersion: 1.0\nLast Updated: 2025-05-07\nClassification: Technical Reference\n1. System Overview\nTrainCraft is an integrated AWS training content development platform with a 9-stage workflow for creating, managing, and publishing AWS training materials. This document provides a detailed technical specification of the data model, workflow stages, form structures, and field definitions.\n2. Database Schema\nThe database schema is implemented using a PostgreSQL-compatible structure through Drizzle ORM with the following core entities:\n2.1 Core Entities\nEntity\nDescription\nPrimary Keys\nRelations\nUsers\nSystem users\nid (serial)\nReferenced by multiple tables\nProjects\nTraining course projects\nid (serial)\nParent for all content entities\nStageContents\nContent for each workflow stage\nid (serial)\nBelongs to Project\nResources\nTraining materials and assets\nid (serial)\nBelongs to Project\nComments\nFeedback annotations\nid (serial)\nBelongs to Project, optionally to Stage\nAIMessages\nConversation history\nid (serial)\nBelongs to Project\n\n2.2 Workflow Entities\nEntity\nDescription\nPrimary Keys\nRelations\nWorkflowStations\nSub-components within stages\nid (serial)\nBelongs to StageContent\nStationContents\nData for specific stations\nid (serial)\nBelongs to WorkflowStation\nAISuggestions\nAI-generated content proposals\nid (serial)\nBelongs to WorkflowStation\nDocumentVersions\nVersion-controlled documents\nid (serial)\nBelongs to WorkflowStation\nQualityChecks\nQA verification records\nid (serial)\nBelongs to WorkflowStation\n\n2.3 Stage-Specific Entities\nEntity\nDescription\nPrimary Keys\nRelations\nCourseBasicInfo\nStage 1 core course metadata\nid (serial)\nBelongs to Project\nTargetAudienceInfo\nStage 1 audience definitions\nid (serial)\nBelongs to Project\nMarketAnalysis\nStage 2 market research data\nid (serial)\nBelongs to Project\nLearningObjectives\nStage 2 educational goals\nid (serial)\nBelongs to Project\nAssessmentStrategy\nStage 2 evaluation framework\nid (serial)\nBelongs to Project\n\n3. Workflow Stages\nThe application implements a standardized 9-stage workflow for course development:\nID\nCode\nName\nPurpose\nStatus Options\n1\nCOURSE\nCourse Information\nCapture foundational course information and business requirements\nnot-started, in-progress, completed, blocked\n2\nPRD\nProduct Requirements Document\nDefine detailed requirements and specifications\nnot-started, in-progress, completed, blocked\n3\nDESIGN\nDesign Workshop\nCreate detailed course design structure\nnot-started, in-progress, completed, blocked\n4\nDDD\nDetailed Design Document\nDefine technical specifications and architecture\nnot-started, in-progress, completed, blocked\n5\nCONTENT\nContent Development\nCreate and organize course content\nnot-started, in-progress, completed, blocked\n6\nLABS\nLab Design\nDesign interactive lab exercises\nnot-started, in-progress, completed, blocked\n7\nEDIT\nVisual Design\nCreate visual assets and apply branding\nnot-started, in-progress, completed, blocked\n8\nPODIUM\nReview & Refinement\nConduct comprehensive review and QA\nnot-started, in-progress, completed, blocked\n9\nPUBLISH\nPublishing\nPrepare and publish final materials\nnot-started, in-progress, completed, blocked\n\n4. Stage 1: Course Information - Technical Details\n4.1 Basic Course Information Station\nFrontend Component: BasicCourseInfoStation.tsx\nDatabase Tables: courseBasicInfo, stageContents\n4.1.1 Form Structure\nField Name\nField Type\nDatabase Field\nValidation Rules\nDefault Value\ncourseTitle\nText Input\ncourseTitle (VARCHAR)\n5-100 chars, required\n\"\"\ncourseCode\nText Input\ncourseCode (VARCHAR)\n3-20 chars, required\n\"\"\ncourseVersion\nText Input\ncourseVersion (VARCHAR)\n1-20 chars, required\n\"1.0\"\ncourseDuration\nNumber Input\ncourseDuration (INTEGER)\n1-200, required\n8\ncourseFormat\nSelect\ncourseFormat (VARCHAR)\nRequired\n\"instructor-led\"\ncourseDescription\nTextarea\ncourseDescription (TEXT)\n10-1000 chars, required\n\"\"\n\n4.1.2 API Endpoints\nMethod\nEndpoint\nPurpose\nRequired Fields\nGET\n/api/projects/${projectId}/course-info\nFetch course info\nNone\nPOST\n/api/course-info\nCreate new course info\nAll fields, projectId\nPUT\n/api/course-info/${id}\nUpdate existing course info\nAll fields\n\n4.2 Target Audience Station\nFrontend Component: TargetAudienceStation.tsx\nDatabase Tables: targetAudienceInfo, stageContents\n4.2.1 Form Structure\nField Name\nField Type\nDatabase Field\nValidation Rules\nDefault Value\nroleProfiles\nMulti-select\nroleProfiles (JSONB)\nMin 1 selection, required\n[]\nrequiredSkills\nMulti-select\nrequiredSkills (JSONB)\nMin 1 selection, required\n[]\nskillLevel\nSelect\nskillLevel (VARCHAR)\nRequired\n\"intermediate\"\nintendedOutcomes\nDynamic Input List\nintendedOutcomes (JSONB)\nMin 1 input, required\n[\"\"]\n\n4.2.2 API Endpoints\nMethod\nEndpoint\nPurpose\nRequired Fields\nGET\n/api/projects/${projectId}/target-audience\nFetch audience info\nNone\nPOST\n/api/target-audience\nCreate new audience info\nAll fields, projectId\nPUT\n/api/target-audience/${id}\nUpdate existing audience info\nAll fields\nPOST\n/api/ai/generate-learning-objectives\nGenerate AI outcomes\ntitle, awsServices, audience, level\n\n5. Stage 2: Product Requirements Document - Technical Details\n5.1 Market Analysis Station\nFrontend Component: Not fully implemented in UI\nDatabase Tables: marketAnalysis, stageContents\n5.1.1 Data Structure\nField Name\nDatabase Field\nField Type\nValidation Rules\nmarketNeeds\nmarketNeeds (TEXT)\nTextarea\nOptional\ncompetitiveAnalysis\ncompetitiveAnalysis (JSONB)\nStructured Data\nOptional\ntargetMarket\ntargetMarket (TEXT)\nTextarea\nOptional\nbusinessJustification\nbusinessJustification (TEXT)\nTextarea\nOptional\n\n5.2 Learning Objectives Station\nFrontend Component: Not fully implemented in UI\nDatabase Tables: learningObjectives, stageContents\n5.2.1 Data Structure\nField Name\nDatabase Field\nField Type\nValidation Rules\nobjectives\nobjectives (JSONB)\nArray of structured objects\nOptional\n\n5.3 Assessment Strategy Station\nFrontend Component: Not fully implemented in UI\nDatabase Tables: assessmentStrategy, stageContents\n5.3.1 Data Structure\nField Name\nDatabase Field\nField Type\nValidation Rules\nassessmentTypes\nassessmentTypes (JSONB)\nArray of types\nOptional\nevaluationCriteria\nevaluationCriteria (JSONB)\nStructured criteria\nOptional\n\n6. Stage 3-9: Implementation Status\nStages 3-9 are defined in the database schema but have not been fully implemented in the frontend UI. The core data structure for these stages follows the stageContents table with stage-specific JSON content.\nStage\nDatabase Storage\nImplementation Status\nDesign Workshop\nstageContents.content (JSON)\nPlaceholder UI only\nDetailed Design Document\nstageContents.content (JSON)\nPlaceholder UI only\nContent Development\nstageContents.content (JSON)\nPlaceholder UI only\nLab Design\nstageContents.content (JSON)\nPlaceholder UI only\nVisual Design\nstageContents.content (JSON)\nPlaceholder UI only\nReview & Refinement\nstageContents.content (JSON)\nPlaceholder UI only\nPublishing\nstageContents.content (JSON)\nPlaceholder UI only\n\n7. Document Generation and Versioning\n7.1 Document Version Control\nDatabase Table: documentVersions\nField\nType\nPurpose\nid\nserial\nPrimary key\nstationId\ninteger\nReference to station\ndocumentType\ntext\nType identifier (PRD, DDD, etc.)\nversionNumber\ninteger\nSequential version\ndocumentContent\njsonb\nFull document content\nstatus\ntext\nDocument status (draft, in_review, approved, published)\nreviewedBy, approvedBy\ninteger\nUser references\nreviewedAt, approvedAt\ntimestamp\nAction timestamps\n\n7.2 Quality Assurance Process\nDatabase Table: qualityChecks\nField\nType\nPurpose\nid\nserial\nPrimary key\nstationId\ninteger\nReference to station\ncheckType\ntext\nCheck category\nstatus\ntext\nCheck status\nfindings\njsonb\nDetailed findings\ncheckedBy\ninteger\nUser reference\ncheckedAt\ntimestamp\nAction timestamp\n\n8. AI Integration Technical Specification\n8.1 AI Message Tracking\nDatabase Table: aiMessages\nField\nType\nPurpose\nid\nserial\nPrimary key\nprojectId\ninteger\nReference to project\nsender\ntext\nOrigin (user/assistant)\ncontent\ntext\nMessage content\ntimestamp\ntimestamp\nCreation time\n\n8.2 AI Suggestion Management\nDatabase Table: aiSuggestions\nField\nType\nPurpose\nid\nserial\nPrimary key\nstationId\ninteger\nReference to station\nsuggestionType\ntext\nType of suggestion\nsuggestionContent\ntext\nProposed content\nstatus\ntext\nSuggestion status\nappliedContent\ntext\nModified application\ncreatedAt, updatedAt\ntimestamp\nTracking timestamps\n\n9. User Interface Technical Components\n9.1 Dashboard Page\nComponent: Dashboard.tsx\nPrimary Features:\nProjectSummary: Statistical overview cards\nProjectsList: Tabular project listing with filtering\nDataSourcesList: Resource management interface\nAwsUnifiedStatusCard: AWS service connection status\n9.2 Project Page\nComponent: ProjectPage.tsx\nPrimary Features:\nWorkflowProgress: Visual workflow stage navigation\nStage-specific form components (9 total)\nForm state management with formRefs system\nStage approval and advancement logic\n9.3 Workflow Page\nComponent: WorkflowPage.tsx\nPrimary Features:\nStageHeader: Stage navigation and project identification\nTab navigation between \"Workflow Stations\" and \"Stage Overview\"\nStation-based content forms for active stage\nActionButtons: Save, Build (AI), and Approve functions\n9.4 AWS Settings Page\nComponent: AwsSettingsPage.tsx\nPrimary Features:\nAWS connection status monitoring\nAwsCredentialsManager: Credential management interface\nService status indicators for each AWS service\nManual synchronization controls\n10. Technical Constants and Enumerations\n10.1 Workflow Stage Definitions\nWORKFLOW_STAGES = [\n  { id: 1, name: \"COURSE\", displayName: \"Course Information\", ... },\n  { id: 2, name: \"PRD\", displayName: \"Product Requirements Document\", ... },\n  // Stages 3-9 following same pattern\n]\n10.2 Status Enumerations\nSTAGE_STATUS = {\n  NOT_STARTED: \"not-started\",\n  IN_PROGRESS: \"in-progress\",\n  COMPLETED: \"completed\",\n  BLOCKED: \"blocked\"\n}\n \nPROJECT_STATUS = {\n  NOT_STARTED: \"not-started\",\n  IN_PROGRESS: \"in-progress\",\n  COMPLETED: \"completed\",\n  PAUSED: \"paused\"\n}\n10.3 Domain-Specific Enumerations\n// Instructional levels\nINSTRUCTIONAL_LEVELS = [\n  { value: \"100\", label: \"100 - Foundational\" },\n  { value: \"200\", label: \"200 - Intermediate\" },\n  { value: \"300\", label: \"300 - Advanced\" },\n  { value: \"400\", label: \"400 - Expert\" }\n]\n \n// Audience skill levels\nAUDIENCE_SKILL_LEVELS = [\n  { value: \"beginner\", label: \"Beginner\" },\n  { value: \"intermediate\", label: \"Intermediate\" },\n  { value: \"advanced\", label: \"Advanced\" },\n  { value: \"expert\", label: \"Expert\" }\n]\n \n// Target audience roles\nTARGET_AUDIENCE_ROLES = [\n  { value: \"solutions-architect\", label: \"Solutions Architect\" },\n  { value: \"developer\", label: \"Developer\" },\n  // etc.\n]\n \n// Course durations\nCOURSE_DURATIONS = [\n  { value: \"4-hours\", label: \"4 Hours\" },\n  { value: \"8-hours\", label: \"8 Hours (1 Day)\" },\n  // etc.\n]\n \n// AWS Services (categorized)\nAWS_SERVICES = {\n  COMPUTE: [\n    { value: \"ec2\", label: \"Amazon EC2\" },\n    // etc.\n  ],\n  // Other categories with services\n}\n11. Data Migration Considerations\nThe application has transitioned from PostgreSQL to exclusively using DynamoDB for data storage. The AWS credential storage has similarly moved from database to file-based storage (.aws-credentials.json).\n11.1 DynamoDB Tables\nThe following DynamoDB tables replace the PostgreSQL schema:\nTable Name\nPartition Key\nSort Key\nGlobal Secondary Indexes\nUsers\nid\n-\nusername-index\nProjects\nid\n-\nuserId-index, status-index\nStageContents\nid\n-\nprojectId-index, stageNumber-index\nResources\nid\n-\nprojectId-index, type-index\nComments\nid\n-\nprojectId-index, stageId-index\n\n12. Technical Implementation Notes\nFrontend implements reactive forms with react-hook-form and Zod validation\nAPI requests managed through TanStack Query with custom hooks\nAWS connectivity handled through file-based credential storage\nForm submissions handled through standardized mutation patterns\nAI assistance integrated through dedicated API endpoints for content generation\nStage approval process updates project.currentStage and stage.status fields\nMock implementations used for stages 2-9 until fully developed\n\nThis technical document represents the current implementation state of the TrainCraft application as of May 7, 2025, and may be updated as development progresses.",
        "RESPONSE": "Here is the updated document with the additional ideas and details incorporated, using full markup for table formatting and text formatting:\n\n<h1>TrainCraft Technical Data Model and Workflow Specification</h1>\n\n<table>\n  <tr>\n    <td><strong>Document ID:</strong></td>\n    <td>TC-TECH-001</td>\n  </tr>\n  <tr>\n    <td><strong>Version:</strong></td>\n    <td>1.1</td>\n  </tr>\n  <tr>\n    <td><strong>Last Updated:</strong></td>\n    <td>2025-05-07</td>\n  </tr>\n  <tr>\n    <td><strong>Classification:</strong></td>\n    <td>Technical Reference</td>\n  </tr>\n</table>\n\n<h2>1. System Overview</h2>\n\n<p>TrainCraft is an integrated AWS training content development platform with a 9-stage workflow for creating, managing, and publishing AWS training materials. This document provides a detailed technical specification of the data model, workflow stages, form structures, and field definitions.</p>\n\n<h2>2. Database Schema</h2>\n\n<p>The database schema is implemented using Amazon DynamoDB with the following core entities:</p>\n\n<h3>2.1 Core Entities</h3>\n\n<table>\n  <tr>\n    <th>Entity</th>\n    <th>Description</th>\n    <th>Partition Key</th>\n    <th>Sort Key</th>\n    <th>Global Secondary Indexes</th>\n  </tr>\n  <tr>\n    <td>Users</td>\n    <td>System users</td>\n    <td>id</td>\n    <td>-</td>\n    <td>username-index</td>\n  </tr>\n  <tr>\n    <td>Projects</td>\n    <td>Training course projects</td>\n    <td>id</td>\n    <td>-</td>\n    <td>userId-index, status-index</td>\n  </tr>\n  <tr>\n    <td>StageContents</td>\n    <td>Content for each workflow stage</td>\n    <td>id</td>\n    <td>-</td>\n    <td>projectId-index, stageNumber-index</td>\n  </tr>\n  <tr>\n    <td>Resources</td>\n    <td>Training materials and assets</td>\n    <td>id</td>\n    <td>-</td>\n    <td>projectId-index, type-index</td>\n  </tr>\n  <tr>\n    <td>Comments</td>\n    <td>Feedback annotations</td>\n    <td>id</td>\n    <td>-</td>\n    <td>projectId-index, stageId-index</td>\n  </tr>\n  <tr>\n    <td>AIMessages</td>\n    <td>Conversation history</td>\n    <td>id</td>\n    <td>-</td>\n    <td>projectId-index</td>\n  </tr>\n</table>\n\n<h3>2.2 Workflow Entities</h3>\n\n<table>\n  <tr>\n    <th>Entity</th>\n    <th>Description</th>\n    <th>Partition Key</th>\n    <th>Sort Key</th>\n    <th>Global Secondary Indexes</th>\n  </tr>\n  <tr>\n    <td>WorkflowStations</td>\n    <td>Sub-components within stages</td>\n    <td>id</td>\n    <td>-</td>\n    <td>stageContentId-index</td>\n  </tr>\n  <tr>\n    <td>StationContents</td>\n    <td>Data for specific stations</td>\n    <td>id</td>\n    <td>-</td>\n    <td>workflowStationId-index</td>\n  </tr>\n  <tr>\n    <td>AISuggestions</td>\n    <td>AI-generated content proposals</td>\n    <td>id</td>\n    <td>-</td>\n    <td>workflowStationId-index</td>\n  </tr>\n  <tr>\n    <td>DocumentVersions</td>\n    <td>Version-controlled documents</td>\n    <td>id</td>\n    <td>-</td>\n    <td>workflowStationId-index</td>\n  </tr>\n  <tr>\n    <td>QualityChecks</td>\n    <td>QA verification records</td>\n    <td>id</td>\n    <td>-</td>\n    <td>workflowStationId-index</td>\n  </tr>\n</table>\n\n<h2>3. Workflow Stages</h2>\n\n<p>The application implements a standardized 9-stage workflow for course development:</p>\n\n<table>\n  <tr>\n    <th>ID</th>\n    <th>Code</th>\n    <th>Name</th>\n    <th>Purpose</th>\n    <th>Status Options</th>\n  </tr>\n  <tr>\n    <td>1</td>\n    <td>COURSE</td>\n    <td>Course Information</td>\n    <td>Capture foundational course information and business requirements</td>\n    <td>not-started, in-progress, completed, blocked</td>\n  </tr>\n  <tr>\n    <td>2</td>\n    <td>PRD</td>\n    <td>Product Requirements Document</td>\n    <td>Define detailed requirements and specifications</td>\n    <td>not-started, in-progress, completed, blocked</td>\n  </tr>\n  <tr>\n    <td>3</td>\n    <td>DESIGN</td>\n    <td>Design Workshop</td>\n    <td>Create detailed course design structure</td>\n    <td>not-started, in-progress, completed, blocked</td>\n  </tr>\n  <tr>\n    <td>4</td>\n    <td>DDD</td>\n    <td>Detailed Design Document</td>\n    <td>Define technical specifications and architecture</td>\n    <td>not-started, in-progress, completed, blocked</td>\n  </tr>\n  <tr>\n    <td>5</td>\n    <td>CONTENT</td>\n    <td>Content Development</td>\n    <td>Create and organize course content</td>\n    <td>not-started, in-progress, completed, blocked</td>\n  </tr>\n  <tr>\n    <td>6</td>\n    <td>LABS</td>\n    <td>Lab Design</td>\n    <td>Design interactive lab exercises</td>\n    <td>not-started, in-progress, completed, blocked</td>\n  </tr>\n  <tr>\n    <td>7</td>\n    <td>EDIT</td>\n    <td>Visual Design</td>\n    <td>Create visual assets and apply branding</td>\n    <td>not-started, in-progress, completed, blocked</td>\n  </tr>\n  <tr>\n    <td>8</td>\n    <td>PODIUM</td>\n    <td>Review & Refinement</td>\n    <td>Conduct comprehensive review and QA</td>\n    <td>not-started, in-progress, completed, blocked</td>\n  </tr>\n  <tr>\n    <td>9</td>\n    <td>PUBLISH</td>\n    <td>Publishing</td>\n    <td>Prepare and publish final materials</td>\n    <td>not-started, in-progress, completed, blocked</td>\n  </tr>\n</table>\n\n<h2>4. Stage 1: Course Information - Technical Details</h2>\n\n<h3>4.1 Basic Course Information Station</h3>\n\n<p><strong>Frontend Component:</strong> Basic",
        "PARAMETERS": {
            "temp": 0.5,
            "topP": 1.0,
            "topK": 250,
            "maxT": 2000,
            "stopSeq": ""
        }
    },
    {
        "timestamp": "2025-05-07 16:25:26",
        "MODEL": "Claude 3S",
        "COMMAND": "Read the doc in the INPUT.\n\nSome of the ideas below are not incorporated into the input yet.  Incorporate those ideas and details INTO the doc in the input.\n\nWrite your response with full markdown for table formatting and text formatting.  \n\nStage 1: Project Initiation\nDefine Project Basics: User creates a new training project by entering core details:\nCourse title and code\nDescription and business need justification\nTarget audience and skill level requirements\nInstructional level (100-400)\nEstimated duration\nSelect AWS Services: User specifies which AWS services will be covered in the training\nSet Project Parameters: Version number, status tracking begins as \"in-progress\"\nData Storage: Project metadata is saved to DynamoDB in the TrainCraft_Projects table\nStage 2: Training Needs Analysis\nAudience Assessment: User defines detailed target audience characteristics\nKnowledge Gap Analysis: Document current vs. desired knowledge state\nBusiness Impact Measurement: Define how training success will be measured\nAI Assistance: May use Amazon Bedrock models to help generate audience profiles\nData Storage: Stage content saved to TrainCraft_StageContents with projectId reference\nStage 3: Design Workshop\nLearning Objectives: Define specific learning outcomes\nContent Structure: Outline modules, topics, and learning flow\nInteractive Elements: Plan hands-on activities, labs, assessments\nCollaboration: Multiple stakeholders may add comments via the Comments system\nReview Process: Design approval workflow using comments/status changes\nStage 4: Content Development\nCreate Course Materials: Develop detailed content for each module\nTechnical Writing: Document AWS service details, use cases, limitations\nAI Augmentation: Use Amazon Bedrock models to enhance content generation\nClaude 3.7 Sonnet for complex technical explanations\nNova Premier for use case narratives\nOther models for specific content tasks\nProgress Tracking: Content completeness tracked by stage/module\nStage 5: Lab Exercise Design\nTechnical Requirements: Define AWS resources needed for labs\nStep-by-Step Instructions: Create detailed lab procedures\nSuccess Criteria: Define completion requirements\nTesting: Validate lab exercises functionality\nResources: Attach supporting resources (code samples, templates) via Resources system\nStage 6: Quality Review\nTechnical Accuracy: Verify AWS service information is current and correct\nPedagogical Review: Ensure content meets learning objectives\nFormatting Standards: Check for consistency with AWS training templates\nComment System: Reviewers add feedback via Comments\nRevision Tracking: Changes logged with updatedAt timestamps\nStage 7: Final Edits\nAddress Review Feedback: Implement changes from Stage 6\nContent Polishing: Final language refinement and technical edits\nVisual Elements: Finalize diagrams, illustrations, presentations\nCross-references: Ensure consistency across all materials\nFinal Approval: Mark stage as completed when ready\nStage 8: Pre-Publication\nPackaging: Assemble all materials into deployment format\nAccess Controls: Set up appropriate permissions\nQuality Assurance: Final verification of all components\nRelease Notes: Document version and change information\nDeployment Plan: Schedule and logistics for rollout\nStage 9: Publication\nRelease: Deploy training content to final destination platforms\nAnnouncement: Notify stakeholders of availability\nAccess Management: Provide credentials to instructors/learners\nResource Tracking: Monitor usage metrics\nProject Status: Mark project as \"completed\" in system\nFinal Storage: All materials archived in connected AWS S3 storage\nThroughout this entire workflow:\nProgress Tracking: Each stage updates currentStage, completedAt timestamps\nCollaboration: Comments system allows for feedback at any stage\nResource Management: Training assets are attached and versioned\nAI Assistance: Amazon Bedrock integration provides AI support across stages\nData Persistence: All information stored in DynamoDB tables",
        "INPUT": "TrainCraft Technical Data Model and Workflow Specification\nDocument ID: TC-TECH-001\nVersion: 1.0\nLast Updated: 2025-05-07\nClassification: Technical Reference\n1. System Overview\nTrainCraft is an integrated AWS training content development platform with a 9-stage workflow for creating, managing, and publishing AWS training materials. This document provides a detailed technical specification of the data model, workflow stages, form structures, and field definitions.\n2. Database Schema\nThe database schema is implemented using a PostgreSQL-compatible structure through Drizzle ORM with the following core entities:\n2.1 Core Entities\nEntity\nDescription\nPrimary Keys\nRelations\nUsers\nSystem users\nid (serial)\nReferenced by multiple tables\nProjects\nTraining course projects\nid (serial)\nParent for all content entities\nStageContents\nContent for each workflow stage\nid (serial)\nBelongs to Project\nResources\nTraining materials and assets\nid (serial)\nBelongs to Project\nComments\nFeedback annotations\nid (serial)\nBelongs to Project, optionally to Stage\nAIMessages\nConversation history\nid (serial)\nBelongs to Project\n\n2.2 Workflow Entities\nEntity\nDescription\nPrimary Keys\nRelations\nWorkflowStations\nSub-components within stages\nid (serial)\nBelongs to StageContent\nStationContents\nData for specific stations\nid (serial)\nBelongs to WorkflowStation\nAISuggestions\nAI-generated content proposals\nid (serial)\nBelongs to WorkflowStation\nDocumentVersions\nVersion-controlled documents\nid (serial)\nBelongs to WorkflowStation\nQualityChecks\nQA verification records\nid (serial)\nBelongs to WorkflowStation\n\n2.3 Stage-Specific Entities\nEntity\nDescription\nPrimary Keys\nRelations\nCourseBasicInfo\nStage 1 core course metadata\nid (serial)\nBelongs to Project\nTargetAudienceInfo\nStage 1 audience definitions\nid (serial)\nBelongs to Project\nMarketAnalysis\nStage 2 market research data\nid (serial)\nBelongs to Project\nLearningObjectives\nStage 2 educational goals\nid (serial)\nBelongs to Project\nAssessmentStrategy\nStage 2 evaluation framework\nid (serial)\nBelongs to Project\n\n3. Workflow Stages\nThe application implements a standardized 9-stage workflow for course development:\nID\nCode\nName\nPurpose\nStatus Options\n1\nCOURSE\nCourse Information\nCapture foundational course information and business requirements\nnot-started, in-progress, completed, blocked\n2\nPRD\nProduct Requirements Document\nDefine detailed requirements and specifications\nnot-started, in-progress, completed, blocked\n3\nDESIGN\nDesign Workshop\nCreate detailed course design structure\nnot-started, in-progress, completed, blocked\n4\nDDD\nDetailed Design Document\nDefine technical specifications and architecture\nnot-started, in-progress, completed, blocked\n5\nCONTENT\nContent Development\nCreate and organize course content\nnot-started, in-progress, completed, blocked\n6\nLABS\nLab Design\nDesign interactive lab exercises\nnot-started, in-progress, completed, blocked\n7\nEDIT\nVisual Design\nCreate visual assets and apply branding\nnot-started, in-progress, completed, blocked\n8\nPODIUM\nReview & Refinement\nConduct comprehensive review and QA\nnot-started, in-progress, completed, blocked\n9\nPUBLISH\nPublishing\nPrepare and publish final materials\nnot-started, in-progress, completed, blocked\n\n4. Stage 1: Course Information - Technical Details\n4.1 Basic Course Information Station\nFrontend Component: BasicCourseInfoStation.tsx\nDatabase Tables: courseBasicInfo, stageContents\n4.1.1 Form Structure\nField Name\nField Type\nDatabase Field\nValidation Rules\nDefault Value\ncourseTitle\nText Input\ncourseTitle (VARCHAR)\n5-100 chars, required\n\"\"\ncourseCode\nText Input\ncourseCode (VARCHAR)\n3-20 chars, required\n\"\"\ncourseVersion\nText Input\ncourseVersion (VARCHAR)\n1-20 chars, required\n\"1.0\"\ncourseDuration\nNumber Input\ncourseDuration (INTEGER)\n1-200, required\n8\ncourseFormat\nSelect\ncourseFormat (VARCHAR)\nRequired\n\"instructor-led\"\ncourseDescription\nTextarea\ncourseDescription (TEXT)\n10-1000 chars, required\n\"\"\n\n4.1.2 API Endpoints\nMethod\nEndpoint\nPurpose\nRequired Fields\nGET\n/api/projects/${projectId}/course-info\nFetch course info\nNone\nPOST\n/api/course-info\nCreate new course info\nAll fields, projectId\nPUT\n/api/course-info/${id}\nUpdate existing course info\nAll fields\n\n4.2 Target Audience Station\nFrontend Component: TargetAudienceStation.tsx\nDatabase Tables: targetAudienceInfo, stageContents\n4.2.1 Form Structure\nField Name\nField Type\nDatabase Field\nValidation Rules\nDefault Value\nroleProfiles\nMulti-select\nroleProfiles (JSONB)\nMin 1 selection, required\n[]\nrequiredSkills\nMulti-select\nrequiredSkills (JSONB)\nMin 1 selection, required\n[]\nskillLevel\nSelect\nskillLevel (VARCHAR)\nRequired\n\"intermediate\"\nintendedOutcomes\nDynamic Input List\nintendedOutcomes (JSONB)\nMin 1 input, required\n[\"\"]\n\n4.2.2 API Endpoints\nMethod\nEndpoint\nPurpose\nRequired Fields\nGET\n/api/projects/${projectId}/target-audience\nFetch audience info\nNone\nPOST\n/api/target-audience\nCreate new audience info\nAll fields, projectId\nPUT\n/api/target-audience/${id}\nUpdate existing audience info\nAll fields\nPOST\n/api/ai/generate-learning-objectives\nGenerate AI outcomes\ntitle, awsServices, audience, level\n\n5. Stage 2: Product Requirements Document - Technical Details\n5.1 Market Analysis Station\nFrontend Component: Not fully implemented in UI\nDatabase Tables: marketAnalysis, stageContents\n5.1.1 Data Structure\nField Name\nDatabase Field\nField Type\nValidation Rules\nmarketNeeds\nmarketNeeds (TEXT)\nTextarea\nOptional\ncompetitiveAnalysis\ncompetitiveAnalysis (JSONB)\nStructured Data\nOptional\ntargetMarket\ntargetMarket (TEXT)\nTextarea\nOptional\nbusinessJustification\nbusinessJustification (TEXT)\nTextarea\nOptional\n\n5.2 Learning Objectives Station\nFrontend Component: Not fully implemented in UI\nDatabase Tables: learningObjectives, stageContents\n5.2.1 Data Structure\nField Name\nDatabase Field\nField Type\nValidation Rules\nobjectives\nobjectives (JSONB)\nArray of structured objects\nOptional\n\n5.3 Assessment Strategy Station\nFrontend Component: Not fully implemented in UI\nDatabase Tables: assessmentStrategy, stageContents\n5.3.1 Data Structure\nField Name\nDatabase Field\nField Type\nValidation Rules\nassessmentTypes\nassessmentTypes (JSONB)\nArray of types\nOptional\nevaluationCriteria\nevaluationCriteria (JSONB)\nStructured criteria\nOptional\n\n6. Stage 3-9: Implementation Status\nStages 3-9 are defined in the database schema but have not been fully implemented in the frontend UI. The core data structure for these stages follows the stageContents table with stage-specific JSON content.\nStage\nDatabase Storage\nImplementation Status\nDesign Workshop\nstageContents.content (JSON)\nPlaceholder UI only\nDetailed Design Document\nstageContents.content (JSON)\nPlaceholder UI only\nContent Development\nstageContents.content (JSON)\nPlaceholder UI only\nLab Design\nstageContents.content (JSON)\nPlaceholder UI only\nVisual Design\nstageContents.content (JSON)\nPlaceholder UI only\nReview & Refinement\nstageContents.content (JSON)\nPlaceholder UI only\nPublishing\nstageContents.content (JSON)\nPlaceholder UI only\n\n7. Document Generation and Versioning\n7.1 Document Version Control\nDatabase Table: documentVersions\nField\nType\nPurpose\nid\nserial\nPrimary key\nstationId\ninteger\nReference to station\ndocumentType\ntext\nType identifier (PRD, DDD, etc.)\nversionNumber\ninteger\nSequential version\ndocumentContent\njsonb\nFull document content\nstatus\ntext\nDocument status (draft, in_review, approved, published)\nreviewedBy, approvedBy\ninteger\nUser references\nreviewedAt, approvedAt\ntimestamp\nAction timestamps\n\n7.2 Quality Assurance Process\nDatabase Table: qualityChecks\nField\nType\nPurpose\nid\nserial\nPrimary key\nstationId\ninteger\nReference to station\ncheckType\ntext\nCheck category\nstatus\ntext\nCheck status\nfindings\njsonb\nDetailed findings\ncheckedBy\ninteger\nUser reference\ncheckedAt\ntimestamp\nAction timestamp\n\n8. AI Integration Technical Specification\n8.1 AI Message Tracking\nDatabase Table: aiMessages\nField\nType\nPurpose\nid\nserial\nPrimary key\nprojectId\ninteger\nReference to project\nsender\ntext\nOrigin (user/assistant)\ncontent\ntext\nMessage content\ntimestamp\ntimestamp\nCreation time\n\n8.2 AI Suggestion Management\nDatabase Table: aiSuggestions\nField\nType\nPurpose\nid\nserial\nPrimary key\nstationId\ninteger\nReference to station\nsuggestionType\ntext\nType of suggestion\nsuggestionContent\ntext\nProposed content\nstatus\ntext\nSuggestion status\nappliedContent\ntext\nModified application\ncreatedAt, updatedAt\ntimestamp\nTracking timestamps\n\n9. User Interface Technical Components\n9.1 Dashboard Page\nComponent: Dashboard.tsx\nPrimary Features:\nProjectSummary: Statistical overview cards\nProjectsList: Tabular project listing with filtering\nDataSourcesList: Resource management interface\nAwsUnifiedStatusCard: AWS service connection status\n9.2 Project Page\nComponent: ProjectPage.tsx\nPrimary Features:\nWorkflowProgress: Visual workflow stage navigation\nStage-specific form components (9 total)\nForm state management with formRefs system\nStage approval and advancement logic\n9.3 Workflow Page\nComponent: WorkflowPage.tsx\nPrimary Features:\nStageHeader: Stage navigation and project identification\nTab navigation between \"Workflow Stations\" and \"Stage Overview\"\nStation-based content forms for active stage\nActionButtons: Save, Build (AI), and Approve functions\n9.4 AWS Settings Page\nComponent: AwsSettingsPage.tsx\nPrimary Features:\nAWS connection status monitoring\nAwsCredentialsManager: Credential management interface\nService status indicators for each AWS service\nManual synchronization controls\n10. Technical Constants and Enumerations\n10.1 Workflow Stage Definitions\nWORKFLOW_STAGES = [\n  { id: 1, name: \"COURSE\", displayName: \"Course Information\", ... },\n  { id: 2, name: \"PRD\", displayName: \"Product Requirements Document\", ... },\n  // Stages 3-9 following same pattern\n]\n10.2 Status Enumerations\nSTAGE_STATUS = {\n  NOT_STARTED: \"not-started\",\n  IN_PROGRESS: \"in-progress\",\n  COMPLETED: \"completed\",\n  BLOCKED: \"blocked\"\n}\n \nPROJECT_STATUS = {\n  NOT_STARTED: \"not-started\",\n  IN_PROGRESS: \"in-progress\",\n  COMPLETED: \"completed\",\n  PAUSED: \"paused\"\n}\n10.3 Domain-Specific Enumerations\n// Instructional levels\nINSTRUCTIONAL_LEVELS = [\n  { value: \"100\", label: \"100 - Foundational\" },\n  { value: \"200\", label: \"200 - Intermediate\" },\n  { value: \"300\", label: \"300 - Advanced\" },\n  { value: \"400\", label: \"400 - Expert\" }\n]\n \n// Audience skill levels\nAUDIENCE_SKILL_LEVELS = [\n  { value: \"beginner\", label: \"Beginner\" },\n  { value: \"intermediate\", label: \"Intermediate\" },\n  { value: \"advanced\", label: \"Advanced\" },\n  { value: \"expert\", label: \"Expert\" }\n]\n \n// Target audience roles\nTARGET_AUDIENCE_ROLES = [\n  { value: \"solutions-architect\", label: \"Solutions Architect\" },\n  { value: \"developer\", label: \"Developer\" },\n  // etc.\n]\n \n// Course durations\nCOURSE_DURATIONS = [\n  { value: \"4-hours\", label: \"4 Hours\" },\n  { value: \"8-hours\", label: \"8 Hours (1 Day)\" },\n  // etc.\n]\n \n// AWS Services (categorized)\nAWS_SERVICES = {\n  COMPUTE: [\n    { value: \"ec2\", label: \"Amazon EC2\" },\n    // etc.\n  ],\n  // Other categories with services\n}\n11. Data Migration Considerations\nThe application has transitioned from PostgreSQL to exclusively using DynamoDB for data storage. The AWS credential storage has similarly moved from database to file-based storage (.aws-credentials.json).\n11.1 DynamoDB Tables\nThe following DynamoDB tables replace the PostgreSQL schema:\nTable Name\nPartition Key\nSort Key\nGlobal Secondary Indexes\nUsers\nid\n-\nusername-index\nProjects\nid\n-\nuserId-index, status-index\nStageContents\nid\n-\nprojectId-index, stageNumber-index\nResources\nid\n-\nprojectId-index, type-index\nComments\nid\n-\nprojectId-index, stageId-index\n\n12. Technical Implementation Notes\nFrontend implements reactive forms with react-hook-form and Zod validation\nAPI requests managed through TanStack Query with custom hooks\nAWS connectivity handled through file-based credential storage\nForm submissions handled through standardized mutation patterns\nAI assistance integrated through dedicated API endpoints for content generation\nStage approval process updates project.currentStage and stage.status fields\nMock implementations used for stages 2-9 until fully developed\n\nThis technical document represents the current implementation state of the TrainCraft application as of May 7, 2025, and may be updated as development progresses.",
        "RESPONSE": "Here is the input document with the new ideas and details incorporated, using proper Markdown formatting:\n\n# TrainCraft Technical Data Model and Workflow Specification\n\n**Document ID:** TC-TECH-001  \n**Version:** 1.0  \n**Last Updated:** 2025-05-07  \n**Classification:** Technical Reference  \n\n## 1. System Overview\n\nTrainCraft is an integrated AWS training content development platform with a 9-stage workflow for creating, managing, and publishing AWS training materials. This document provides a detailed technical specification of the data model, workflow stages, form structures, and field definitions.\n\n## 2. Database Schema\n\nThe database schema is implemented using a PostgreSQL-compatible structure through Drizzle ORM with the following core entities:\n\n### 2.1 Core Entities\n\n| Entity    | Description                   | Primary Keys | Relations                       |\n|-----------|--------------------------------|--------------|----------------------------------|\n| Users     | System users                  | id (serial)  | Referenced by multiple tables   |\n| Projects  | Training course projects      | id (serial)  | Parent for all content entities |\n| StageContents | Content for each workflow stage | id (serial)  | Belongs to Project              |\n| Resources | Training materials and assets | id (serial)  | Belongs to Project              |\n| Comments  | Feedback annotations          | id (serial)  | Belongs to Project, optionally to Stage |\n| AIMessages | Conversation history         | id (serial)  | Belongs to Project              |\n\n### 2.2 Workflow Entities\n\n| Entity           | Description                   | Primary Keys | Relations                      |\n|-------------------|--------------------------------|--------------|--------------------------------|\n| WorkflowStations | Sub-components within stages  | id (serial)  | Belongs to StageContent        |\n| StationContents  | Data for specific stations    | id (serial)  | Belongs to WorkflowStation     |\n| AISuggestions    | AI-generated content proposals | id (serial)  | Belongs to WorkflowStation     |\n| DocumentVersions | Version-controlled documents  | id (serial)  | Belongs to WorkflowStation     |\n| QualityChecks    | QA verification records       | id (serial)  | Belongs to WorkflowStation     |\n\n### 2.3 Stage-Specific Entities\n\n| Entity             | Description                   | Primary Keys | Relations                      |\n|---------------------|--------------------------------|--------------|--------------------------------|\n| CourseBasicInfo    | Stage 1 core course metadata  | id (serial)  | Belongs to Project             |\n| TargetAudienceInfo | Stage 1 audience definitions  | id (serial)  | Belongs to Project             |\n| MarketAnalysis     | Stage 2 market research data  | id (serial)  | Belongs to Project             |\n| LearningObjectives | Stage 2 educational goals     | id (serial)  | Belongs to Project             |\n| AssessmentStrategy | Stage 2 evaluation framework | id (serial)  | Belongs to Project             |\n\n## 3. Workflow Stages\n\nThe application implements a standardized 9-stage workflow for course development:\n\n| ID | Code    | Name                     | Purpose                                                   | Status Options                                |\n|----|---------|--------------------------|-----------------------------------------------------------|-----------------------------------------------|\n| 1  | COURSE  | Course Information       | Capture foundational course information and business requirements | not-started, in-progress, completed, blocked |\n| 2  | PRD     | Product Requirements Document | Define detailed requirements and specifications        | not-started, in-progress, completed, blocked |\n| 3  | DESIGN  | Design Workshop          | Create detailed course design structure                  | not-started, in-progress, completed, blocked |\n| 4  | DDD     | Detailed Design Document | Define technical specifications and architecture         | not-started, in-progress, completed, blocked |\n| 5  | CONTENT | Content Development      | Create and organize course content                       | not-started, in-progress, completed, blocked |\n| 6  | LABS    | Lab Design               | Design interactive lab exercises                         | not-started, in-progress, completed, blocked |\n| 7  | EDIT    | Visual Design            | Create visual assets and apply branding                 | not-started, in-progress, completed, blocked |\n| 8  | PODIUM  | Review & Refinement      | Conduct comprehensive review and QA                     | not-started, in-progress, completed, blocked |\n| 9  | PUBLISH | Publishing               | Prepare and publish final materials                     | not-started, in-progress, completed, blocked |\n\n## 4. Stage 1: Course Information - Technical Details\n\n### 4.1 Basic Course Information Station\n\n**Frontend Component:** `BasicCourseInfoStation.tsx`  \n**Database Tables:** `courseBasicInfo`, `stageContents`\n\n#### 4.1.1 Form Structure\n\n| Field Name      | Field Type | Database Field     | Validation Rules                | Default Value |\n|-----------------|------------|---------------------|----------------------------------|---------------|\n| courseTitle     | Text Input | courseTitle (VARCHAR) | 5-100 chars, required           | \"\"            |\n| courseCode      | Text Input | courseCode (VARCHAR)  | 3-20 chars, required            | \"\"            |\n| courseVersion   | Text Input | courseVersion (VARCHAR) | 1-20 chars, required            | \"1.0\"         |\n| courseDuration  | Number Input | courseDuration (INTEGER) | 1-200, required                 | 8             |\n| courseFormat    | Select     | courseFormat (VARCHAR)  | Required                        | \"instructor-led\" |\n| courseDescription | Textarea   | courseDescription (TEXT) | 10-1000 chars, required         | \"\"            |\n\n#### 4.1.2 API Endpoints\n\n| Method | Endpoint                          | Purpose                    | Required Fields |\n|--------|------------------------------------|-----------------------------|-----------------|\n| GET    | /api/projects/${projectId}/course-info | Fetch course info           | None            |\n| POST   | /api/course-info                  | Create new course info      | All fields, projectId |\n| PUT    | /api/course-info/${id}            | Update existing course info | All fields      |\n\n### 4.2 Target Audience Station\n\n**Frontend Component:** `TargetAudienceStation.tsx`  \n**Database Tables:** `targetAudienceInfo`, `stageContents`\n\n#### 4.2.1 Form Structure\n\n| Field Name      | Field Type    | Database Field     | Validation Rules      | Default Value |\n|-----------------|----------------|---------------------|------------------------|---------------|\n| roleProfiles    | Multi-select  | roleProfiles (JSONB) | Min 1 selection, required | []            |\n| requiredSkills  | Multi-select  | requiredSkills (JSONB) | Min 1 selection, required | []            |\n| skillLevel      | Select        | skillLevel (VARCHAR)  | Required                 | \"intermediate\" |\n| intendedOutcomes | Dynamic Input List | intendedOutcomes (JSONB) | Min 1 input, required    | [\"\"]          |\n\n#### 4.2.2 API Endpoints\n\n| Method | Endpoint                          | Purpose                    | Required Fields |\n|--------|------------------------------------|-----------------------------|-----------------|\n| GET    | /api/projects/${projectId}/target-audience | Fetch audience info        | None            |\n| POST   | /api/target-audience              | Create new audience info   | All fields, projectId |\n| PUT    | /api/target-audience/${id}        | Update existing audience info | All fields      |\n| POST   | /api/ai/generate-learning-objectives | Generate AI outcomes       | title, awsServices, audience, level |\n\n## 5. Stage 2: Product Requirements Document - Technical Details\n\n### 5.1 Market Analysis Station\n\n**Frontend Component:** Not fully implemented in UI  \n**Database Tables:** `marketAnalysis`, `stageContents`\n\n#### 5.1.1 Data Structure\n\n| Field Name          | Database Field       | Field Type    | Validation Rules |\n|----------------------|------------------------|----------------|------------------|\n| marketNeeds         | marketNeeds (TEXT)     | Textarea      | Optional         |\n| competitiveAnalysis | competitiveAnalysis (JSONB) | Structured Data | Optional         |\n| targetMarket        | targetMarket (TEXT)    | Textarea      | Optional         |\n| businessJustification | businessJustification (TEXT) | Textarea      | Optional         |\n\n### 5.2 Learning Objectives Station\n\n**Frontend Component:** Not fully implemented in UI  \n**Database Tables:** `learningObjectives`, `stageContents`\n\n#### 5.2.1 Data Structure\n\n| Field Name | Database Field | Field Type | Validation Rules |\n|------------|-----------------|------------|------------------|\n| objectives | objectives (JSONB) | Array of structured objects | Optional         |\n\n### 5.3 Assessment Strategy Station\n\n**Frontend Component:** Not fully implemented in UI  \n**Database Tables:** `assessmentStrategy`, `stageContents`\n\n#### 5.3.1 Data Structure\n\n| Field Name         | Database Field       | Field Type    | Validation Rules |\n|----------------------|------------------------|----------------|------------------|\n| assessmentTypes    | assessmentTypes (JSONB) | Array of types | Optional         |\n| evaluationCriteria | evaluationCriteria (JSONB) | Structured criteria | Optional         |\n\n## 6. Stage 3-9: Implementation Status\n\nStages 3-9 are defined in the database schema but have not been fully implemented in the frontend UI. The core data structure for these stages follows the `stageContents` table with stage-specific JSON content.\n\n| Stage                  | Database Storage                | Implementation Status |\n|------------------------|----------------------------------|------------------------|\n| Design Workshop        | `stageContents.content` (JSON)  | Placeholder UI only   |\n| Detailed Design Document | `stageContents.content` (JSON)  | Placeholder UI only   |\n| Content Development    | `stageContents.content` (JSON)  | Placeholder UI only   |\n| Lab Design             | `stageContents.content` (JSON)  | Placeholder UI only   |\n| Visual Design          | `stageContents.content` (JSON)  | Placeholder UI only   |\n| Review & Refinement    | `stageContents.content` (JSON)  | Placeholder UI only   |\n| Publishing             | `stageContents.content` (JSON)  | Placeholder UI only   |\n\n## 7. Document Generation and Versioning\n\n### 7.1 Document Version Control\n\n**Database Table:** `documentVersions`\n\n| Field          | Type      | Purpose                                       |\n|-----------------|------------|------------------------------------------------|\n| id              | serial     | Primary key                                    |\n| stationId       | integer    | Reference to station                           |\n| documentType    | text       | Type identifier (PRD, DDD, etc.)               |\n| versionNumber   | integer    | Sequential version                             |\n| documentContent | jsonb      | Full document content                          |\n| status          | text       | Document status (draft, in_review, approved, published) |\n| reviewedBy, approvedBy | integer    | User references                               |\n| reviewedAt, approvedAt | timestamp  | Action timestamps                             |\n\n### 7.2 Quality Assurance Process\n\n**Database Table:** `qualityChecks`\n\n| Field     | Type      | Purpose                                       |\n|------------|------------|------------------------------------------------|\n| id         | serial     | Primary key                                    |\n| stationId  | integer    | Reference to station                           |\n| checkType  | text       | Check category                                 |\n| status     | text       | Check status                                   |\n| findings   | jsonb      | Detailed findings                              |\n| checkedBy  | integer    | User reference                                 |\n| checkedAt  | timestamp  | Action timestamp                               |\n\n## 8. AI Integration Technical Specification\n\n### 8.1 AI Message Tracking\n\n**Database Table:** `aiMessages`\n\n| Field     | Type      | Purpose                                       |\n|------------|------------|------------------------------------------------|\n| id         | serial     | Primary key                                    |\n| projectId  | integer    | Reference to project                           |\n| sender     | text       | Origin (user/assistant)                        |\n| content    | text       | Message content                                |\n| timestamp  | timestamp  | Creation time                                  |\n\n### 8.2 AI Suggestion Management\n\n**Database Table:** `aiSuggestions`\n\n| Field            | Type      | Purpose                                       |\n|-------------------|------------|------------------------------------------------|\n| id                | serial     | Primary key                                    |\n| stationId         | integer    | Reference to station                           |\n| suggestionType    | text       | Type of suggestion                             |\n| suggestionContent | text       | Proposed content                               |\n| status            | text       | Suggestion status                              |\n| appliedContent    | text       | Modified application                           |\n| createdAt, updatedAt | timestamp  | Tracking timestamps                            |\n\n## 9. User Interface Technical Components\n\n### 9.1 Dashboard Page\n\n**Component:** `Dashboard.tsx`\n\n**Primary Features:**\n- `ProjectSummary`: Statistical overview cards\n- `ProjectsList`: Tabular project listing with filtering\n- `DataSourcesList`: Resource management interface\n- `AwsUnifiedStatusCard`: AWS service connection status\n\n### 9.2 Project Page\n\n**Component:** `ProjectPage.tsx`\n\n**Primary Features:**\n- `WorkflowProgress`: Visual workflow stage navigation\n- Stage-specific form components (9 total)\n- Form state management with `formRefs` system\n- Stage approval and advancement logic\n\n### 9.3 Workflow Page\n\n**Component:** `WorkflowPage.tsx`\n\n**Primary Features:**\n- `StageHeader`: Stage navigation and project identification\n- Tab navigation between \"Workflow Stations\" and \"Stage Overview\"\n- Station-based content forms for active stage\n- `ActionButtons`: Save, Build (AI), and Approve functions\n\n### 9.4 AWS Settings Page\n\n**Component:** `AwsSettingsPage.tsx`\n\n**Primary Features:**\n- AWS connection status monitoring\n- `AwsCredentialsManager`: Credential management interface\n- Service status indicators for each AWS service\n- Manual synchronization controls\n\n## 10. Technical Constants and Enumerations\n\n### 10.1 Workflow Stage Definitions\n\n```js\nWORKFLOW_STAGES = [\n  { id: 1, name: \"COURSE\", displayName: \"Course Information\", ... },\n  { id: 2, name: \"PRD\", displayName: \"Product Requirements Document\", ... },\n  // ...\n]\n```\n\n### 10.2 Status Enumerations\n\n```js\nSTAGE_STATUS = {\n  NOT_STARTED: \"not-started\",\n  IN_PROGRESS: \"in-progress\",\n  COMPLETED: \"completed\",\n  BLOCKED: \"blocked\"\n}\n\nPROJECT_STATUS = {\n  NOT_STARTED: \"not-started\", \n  IN_PROGRESS: \"in-progress\",\n  COMPLETED: \"completed\",\n  PAUSED: \"paused\"\n}\n```\n\n### 10.3 Domain-Specific Enumerations\n\n```js\nINSTRUCTIONAL_LEVELS = [\n  { value: \"100\", label: \"100 - Foundational\" },\n  { value: \"200\", label: \"200 - Intermediate\" },\n  { value: \"300\", label: \"300 - Advanced\" },\n  { value: \"400\", label: \"400 - Expert\" }\n]\n\nAUDIENCE_SKILL_LEVELS = [\n  { value: \"beginner\", label: \"Beginner\" },\n  { value: \"intermediate\", label: \"Intermediate\" },\n  { value: \"advanced\", label: \"Advanced\" },\n  { value: \"expert\", label: \"Expert\" }\n]\n\nTARGET_AUDIENCE_ROLES = [\n  { value: \"solutions-architect\", label: \"Solutions Architect\" },\n  { value: \"developer\", label: \"Developer\" },\n  // ...\n]\n\nCOURSE_DURATIONS = [\n  { value: \"4-hours\", label: \"4 Hours\" },\n  { value: \"8-hours\", label: \"8 Hours (1 Day)\" },\n  // ...\n]\n\nAWS_SERVICES = {\n  COMPUTE: [\n    { value: \"ec2\", label: \"Amazon EC2\" },\n    // ...\n  ],\n  // ...\n}\n```\n\n## 11. Data Migration Considerations\n\nThe application has transitioned from PostgreSQL to exclusively using DynamoDB for data storage. The AWS credential storage has similarly moved from database to file-based storage (`.aws-credentials.json`).\n\n### 11.1 DynamoDB Tables\n\nThe following DynamoDB tables replace the PostgreSQL schema:\n\n| Table Name   | Partition Key | Sort Key | Global Secondary Indexes                     |\n|---------------|----------------|----------|--------------------------------------------|\n| Users        | id             | -        | username-index                             |\n| Projects     | id             | -        | userId-index, status-index                 |\n| StageContents | id             | -        ",
        "PARAMETERS": {
            "temp": 0.5,
            "topP": 1.0,
            "topK": 250,
            "maxT": 200000,
            "stopSeq": ""
        }
    },
    {
        "timestamp": "2025-05-07 16:25:26",
        "MODEL": "Claude 35S",
        "COMMAND": "Read the doc in the INPUT.\n\nSome of the ideas below are not incorporated into the input yet.  Incorporate those ideas and details INTO the doc in the input.\n\nWrite your response with full markdown for table formatting and text formatting.  \n\nStage 1: Project Initiation\nDefine Project Basics: User creates a new training project by entering core details:\nCourse title and code\nDescription and business need justification\nTarget audience and skill level requirements\nInstructional level (100-400)\nEstimated duration\nSelect AWS Services: User specifies which AWS services will be covered in the training\nSet Project Parameters: Version number, status tracking begins as \"in-progress\"\nData Storage: Project metadata is saved to DynamoDB in the TrainCraft_Projects table\nStage 2: Training Needs Analysis\nAudience Assessment: User defines detailed target audience characteristics\nKnowledge Gap Analysis: Document current vs. desired knowledge state\nBusiness Impact Measurement: Define how training success will be measured\nAI Assistance: May use Amazon Bedrock models to help generate audience profiles\nData Storage: Stage content saved to TrainCraft_StageContents with projectId reference\nStage 3: Design Workshop\nLearning Objectives: Define specific learning outcomes\nContent Structure: Outline modules, topics, and learning flow\nInteractive Elements: Plan hands-on activities, labs, assessments\nCollaboration: Multiple stakeholders may add comments via the Comments system\nReview Process: Design approval workflow using comments/status changes\nStage 4: Content Development\nCreate Course Materials: Develop detailed content for each module\nTechnical Writing: Document AWS service details, use cases, limitations\nAI Augmentation: Use Amazon Bedrock models to enhance content generation\nClaude 3.7 Sonnet for complex technical explanations\nNova Premier for use case narratives\nOther models for specific content tasks\nProgress Tracking: Content completeness tracked by stage/module\nStage 5: Lab Exercise Design\nTechnical Requirements: Define AWS resources needed for labs\nStep-by-Step Instructions: Create detailed lab procedures\nSuccess Criteria: Define completion requirements\nTesting: Validate lab exercises functionality\nResources: Attach supporting resources (code samples, templates) via Resources system\nStage 6: Quality Review\nTechnical Accuracy: Verify AWS service information is current and correct\nPedagogical Review: Ensure content meets learning objectives\nFormatting Standards: Check for consistency with AWS training templates\nComment System: Reviewers add feedback via Comments\nRevision Tracking: Changes logged with updatedAt timestamps\nStage 7: Final Edits\nAddress Review Feedback: Implement changes from Stage 6\nContent Polishing: Final language refinement and technical edits\nVisual Elements: Finalize diagrams, illustrations, presentations\nCross-references: Ensure consistency across all materials\nFinal Approval: Mark stage as completed when ready\nStage 8: Pre-Publication\nPackaging: Assemble all materials into deployment format\nAccess Controls: Set up appropriate permissions\nQuality Assurance: Final verification of all components\nRelease Notes: Document version and change information\nDeployment Plan: Schedule and logistics for rollout\nStage 9: Publication\nRelease: Deploy training content to final destination platforms\nAnnouncement: Notify stakeholders of availability\nAccess Management: Provide credentials to instructors/learners\nResource Tracking: Monitor usage metrics\nProject Status: Mark project as \"completed\" in system\nFinal Storage: All materials archived in connected AWS S3 storage\nThroughout this entire workflow:\nProgress Tracking: Each stage updates currentStage, completedAt timestamps\nCollaboration: Comments system allows for feedback at any stage\nResource Management: Training assets are attached and versioned\nAI Assistance: Amazon Bedrock integration provides AI support across stages\nData Persistence: All information stored in DynamoDB tables",
        "INPUT": "TrainCraft Technical Data Model and Workflow Specification\nDocument ID: TC-TECH-001\nVersion: 1.0\nLast Updated: 2025-05-07\nClassification: Technical Reference\n1. System Overview\nTrainCraft is an integrated AWS training content development platform with a 9-stage workflow for creating, managing, and publishing AWS training materials. This document provides a detailed technical specification of the data model, workflow stages, form structures, and field definitions.\n2. Database Schema\nThe database schema is implemented using a PostgreSQL-compatible structure through Drizzle ORM with the following core entities:\n2.1 Core Entities\nEntity\nDescription\nPrimary Keys\nRelations\nUsers\nSystem users\nid (serial)\nReferenced by multiple tables\nProjects\nTraining course projects\nid (serial)\nParent for all content entities\nStageContents\nContent for each workflow stage\nid (serial)\nBelongs to Project\nResources\nTraining materials and assets\nid (serial)\nBelongs to Project\nComments\nFeedback annotations\nid (serial)\nBelongs to Project, optionally to Stage\nAIMessages\nConversation history\nid (serial)\nBelongs to Project\n\n2.2 Workflow Entities\nEntity\nDescription\nPrimary Keys\nRelations\nWorkflowStations\nSub-components within stages\nid (serial)\nBelongs to StageContent\nStationContents\nData for specific stations\nid (serial)\nBelongs to WorkflowStation\nAISuggestions\nAI-generated content proposals\nid (serial)\nBelongs to WorkflowStation\nDocumentVersions\nVersion-controlled documents\nid (serial)\nBelongs to WorkflowStation\nQualityChecks\nQA verification records\nid (serial)\nBelongs to WorkflowStation\n\n2.3 Stage-Specific Entities\nEntity\nDescription\nPrimary Keys\nRelations\nCourseBasicInfo\nStage 1 core course metadata\nid (serial)\nBelongs to Project\nTargetAudienceInfo\nStage 1 audience definitions\nid (serial)\nBelongs to Project\nMarketAnalysis\nStage 2 market research data\nid (serial)\nBelongs to Project\nLearningObjectives\nStage 2 educational goals\nid (serial)\nBelongs to Project\nAssessmentStrategy\nStage 2 evaluation framework\nid (serial)\nBelongs to Project\n\n3. Workflow Stages\nThe application implements a standardized 9-stage workflow for course development:\nID\nCode\nName\nPurpose\nStatus Options\n1\nCOURSE\nCourse Information\nCapture foundational course information and business requirements\nnot-started, in-progress, completed, blocked\n2\nPRD\nProduct Requirements Document\nDefine detailed requirements and specifications\nnot-started, in-progress, completed, blocked\n3\nDESIGN\nDesign Workshop\nCreate detailed course design structure\nnot-started, in-progress, completed, blocked\n4\nDDD\nDetailed Design Document\nDefine technical specifications and architecture\nnot-started, in-progress, completed, blocked\n5\nCONTENT\nContent Development\nCreate and organize course content\nnot-started, in-progress, completed, blocked\n6\nLABS\nLab Design\nDesign interactive lab exercises\nnot-started, in-progress, completed, blocked\n7\nEDIT\nVisual Design\nCreate visual assets and apply branding\nnot-started, in-progress, completed, blocked\n8\nPODIUM\nReview & Refinement\nConduct comprehensive review and QA\nnot-started, in-progress, completed, blocked\n9\nPUBLISH\nPublishing\nPrepare and publish final materials\nnot-started, in-progress, completed, blocked\n\n4. Stage 1: Course Information - Technical Details\n4.1 Basic Course Information Station\nFrontend Component: BasicCourseInfoStation.tsx\nDatabase Tables: courseBasicInfo, stageContents\n4.1.1 Form Structure\nField Name\nField Type\nDatabase Field\nValidation Rules\nDefault Value\ncourseTitle\nText Input\ncourseTitle (VARCHAR)\n5-100 chars, required\n\"\"\ncourseCode\nText Input\ncourseCode (VARCHAR)\n3-20 chars, required\n\"\"\ncourseVersion\nText Input\ncourseVersion (VARCHAR)\n1-20 chars, required\n\"1.0\"\ncourseDuration\nNumber Input\ncourseDuration (INTEGER)\n1-200, required\n8\ncourseFormat\nSelect\ncourseFormat (VARCHAR)\nRequired\n\"instructor-led\"\ncourseDescription\nTextarea\ncourseDescription (TEXT)\n10-1000 chars, required\n\"\"\n\n4.1.2 API Endpoints\nMethod\nEndpoint\nPurpose\nRequired Fields\nGET\n/api/projects/${projectId}/course-info\nFetch course info\nNone\nPOST\n/api/course-info\nCreate new course info\nAll fields, projectId\nPUT\n/api/course-info/${id}\nUpdate existing course info\nAll fields\n\n4.2 Target Audience Station\nFrontend Component: TargetAudienceStation.tsx\nDatabase Tables: targetAudienceInfo, stageContents\n4.2.1 Form Structure\nField Name\nField Type\nDatabase Field\nValidation Rules\nDefault Value\nroleProfiles\nMulti-select\nroleProfiles (JSONB)\nMin 1 selection, required\n[]\nrequiredSkills\nMulti-select\nrequiredSkills (JSONB)\nMin 1 selection, required\n[]\nskillLevel\nSelect\nskillLevel (VARCHAR)\nRequired\n\"intermediate\"\nintendedOutcomes\nDynamic Input List\nintendedOutcomes (JSONB)\nMin 1 input, required\n[\"\"]\n\n4.2.2 API Endpoints\nMethod\nEndpoint\nPurpose\nRequired Fields\nGET\n/api/projects/${projectId}/target-audience\nFetch audience info\nNone\nPOST\n/api/target-audience\nCreate new audience info\nAll fields, projectId\nPUT\n/api/target-audience/${id}\nUpdate existing audience info\nAll fields\nPOST\n/api/ai/generate-learning-objectives\nGenerate AI outcomes\ntitle, awsServices, audience, level\n\n5. Stage 2: Product Requirements Document - Technical Details\n5.1 Market Analysis Station\nFrontend Component: Not fully implemented in UI\nDatabase Tables: marketAnalysis, stageContents\n5.1.1 Data Structure\nField Name\nDatabase Field\nField Type\nValidation Rules\nmarketNeeds\nmarketNeeds (TEXT)\nTextarea\nOptional\ncompetitiveAnalysis\ncompetitiveAnalysis (JSONB)\nStructured Data\nOptional\ntargetMarket\ntargetMarket (TEXT)\nTextarea\nOptional\nbusinessJustification\nbusinessJustification (TEXT)\nTextarea\nOptional\n\n5.2 Learning Objectives Station\nFrontend Component: Not fully implemented in UI\nDatabase Tables: learningObjectives, stageContents\n5.2.1 Data Structure\nField Name\nDatabase Field\nField Type\nValidation Rules\nobjectives\nobjectives (JSONB)\nArray of structured objects\nOptional\n\n5.3 Assessment Strategy Station\nFrontend Component: Not fully implemented in UI\nDatabase Tables: assessmentStrategy, stageContents\n5.3.1 Data Structure\nField Name\nDatabase Field\nField Type\nValidation Rules\nassessmentTypes\nassessmentTypes (JSONB)\nArray of types\nOptional\nevaluationCriteria\nevaluationCriteria (JSONB)\nStructured criteria\nOptional\n\n6. Stage 3-9: Implementation Status\nStages 3-9 are defined in the database schema but have not been fully implemented in the frontend UI. The core data structure for these stages follows the stageContents table with stage-specific JSON content.\nStage\nDatabase Storage\nImplementation Status\nDesign Workshop\nstageContents.content (JSON)\nPlaceholder UI only\nDetailed Design Document\nstageContents.content (JSON)\nPlaceholder UI only\nContent Development\nstageContents.content (JSON)\nPlaceholder UI only\nLab Design\nstageContents.content (JSON)\nPlaceholder UI only\nVisual Design\nstageContents.content (JSON)\nPlaceholder UI only\nReview & Refinement\nstageContents.content (JSON)\nPlaceholder UI only\nPublishing\nstageContents.content (JSON)\nPlaceholder UI only\n\n7. Document Generation and Versioning\n7.1 Document Version Control\nDatabase Table: documentVersions\nField\nType\nPurpose\nid\nserial\nPrimary key\nstationId\ninteger\nReference to station\ndocumentType\ntext\nType identifier (PRD, DDD, etc.)\nversionNumber\ninteger\nSequential version\ndocumentContent\njsonb\nFull document content\nstatus\ntext\nDocument status (draft, in_review, approved, published)\nreviewedBy, approvedBy\ninteger\nUser references\nreviewedAt, approvedAt\ntimestamp\nAction timestamps\n\n7.2 Quality Assurance Process\nDatabase Table: qualityChecks\nField\nType\nPurpose\nid\nserial\nPrimary key\nstationId\ninteger\nReference to station\ncheckType\ntext\nCheck category\nstatus\ntext\nCheck status\nfindings\njsonb\nDetailed findings\ncheckedBy\ninteger\nUser reference\ncheckedAt\ntimestamp\nAction timestamp\n\n8. AI Integration Technical Specification\n8.1 AI Message Tracking\nDatabase Table: aiMessages\nField\nType\nPurpose\nid\nserial\nPrimary key\nprojectId\ninteger\nReference to project\nsender\ntext\nOrigin (user/assistant)\ncontent\ntext\nMessage content\ntimestamp\ntimestamp\nCreation time\n\n8.2 AI Suggestion Management\nDatabase Table: aiSuggestions\nField\nType\nPurpose\nid\nserial\nPrimary key\nstationId\ninteger\nReference to station\nsuggestionType\ntext\nType of suggestion\nsuggestionContent\ntext\nProposed content\nstatus\ntext\nSuggestion status\nappliedContent\ntext\nModified application\ncreatedAt, updatedAt\ntimestamp\nTracking timestamps\n\n9. User Interface Technical Components\n9.1 Dashboard Page\nComponent: Dashboard.tsx\nPrimary Features:\nProjectSummary: Statistical overview cards\nProjectsList: Tabular project listing with filtering\nDataSourcesList: Resource management interface\nAwsUnifiedStatusCard: AWS service connection status\n9.2 Project Page\nComponent: ProjectPage.tsx\nPrimary Features:\nWorkflowProgress: Visual workflow stage navigation\nStage-specific form components (9 total)\nForm state management with formRefs system\nStage approval and advancement logic\n9.3 Workflow Page\nComponent: WorkflowPage.tsx\nPrimary Features:\nStageHeader: Stage navigation and project identification\nTab navigation between \"Workflow Stations\" and \"Stage Overview\"\nStation-based content forms for active stage\nActionButtons: Save, Build (AI), and Approve functions\n9.4 AWS Settings Page\nComponent: AwsSettingsPage.tsx\nPrimary Features:\nAWS connection status monitoring\nAwsCredentialsManager: Credential management interface\nService status indicators for each AWS service\nManual synchronization controls\n10. Technical Constants and Enumerations\n10.1 Workflow Stage Definitions\nWORKFLOW_STAGES = [\n  { id: 1, name: \"COURSE\", displayName: \"Course Information\", ... },\n  { id: 2, name: \"PRD\", displayName: \"Product Requirements Document\", ... },\n  // Stages 3-9 following same pattern\n]\n10.2 Status Enumerations\nSTAGE_STATUS = {\n  NOT_STARTED: \"not-started\",\n  IN_PROGRESS: \"in-progress\",\n  COMPLETED: \"completed\",\n  BLOCKED: \"blocked\"\n}\n \nPROJECT_STATUS = {\n  NOT_STARTED: \"not-started\",\n  IN_PROGRESS: \"in-progress\",\n  COMPLETED: \"completed\",\n  PAUSED: \"paused\"\n}\n10.3 Domain-Specific Enumerations\n// Instructional levels\nINSTRUCTIONAL_LEVELS = [\n  { value: \"100\", label: \"100 - Foundational\" },\n  { value: \"200\", label: \"200 - Intermediate\" },\n  { value: \"300\", label: \"300 - Advanced\" },\n  { value: \"400\", label: \"400 - Expert\" }\n]\n \n// Audience skill levels\nAUDIENCE_SKILL_LEVELS = [\n  { value: \"beginner\", label: \"Beginner\" },\n  { value: \"intermediate\", label: \"Intermediate\" },\n  { value: \"advanced\", label: \"Advanced\" },\n  { value: \"expert\", label: \"Expert\" }\n]\n \n// Target audience roles\nTARGET_AUDIENCE_ROLES = [\n  { value: \"solutions-architect\", label: \"Solutions Architect\" },\n  { value: \"developer\", label: \"Developer\" },\n  // etc.\n]\n \n// Course durations\nCOURSE_DURATIONS = [\n  { value: \"4-hours\", label: \"4 Hours\" },\n  { value: \"8-hours\", label: \"8 Hours (1 Day)\" },\n  // etc.\n]\n \n// AWS Services (categorized)\nAWS_SERVICES = {\n  COMPUTE: [\n    { value: \"ec2\", label: \"Amazon EC2\" },\n    // etc.\n  ],\n  // Other categories with services\n}\n11. Data Migration Considerations\nThe application has transitioned from PostgreSQL to exclusively using DynamoDB for data storage. The AWS credential storage has similarly moved from database to file-based storage (.aws-credentials.json).\n11.1 DynamoDB Tables\nThe following DynamoDB tables replace the PostgreSQL schema:\nTable Name\nPartition Key\nSort Key\nGlobal Secondary Indexes\nUsers\nid\n-\nusername-index\nProjects\nid\n-\nuserId-index, status-index\nStageContents\nid\n-\nprojectId-index, stageNumber-index\nResources\nid\n-\nprojectId-index, type-index\nComments\nid\n-\nprojectId-index, stageId-index\n\n12. Technical Implementation Notes\nFrontend implements reactive forms with react-hook-form and Zod validation\nAPI requests managed through TanStack Query with custom hooks\nAWS connectivity handled through file-based credential storage\nForm submissions handled through standardized mutation patterns\nAI assistance integrated through dedicated API endpoints for content generation\nStage approval process updates project.currentStage and stage.status fields\nMock implementations used for stages 2-9 until fully developed\n\nThis technical document represents the current implementation state of the TrainCraft application as of May 7, 2025, and may be updated as development progresses.",
        "RESPONSE": "Here is the input document with the new ideas and details incorporated, using proper Markdown formatting:\n\n# TrainCraft Technical Data Model and Workflow Specification\n\n**Document ID:** TC-TECH-001  \n**Version:** 1.0  \n**Last Updated:** 2025-05-07  \n**Classification:** Technical Reference  \n\n## 1. System Overview\n\nTrainCraft is an integrated AWS training content development platform with a 9-stage workflow for creating, managing, and publishing AWS training materials. This document provides a detailed technical specification of the data model, workflow stages, form structures, and field definitions.\n\n## 2. Database Schema\n\nThe database schema is implemented using a PostgreSQL-compatible structure through Drizzle ORM with the following core entities:\n\n### 2.1 Core Entities\n\n| Entity    | Description                   | Primary Keys | Relations                       |\n|-----------|--------------------------------|--------------|----------------------------------|\n| Users     | System users                  | id (serial)  | Referenced by multiple tables   |\n| Projects  | Training course projects      | id (serial)  | Parent for all content entities |\n| StageContents | Content for each workflow stage | id (serial)  | Belongs to Project              |\n| Resources | Training materials and assets | id (serial)  | Belongs to Project              |\n| Comments  | Feedback annotations          | id (serial)  | Belongs to Project, optionally to Stage |\n| AIMessages | Conversation history         | id (serial)  | Belongs to Project              |\n\n### 2.2 Workflow Entities\n\n| Entity           | Description                   | Primary Keys | Relations                      |\n|-------------------|--------------------------------|--------------|--------------------------------|\n| WorkflowStations | Sub-components within stages  | id (serial)  | Belongs to StageContent        |\n| StationContents  | Data for specific stations    | id (serial)  | Belongs to WorkflowStation     |\n| AISuggestions    | AI-generated content proposals | id (serial)  | Belongs to WorkflowStation     |\n| DocumentVersions | Version-controlled documents  | id (serial)  | Belongs to WorkflowStation     |\n| QualityChecks    | QA verification records       | id (serial)  | Belongs to WorkflowStation     |\n\n### 2.3 Stage-Specific Entities\n\n| Entity             | Description                   | Primary Keys | Relations                      |\n|---------------------|--------------------------------|--------------|--------------------------------|\n| CourseBasicInfo    | Stage 1 core course metadata  | id (serial)  | Belongs to Project             |\n| TargetAudienceInfo | Stage 1 audience definitions  | id (serial)  | Belongs to Project             |\n| MarketAnalysis     | Stage 2 market research data  | id (serial)  | Belongs to Project             |\n| LearningObjectives | Stage 2 educational goals     | id (serial)  | Belongs to Project             |\n| AssessmentStrategy | Stage 2 evaluation framework | id (serial)  | Belongs to Project             |\n\n## 3. Workflow Stages\n\nThe application implements a standardized 9-stage workflow for course development:\n\n| ID | Code    | Name                     | Purpose                                                   | Status Options                                |\n|----|---------|--------------------------|-----------------------------------------------------------|-----------------------------------------------|\n| 1  | COURSE  | Course Information       | Capture foundational course information and business requirements | not-started, in-progress, completed, blocked |\n| 2  | PRD     | Product Requirements Document | Define detailed requirements and specifications        | not-started, in-progress, completed, blocked |\n| 3  | DESIGN  | Design Workshop          | Create detailed course design structure                  | not-started, in-progress, completed, blocked |\n| 4  | DDD     | Detailed Design Document | Define technical specifications and architecture         | not-started, in-progress, completed, blocked |\n| 5  | CONTENT | Content Development      | Create and organize course content                       | not-started, in-progress, completed, blocked |\n| 6  | LABS    | Lab Design               | Design interactive lab exercises                         | not-started, in-progress, completed, blocked |\n| 7  | EDIT    | Visual Design            | Create visual assets and apply branding                 | not-started, in-progress, completed, blocked |\n| 8  | PODIUM  | Review & Refinement      | Conduct comprehensive review and QA                     | not-started, in-progress, completed, blocked |\n| 9  | PUBLISH | Publishing               | Prepare and publish final materials                     | not-started, in-progress, completed, blocked |\n\n## 4. Stage 1: Course Information - Technical Details\n\n### 4.1 Basic Course Information Station\n\n**Frontend Component:** `BasicCourseInfoStation.tsx`  \n**Database Tables:** `courseBasicInfo`, `stageContents`\n\n#### 4.1.1 Form Structure\n\n| Field Name      | Field Type | Database Field     | Validation Rules                | Default Value |\n|-----------------|------------|---------------------|----------------------------------|---------------|\n| courseTitle     | Text Input | courseTitle (VARCHAR) | 5-100 chars, required           | \"\"            |\n| courseCode      | Text Input | courseCode (VARCHAR)  | 3-20 chars, required            | \"\"            |\n| courseVersion   | Text Input | courseVersion (VARCHAR) | 1-20 chars, required            | \"1.0\"         |\n| courseDuration  | Number Input | courseDuration (INTEGER) | 1-200, required                 | 8             |\n| courseFormat    | Select     | courseFormat (VARCHAR)  | Required                        | \"instructor-led\" |\n| courseDescription | Textarea   | courseDescription (TEXT) | 10-1000 chars, required         | \"\"            |\n\n#### 4.1.2 API Endpoints\n\n| Method | Endpoint                          | Purpose                    | Required Fields |\n|--------|------------------------------------|-----------------------------|-----------------|\n| GET    | /api/projects/${projectId}/course-info | Fetch course info           | None            |\n| POST   | /api/course-info                  | Create new course info      | All fields, projectId |\n| PUT    | /api/course-info/${id}            | Update existing course info | All fields      |\n\n### 4.2 Target Audience Station\n\n**Frontend Component:** `TargetAudienceStation.tsx`  \n**Database Tables:** `targetAudienceInfo`, `stageContents`\n\n#### 4.2.1 Form Structure\n\n| Field Name      | Field Type    | Database Field     | Validation Rules      | Default Value |\n|-----------------|----------------|---------------------|------------------------|---------------|\n| roleProfiles    | Multi-select  | roleProfiles (JSONB) | Min 1 selection, required | []            |\n| requiredSkills  | Multi-select  | requiredSkills (JSONB) | Min 1 selection, required | []            |\n| skillLevel      | Select        | skillLevel (VARCHAR)  | Required                 | \"intermediate\" |\n| intendedOutcomes | Dynamic Input List | intendedOutcomes (JSONB) | Min 1 input, required    | [\"\"]          |\n\n#### 4.2.2 API Endpoints\n\n| Method | Endpoint                          | Purpose                    | Required Fields |\n|--------|------------------------------------|-----------------------------|-----------------|\n| GET    | /api/projects/${projectId}/target-audience | Fetch audience info        | None            |\n| POST   | /api/target-audience              | Create new audience info   | All fields, projectId |\n| PUT    | /api/target-audience/${id}        | Update existing audience info | All fields      |\n| POST   | /api/ai/generate-learning-objectives | Generate AI outcomes       | title, awsServices, audience, level |\n\n## 5. Stage 2: Product Requirements Document - Technical Details\n\n### 5.1 Market Analysis Station\n\n**Frontend Component:** Not fully implemented in UI  \n**Database Tables:** `marketAnalysis`, `stageContents`\n\n#### 5.1.1 Data Structure\n\n| Field Name          | Database Field       | Field Type    | Validation Rules |\n|----------------------|------------------------|----------------|------------------|\n| marketNeeds         | marketNeeds (TEXT)     | Textarea      | Optional         |\n| competitiveAnalysis | competitiveAnalysis (JSONB) | Structured Data | Optional         |\n| targetMarket        | targetMarket (TEXT)    | Textarea      | Optional         |\n| businessJustification | businessJustification (TEXT) | Textarea      | Optional         |\n\n### 5.2 Learning Objectives Station\n\n**Frontend Component:** Not fully implemented in UI  \n**Database Tables:** `learningObjectives`, `stageContents`\n\n#### 5.2.1 Data Structure\n\n| Field Name | Database Field | Field Type | Validation Rules |\n|------------|-----------------|------------|------------------|\n| objectives | objectives (JSONB) | Array of structured objects | Optional         |\n\n### 5.3 Assessment Strategy Station\n\n**Frontend Component:** Not fully implemented in UI  \n**Database Tables:** `assessmentStrategy`, `stageContents`\n\n#### 5.3.1 Data Structure\n\n| Field Name         | Database Field       | Field Type    | Validation Rules |\n|----------------------|------------------------|----------------|------------------|\n| assessmentTypes    | assessmentTypes (JSONB) | Array of types | Optional         |\n| evaluationCriteria | evaluationCriteria (JSONB) | Structured criteria | Optional         |\n\n## 6. Stage 3-9: Implementation Status\n\nStages 3-9 are defined in the database schema but have not been fully implemented in the frontend UI. The core data structure for these stages follows the `stageContents` table with stage-specific JSON content.\n\n| Stage                  | Database Storage                | Implementation Status |\n|------------------------|----------------------------------|------------------------|\n| Design Workshop        | `stageContents.content` (JSON)  | Placeholder UI only   |\n| Detailed Design Document | `stageContents.content` (JSON)  | Placeholder UI only   |\n| Content Development    | `stageContents.content` (JSON)  | Placeholder UI only   |\n| Lab Design             | `stageContents.content` (JSON)  | Placeholder UI only   |\n| Visual Design          | `stageContents.content` (JSON)  | Placeholder UI only   |\n| Review & Refinement    | `stageContents.content` (JSON)  | Placeholder UI only   |\n| Publishing             | `stageContents.content` (JSON)  | Placeholder UI only   |\n\n## 7. Document Generation and Versioning\n\n### 7.1 Document Version Control\n\n**Database Table:** `documentVersions`\n\n| Field          | Type      | Purpose                                       |\n|-----------------|------------|------------------------------------------------|\n| id              | serial     | Primary key                                    |\n| stationId       | integer    | Reference to station                           |\n| documentType    | text       | Type identifier (PRD, DDD, etc.)               |\n| versionNumber   | integer    | Sequential version                             |\n| documentContent | jsonb      | Full document content                          |\n| status          | text       | Document status (draft, in_review, approved, published) |\n| reviewedBy, approvedBy | integer    | User references                               |\n| reviewedAt, approvedAt | timestamp  | Action timestamps                             |\n\n### 7.2 Quality Assurance Process\n\n**Database Table:** `qualityChecks`\n\n| Field     | Type      | Purpose                                       |\n|------------|------------|------------------------------------------------|\n| id         | serial     | Primary key                                    |\n| stationId  | integer    | Reference to station                           |\n| checkType  | text       | Check category                                 |\n| status     | text       | Check status                                   |\n| findings   | jsonb      | Detailed findings                              |\n| checkedBy  | integer    | User reference                                 |\n| checkedAt  | timestamp  | Action timestamp                               |\n\n## 8. AI Integration Technical Specification\n\n### 8.1 AI Message Tracking\n\n**Database Table:** `aiMessages`\n\n| Field     | Type      | Purpose                                       |\n|------------|------------|------------------------------------------------|\n| id         | serial     | Primary key                                    |\n| projectId  | integer    | Reference to project                           |\n| sender     | text       | Origin (user/assistant)                        |\n| content    | text       | Message content                                |\n| timestamp  | timestamp  | Creation time                                  |\n\n### 8.2 AI Suggestion Management\n\n**Database Table:** `aiSuggestions`\n\n| Field            | Type      | Purpose                                       |\n|-------------------|------------|------------------------------------------------|\n| id                | serial     | Primary key                                    |\n| stationId         | integer    | Reference to station                           |\n| suggestionType    | text       | Type of suggestion                             |\n| suggestionContent | text       | Proposed content                               |\n| status            | text       | Suggestion status                              |\n| appliedContent    | text       | Modified application                           |\n| createdAt, updatedAt | timestamp  | Tracking timestamps                            |\n\n## 9. User Interface Technical Components\n\n### 9.1 Dashboard Page\n\n**Component:** `Dashboard.tsx`\n\n**Primary Features:**\n- `ProjectSummary`: Statistical overview cards\n- `ProjectsList`: Tabular project listing with filtering\n- `DataSourcesList`: Resource management interface\n- `AwsUnifiedStatusCard`: AWS service connection status\n\n### 9.2 Project Page\n\n**Component:** `ProjectPage.tsx`\n\n**Primary Features:**\n- `WorkflowProgress`: Visual workflow stage navigation\n- Stage-specific form components (9 total)\n- Form state management with `formRefs` system\n- Stage approval and advancement logic\n\n### 9.3 Workflow Page\n\n**Component:** `WorkflowPage.tsx`\n\n**Primary Features:**\n- `StageHeader`: Stage navigation and project identification\n- Tab navigation between \"Workflow Stations\" and \"Stage Overview\"\n- Station-based content forms for active stage\n- `ActionButtons`: Save, Build (AI), and Approve functions\n\n### 9.4 AWS Settings Page\n\n**Component:** `AwsSettingsPage.tsx`\n\n**Primary Features:**\n- AWS connection status monitoring\n- `AwsCredentialsManager`: Credential management interface\n- Service status indicators for each AWS service\n- Manual synchronization controls\n\n## 10. Technical Constants and Enumerations\n\n### 10.1 Workflow Stage Definitions\n\n```js\nWORKFLOW_STAGES = [\n  { id: 1, name: \"COURSE\", displayName: \"Course Information\", ... },\n  { id: 2, name: \"PRD\", displayName: \"Product Requirements Document\", ... },\n  // ...\n]\n```\n\n### 10.2 Status Enumerations\n\n```js\nSTAGE_STATUS = {\n  NOT_STARTED: \"not-started\",\n  IN_PROGRESS: \"in-progress\",\n  COMPLETED: \"completed\",\n  BLOCKED: \"blocked\"\n}\n\nPROJECT_STATUS = {\n  NOT_STARTED: \"not-started\", \n  IN_PROGRESS: \"in-progress\",\n  COMPLETED: \"completed\",\n  PAUSED: \"paused\"\n}\n```\n\n### 10.3 Domain-Specific Enumerations\n\n```js\nINSTRUCTIONAL_LEVELS = [\n  { value: \"100\", label: \"100 - Foundational\" },\n  { value: \"200\", label: \"200 - Intermediate\" },\n  { value: \"300\", label: \"300 - Advanced\" },\n  { value: \"400\", label: \"400 - Expert\" }\n]\n\nAUDIENCE_SKILL_LEVELS = [\n  { value: \"beginner\", label: \"Beginner\" },\n  { value: \"intermediate\", label: \"Intermediate\" },\n  { value: \"advanced\", label: \"Advanced\" },\n  { value: \"expert\", label: \"Expert\" }\n]\n\nTARGET_AUDIENCE_ROLES = [\n  { value: \"solutions-architect\", label: \"Solutions Architect\" },\n  { value: \"developer\", label: \"Developer\" },\n  // ...\n]\n\nCOURSE_DURATIONS = [\n  { value: \"4-hours\", label: \"4 Hours\" },\n  { value: \"8-hours\", label: \"8 Hours (1 Day)\" },\n  // ...\n]\n\nAWS_SERVICES = {\n  COMPUTE: [\n    { value: \"ec2\", label: \"Amazon EC2\" },\n    // ...\n  ],\n  // ...\n}\n```\n\n## 11. Data Migration Considerations\n\nThe application has transitioned from PostgreSQL to exclusively using DynamoDB for data storage. The AWS credential storage has similarly moved from database to file-based storage (`.aws-credentials.json`).\n\n### 11.1 DynamoDB Tables\n\nThe following DynamoDB tables replace the PostgreSQL schema:\n\n| Table Name   | Partition Key | Sort Key | Global Secondary Indexes                     |\n|---------------|----------------|----------|--------------------------------------------|\n| Users        | id             | -        | username-index                             |\n| Projects     | id             | -        | userId-index, status-index                 |\n| StageContents | id             | -        input, required</td>\n    <td>[\"\"]</td>\n  </tr>\n</table>\n\n<h4>4.2.2 API Endpoints</h4>\n\n<table>\n  <tr>\n    <th>Method</th>\n    <th>Endpoint</th>\n    <th>Purpose</th>\n    <th>Required Fields</th>\n  </tr>\n  <tr>\n    <td>GET</td>\n    <td>/api/projects/${projectId}</td>\n    <td>Fetch audience info</td>\n    <td>None</td>\n  </tr>\n  <tr>\n    <td>PUT</td>\n    <td>/api/projects/${id}</td>\n    <td>Update existing audience info</td>\n    <td>All fields</td>\n  </tr>\n  <tr>\n    <td>POST</td>\n    <td>/api/ai/generate-learning-objectives</td>\n    <td>Generate AI outcomes</td>\n    <td>title, awsServices, audience, level</td>\n  </tr>\n</table>\n\n<h2>5. Stage 2: Training Needs Analysis - Technical Details</h2>\n\n<h3>5.1 Audience Assessment Station</h3>\n\n<p><strong>Frontend Component:</strong> AudienceAssessmentStation.tsx<br>\n<strong>Database Table:</strong> TrainCraft_StageContents</p>\n\n<h4>5.1.1 Data Structure</h4>\n\n<table>\n  <tr>\n    <th>Field Name</th>\n    <th>Database Field</th>\n    <th>Field Type</th>\n    <th>Validation Rules</th>\n  </tr>\n  <tr>\n    <td>detailedCharacteristics</td>\n    <td>detailedCharacteristics (String)</td>\n    <td>Textarea</td>\n    <td>Required</td>\n  </tr>\n  <tr>\n    <td>priorKnowledge</td>\n    <td>priorKnowledge (List)</td>\n    <td>Dynamic Input List</td>\n    <td>Min 1 input, required</td>\n  </tr>\n  <tr>\n    <td>learningPreferences</td>\n    <td>learningPreferences (List)</td>\n    <td>Multi-select</td>\n    <td>Min 1 selection, required</td>\n  </tr>\n</table>\n\n<h3>5.2 Knowledge Gap Analysis Station</h3>\n\n<p><strong>Frontend Component:</strong> KnowledgeGapStation.tsx<br>\n<strong>Database Table:</strong> TrainCraft_StageContents</p>\n\n<h4>5.2.1 Data Structure</h4>\n\n<table>\n  <tr>\n    <th>Field Name</th>\n    <th>Database Field</th>\n    <th>Field Type</th>\n    <th>Validation Rules</th>\n  </tr>\n  <tr>\n    <td>currentKnowledgeState</td>\n    <td>currentKnowledgeState (String)</td>\n    <td>Textarea</td>\n    <td>Required</td>\n  </tr>\n  <tr>\n    <td>desiredKnowledgeState</td>\n    <td>desiredKnowledgeState (String)</td>\n    <td>Textarea</td>\n    <td>Required</td>\n  </tr>\n  <tr>\n    <td>identifiedGaps</td>\n    <td>identifiedGaps (List)</td>\n    <td>Dynamic Input List</td>\n    <td>Min 1 input, required</td>\n  </tr>\n</table>\n\n<h3>5.3 Business Impact Measurement Station</h3>\n\n<p><strong>Frontend Component:</strong> BusinessImpactStation.tsx<br",
        "PARAMETERS": {
            "temp": 0.5,
            "topP": 1.0,
            "topK": 250,
            "maxT": 200000,
            "stopSeq": ""
        }
    },
    {
        "timestamp": "2025-05-08 09:16:55",
        "MODEL": "Claude 35S",
        "COMMAND": "Take this text and arrange it into columns :\n\nSlide #\nSlide title\nReviewer\nComment\nRecommendation\n27\nAmazon SageMaker Studio & SageMaker Pipelines\nTomita Shuhei\nStudents may wonder whether they should use both the Sagemaker pipeline or the Codepipeline. It would be good to add a comparison document.\n \n",
        "INPUT": "Page 36\n\ufffc\nEyal Tamir\n3/21/2025, 4:54 AM\nIs there a reason why Trainium and Inferentia are not discussed here? Students may want to know when to use also GPU vs Trainium for Training or GPU vs Inferentia for Inference. Consider adding a comparison slide for each. Later, we can decide if to put it as hidden and use it only upon demand in the class.\nReplyResolve\nVersion 1\nPage 13\nIK\nIvan Kopas\n5/6/2025, 10:00 AM\nlet's not talk about competitors. I would remove this part\nReplyResolve\nPage 14\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\na portion of the font seems to be hidden by the icon\nReplyResolve\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nWhile this is true, let's also call out that support for images, text and time-series data is available, as well\nReplyResolve\nPage 15\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nI would replace with AWS SDK for Python (boto3) as a more common alternative to SageMaker AI Python SDK\nReplyResolve\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nLet's remove this since SM Studio only allows us to review the experiments generated by the Autopilot, given that SageMaker Studio Classic has been deprecated. For creating the autopilot jobs and managing them, we have to use SM Canvas now.\nReplyResolve\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nsimilar to the comment above, while this is true, I would replace it or complement it with AWS SDK for Python (boto3) since that's the mechanism builder more commonly use than building the service APIs requests themselves.\nReplyResolve\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nreplace \"triggered\" with \"created\". In my work with other teams across the org, \"triggered\" has increasingly become a forbidden word as a result of the current societal dynamics\nReplyResolve\nPage 17\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nIt'd be great if we can add another layer in the hierarhy to show categorization by learning type (supervised, unsupervised and reinforcement learning) and data type (tabular, images, text, time series).\nReplyResolve\nPage 18\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nLinear learner can also be used for multiclass classification tasks, which is not shown here (only binary classification and regression are shown). https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html\nReplyResolve\nPage 21\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nIt'd be great to list one quick use case for each of these that we can mention. For instance, a use case for K-Means can be to optimize a marketing campaign by clustering all customers into cluster that will be most susceptible to email ads, physical mail ads and text message ads.\n\n\nIncluding a short specific use case for each of these algorithms will make these concepts more approachable and digestible for students.\n\n\nSee 15 examples of use cases here: https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html\nReplyResolve\nPage 22\n\ufffc\nEyal Tamir\n3/21/2025, 4:33 AM\nSlide title should be consistent \"SageMaker AI\". To shorten it consider: \"SageMaker AI built-in Algorithms (Image/Video/Time-Series)\" see if this could fit in a single line.\nReplyResolve\nPage 23\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nCan we include one potential approach in the notes? This will be helpful to both the ramping instructors as well as students that rely on the ebook because they missed a part of the class\nReplyResolve\nPage 29\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nTransformers are a type of a neural net architecture, like CNNs, RNNs, GANs, etc.\nReplyResolve\nPage 31\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nWhat's MediScan? Did we copy this from somewhere else? The name of our company is AnyCompany",
        "RESPONSE": "Here's the text arranged into columns:\n\nSlide # | Slide title | Reviewer | Comment | Recommendation\n--------|-------------|----------|---------|------------------\n27 | Amazon SageMaker Studio & SageMaker Pipelines | Tomita Shuhei | Students may wonder whether they should use both the Sagemaker pipeline or the Codepipeline. It would be good to add a comparison document. | \n36 | | Eyal Tamir | Is there a reason why Trainium and Inferentia are not discussed here? Students may want to know when to use also GPU vs Trainium for Training or GPU vs Inferentia for Inference. Consider adding a comparison slide for each. Later, we can decide if to put it as hidden and use it only upon demand in the class. | \n13 | | Ivan Kopas | let's not talk about competitors. I would remove this part | \n14 | | Ivan Kopas | a portion of the font seems to be hidden by the icon | \n14 | | Ivan Kopas | While this is true, let's also call out that support for images, text and time-series data is available, as well | \n15 | | Ivan Kopas | I would replace with AWS SDK for Python (boto3) as a more common alternative to SageMaker AI Python SDK | \n15 | | Ivan Kopas | Let's remove this since SM Studio only allows us to review the experiments generated by the Autopilot, given that SageMaker Studio Classic has been deprecated. For creating the autopilot jobs and managing them, we have to use SM Canvas now. | \n15 | | Ivan Kopas | similar to the comment above, while this is true, I would replace it or complement it with AWS SDK for Python (boto3) since that's the mechanism builder more commonly use than building the service APIs requests themselves. | \n15 | | Ivan Kopas | replace \"triggered\" with \"created\". In my work with other teams across the org, \"triggered\" has increasingly become a forbidden word as a result of the current societal dynamics | \n17 | | Ivan Kopas | It'd be great if we can add another layer in the hierarhy to show categorization by learning type (supervised, unsupervised and reinforcement learning) and data type (tabular, images, text, time series). | \n18 | | Ivan Kopas | Linear learner can also be used for multiclass classification tasks, which is not shown here (only binary classification and regression are shown). https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html | \n21 | | Ivan Kopas | It'd be great to list one quick use case for each of these that we can mention. For instance, a use case for K-Means can be to optimize a marketing campaign by clustering all customers into cluster that will be most susceptible to email ads, physical mail ads and text message ads. Including a short specific use case for each of these algorithms will make these concepts more approachable and digestible for students. See 15 examples of use cases here: https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html | \n22 | | Eyal Tamir | Slide title should be consistent \"SageMaker AI\". To shorten it consider: \"SageMaker AI built-in Algorithms (Image/Video/Time-Series)\" see if this could fit in a single line. | \n23 | | Ivan Kopas | Can we include one potential approach in the notes? This will be helpful to both the ramping instructors as well as students that rely on the ebook because they missed a part of the class | \n29 | | Ivan Kopas | Transformers are a type of a neural net architecture, like CNNs, RNNs, GANs, etc. | \n31 | | Ivan Kopas | What's MediScan? Did we copy this from somewhere else? The name of our company is AnyCompany | ",
        "PARAMETERS": {
            "temp": 0.5,
            "topP": 1.0,
            "topK": 250,
            "maxT": 200000,
            "stopSeq": ""
        }
    },
    {
        "timestamp": "2025-05-08 09:17:47",
        "MODEL": "Nova Pro",
        "COMMAND": "Take this text and arrange it into columns (add markdown):\n\nSlide #\nSlide title\nReviewer\nComment\nRecommendation\n27\nAmazon SageMaker Studio & SageMaker Pipelines\nTomita Shuhei\nStudents may wonder whether they should use both the Sagemaker pipeline or the Codepipeline. It would be good to add a comparison document.\n \n",
        "INPUT": "Page 36\n\ufffc\nEyal Tamir\n3/21/2025, 4:54 AM\nIs there a reason why Trainium and Inferentia are not discussed here? Students may want to know when to use also GPU vs Trainium for Training or GPU vs Inferentia for Inference. Consider adding a comparison slide for each. Later, we can decide if to put it as hidden and use it only upon demand in the class.\nReplyResolve\nVersion 1\nPage 13\nIK\nIvan Kopas\n5/6/2025, 10:00 AM\nlet's not talk about competitors. I would remove this part\nReplyResolve\nPage 14\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\na portion of the font seems to be hidden by the icon\nReplyResolve\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nWhile this is true, let's also call out that support for images, text and time-series data is available, as well\nReplyResolve\nPage 15\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nI would replace with AWS SDK for Python (boto3) as a more common alternative to SageMaker AI Python SDK\nReplyResolve\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nLet's remove this since SM Studio only allows us to review the experiments generated by the Autopilot, given that SageMaker Studio Classic has been deprecated. For creating the autopilot jobs and managing them, we have to use SM Canvas now.\nReplyResolve\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nsimilar to the comment above, while this is true, I would replace it or complement it with AWS SDK for Python (boto3) since that's the mechanism builder more commonly use than building the service APIs requests themselves.\nReplyResolve\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nreplace \"triggered\" with \"created\". In my work with other teams across the org, \"triggered\" has increasingly become a forbidden word as a result of the current societal dynamics\nReplyResolve\nPage 17\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nIt'd be great if we can add another layer in the hierarhy to show categorization by learning type (supervised, unsupervised and reinforcement learning) and data type (tabular, images, text, time series).\nReplyResolve\nPage 18\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nLinear learner can also be used for multiclass classification tasks, which is not shown here (only binary classification and regression are shown). https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html\nReplyResolve\nPage 21\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nIt'd be great to list one quick use case for each of these that we can mention. For instance, a use case for K-Means can be to optimize a marketing campaign by clustering all customers into cluster that will be most susceptible to email ads, physical mail ads and text message ads.\n\n\nIncluding a short specific use case for each of these algorithms will make these concepts more approachable and digestible for students.\n\n\nSee 15 examples of use cases here: https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html\nReplyResolve\nPage 22\n\ufffc\nEyal Tamir\n3/21/2025, 4:33 AM\nSlide title should be consistent \"SageMaker AI\". To shorten it consider: \"SageMaker AI built-in Algorithms (Image/Video/Time-Series)\" see if this could fit in a single line.\nReplyResolve\nPage 23\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nCan we include one potential approach in the notes? This will be helpful to both the ramping instructors as well as students that rely on the ebook because they missed a part of the class\nReplyResolve\nPage 29\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nTransformers are a type of a neural net architecture, like CNNs, RNNs, GANs, etc.\nReplyResolve\nPage 31\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nWhat's MediScan? Did we copy this from somewhere else? The name of our company is AnyCompany",
        "RESPONSE": "Certainly! Here's the text arranged into columns using markdown:\n\n```markdown\n| Slide # | Slide Title                                                                 | Reviewer         | Comment                                                                                                                                                                                                                           | Recommendation                                                                                     |\n|---------|-----------------------------------------------------------------------------|------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------|\n| 27      | Amazon SageMaker Studio & SageMaker Pipelines                               | Tomita Shuhei    | Students may wonder whether they should use both the Sagemaker pipeline or the Codepipeline. It would be good to add a comparison document.                                                                                       |                                                                                                    |\n| Page 36 |                                                                             | Eyal Tamir       | 3/21/2025, 4:54 AM <br> Is there a reason why Trainium and Inferentia are not discussed here? Students may want to know when to use also GPU vs Trainium for Training or GPU vs Inferentia for Inference.                          | Consider adding a comparison slide for each. Later, we can decide if to put it as hidden and use it only upon demand in the class. |\n| Page 13 |                                                                             | Ivan Kopas (IK)  | 5/6/2025, 10:00 AM <br> let's not talk about competitors. I would remove this part                                                                                                                                               |                                                                                                    |\n| Page 14 |                                                                             | Ivan Kopas (IK)  | 3/19/2025, 4:59 PM <br> a portion of the font seems to be hidden by the icon                                                                                                                                                     |                                                                                                    |\n|         |                                                                             | Ivan Kopas (IK)  | 3/19/2025, 4:59 PM <br> While this is true, let's also call out that support for images, text and time-series data is available, as well                                                                                          |                                                                                                    |\n| Page 15 |                                                                             | Ivan Kopas (IK)  | 3/19/2025, 4:59 PM <br> I would replace with AWS SDK for Python (boto3) as a more common alternative to SageMaker AI Python SDK                                                                                                   |                                                                                                    |\n|         |                                                                             | Ivan Kopas (IK)  | 3/19/2025, 4:59 PM <br> Let's remove this since SM Studio only allows us to review the experiments generated by the Autopilot, given that SageMaker Studio Classic has been deprecated. For creating the autopilot jobs and managing them, we have to use SM Canvas now. |                                                                                                    |\n|         |                                                                             | Ivan Kopas (IK)  | 3/19/2025, 4:59 PM <br> similar to the comment above, while this is true, I would replace it or complement it with AWS SDK for Python (boto3) since that's the mechanism builder more commonly use than building the service APIs requests themselves. |                                                                                                    |\n|         |                                                                             | Ivan Kopas (IK)  | 3/19/2025, 4:59 PM <br> replace \"triggered\" with \"created\". In my work with other teams across the org, \"triggered\" has increasingly become a forbidden word as a result of the current societal dynamics                         |                                                                                                    |\n| Page 17 |                                                                             | Ivan Kopas (IK)  | 3/19/2025, 4:59 PM <br> It'd be great if we can add another layer in the hierarchy to show categorization by learning type (supervised, unsupervised and reinforcement learning) and data type (tabular, images, text, time series). |                                                                                                    |\n| Page 18 |                                                                             | Ivan Kopas (IK)  | 3/19/2025, 4:59 PM <br> Linear learner can also be used for multiclass classification tasks, which is not shown here (only binary classification and regression are shown). https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html |                                                                                                    |\n| Page 21 |                                                                             | Ivan Kopas (IK)  | 3/19/2025, 4:59 PM <br> It'd be great to list one quick use case for each of these that we can mention. For instance, a use case for K-Means can be to optimize a marketing campaign by clustering all customers into cluster that will be most susceptible to email ads, physical mail ads and text message ads. |                                                                                                    |\n|         |                                                                             |                  | Including a short specific use case for each of these algorithms will make these concepts more approachable and digestible for students. <br> See 15 examples of use cases here: https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html |                                                                                                    |\n| Page 22 |                                                                             | Eyal Tamir       | 3/21/2025, 4:33 AM <br> Slide title should be consistent \"SageMaker AI\". To shorten it consider: \"SageMaker AI built-in Algorithms (Image/Video/Time-Series)\" see if this could fit in a single line.                              |                                                                                                    |\n| Page 23 |                                                                             | Ivan Kopas (IK)  | 3/19/2025, 4:59 PM <br> Can we include one potential approach in the notes? This will be helpful to both the ramping instructors as well as students that rely on the ebook because they missed a part of the class              |                                                                                                    |\n| Page 29 |                                                                             | Ivan Kopas (IK)  | 3/19/2025, 4:59 PM <br> Transformers are a type of a neural net architecture, like CNNs, RNNs, GANs, etc.                                                                                                                         |                                                                                                    |\n| Page 31 |                                                                             | Ivan Kopas (IK)  | 3/19/2025, 4:59 PM <br> What's MediScan? Did we copy this from somewhere else? The name of our company is AnyCompany                                                                                                             |                                                                                                    |\n```\n\nNote: Some cells are left blank where there was no specific information provided in the original text.\n",
        "PARAMETERS": {
            "max_new_tokens": "2000",
            "top_p": "0.9",
            "top_k": "20",
            "temperature": "0.7"
        }
    },
    {
        "timestamp": "2025-05-08 09:25:36",
        "MODEL": "Nova Pro",
        "COMMAND": "Take this text and arrange it into columns (add markdown)\n\nMake sure the slide number just takes the number from the page number.\n\n\nSlide #\nSlide title\nReviewer\nComment\nRecommendation\n27\nAmazon SageMaker Studio & SageMaker Pipelines\nTomita Shuhei\nStudents may wonder whether they should use both the Sagemaker pipeline or the Codepipeline. It would be good to add a comparison document.\n \n",
        "INPUT": "Settings244\n\ufffc\nRecentsMy FilesTeam SpacesSharedFavorites\nRI\nMLMLEA_Mod05_Choosing_a_Modeling_Approach_InstructorDeck.pdf\nv3 of 3\n\n\nComments\n\nAuto Refresh Comments\n\n\nAdvanced\nS\n\nFilter Comments\nVersion 2\nPage 36\n\ufffc\nEyal Tamir\n3/21/2025, 4:54 AM\nIs there a reason why Trainium and Inferentia are not discussed here? Students may want to know when to use also GPU vs Trainium for Training or GPU vs Inferentia for Inference. Consider adding a comparison slide for each. Later, we can decide if to put it as hidden and use it only upon demand in the class.\nReplyResolve\nVersion 1\nPage 13\nIK\nIvan Kopas\n5/6/2025, 10:00 AM\nlet's not talk about competitors. I would remove this part\nReplyResolve\nPage 14\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\na portion of the font seems to be hidden by the icon\nReplyResolve\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nWhile this is true, let's also call out that support for images, text and time-series data is available, as well\nReplyResolve\nPage 15\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nI would replace with AWS SDK for Python (boto3) as a more common alternative to SageMaker AI Python SDK\nReplyResolve\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nLet's remove this since SM Studio only allows us to review the experiments generated by the Autopilot, given that SageMaker Studio Classic has been deprecated. For creating the autopilot jobs and managing them, we have to use SM Canvas now.\nReplyResolve\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nsimilar to the comment above, while this is true, I would replace it or complement it with AWS SDK for Python (boto3) since that's the mechanism builder more commonly use than building the service APIs requests themselves.\nReplyResolve\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nreplace \"triggered\" with \"created\". In my work with other teams across the org, \"triggered\" has increasingly become a forbidden word as a result of the current societal dynamics\nReplyResolve\nPage 17\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nIt'd be great if we can add another layer in the hierarhy to show categorization by learning type (supervised, unsupervised and reinforcement learning) and data type (tabular, images, text, time series).\nReplyResolve\nPage 18\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nLinear learner can also be used for multiclass classification tasks, which is not shown here (only binary classification and regression are shown). https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html\nReplyResolve\nPage 21\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nIt'd be great to list one quick use case for each of these that we can mention. For instance, a use case for K-Means can be to optimize a marketing campaign by clustering all customers into cluster that will be most susceptible to email ads, physical mail ads and text message ads.\n\n\nIncluding a short specific use case for each of these algorithms will make these concepts more approachable and digestible for students.\n\n\nSee 15 examples of use cases here: https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html\nReplyResolve\nPage 22\n\ufffc\nEyal Tamir\n3/21/2025, 4:33 AM\nSlide title should be consistent \"SageMaker AI\". To shorten it consider: \"SageMaker AI built-in Algorithms (Image/Video/Time-Series)\" see if this could fit in a single line.\nReplyResolve\nPage 23\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nCan we include one potential approach in the notes? This will be helpful to both the ramping instructors as well as students that rely on the ebook because they missed a part of the class\nReplyResolve\nPage 29\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nTransformers are a type of a neural net architecture, like CNNs, RNNs, GANs, etc.\nReplyResolve\nPage 31\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nWhat's MediScan? Did we copy this from somewhere else? The name of our company is AnyCompany",
        "RESPONSE": "Certainly! Here's the text arranged into columns using Markdown format:\n\n```markdown\n| Slide # | Slide Title                                                                 | Reviewer         | Comment                                                                                                                                                                                                                           | Recommendation                                                                                     |\n|---------|-----------------------------------------------------------------------------|------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------|\n| 27      | Amazon SageMaker Studio & SageMaker Pipelines                               | Tomita Shuhei    | Students may wonder whether they should use both the Sagemaker pipeline or the Codepipeline. It would be good to add a comparison document.                                                                                       |                                                                                                    |\n| 36      |                                                                             | Eyal Tamir       | Is there a reason why Trainium and Inferentia are not discussed here? Students may want to know when to use also GPU vs Trainium for Training or GPU vs Inferentia for Inference. Consider adding a comparison slide for each.   |                                                                                                    |\n| 13      |                                                                             | Ivan Kopas       | let's not talk about competitors. I would remove this part                                                                                                                                                                        |                                                                                                    |\n| 14      |                                                                             | Ivan Kopas       | a portion of the font seems to be hidden by the icon                                                                                                                                                                              |                                                                                                    |\n| 14      |                                                                             | Ivan Kopas       | While this is true, let's also call out that support for images, text and time-series data is available, as well                                                                                                                  |                                                                                                    |\n| 15      |                                                                             | Ivan Kopas       | I would replace with AWS SDK for Python (boto3) as a more common alternative to SageMaker AI Python SDK                                                                                                                           |                                                                                                    |\n| 15      |                                                                             | Ivan Kopas       | Let's remove this since SM Studio only allows us to review the experiments generated by the Autopilot, given that SageMaker Studio Classic has been deprecated. For creating the autopilot jobs and managing them, we have to use SM Canvas now. |                                                                                                    |\n| 15      |                                                                             | Ivan Kopas       | similar to the comment above, while this is true, I would replace it or complement it with AWS SDK for Python (boto3) since that's the mechanism builder more commonly use than building the service APIs requests themselves.    |                                                                                                    |\n| 15      |                                                                             | Ivan Kopas       | replace \"triggered\" with \"created\". In my work with other teams across the org, \"triggered\" has increasingly become a forbidden word as a result of the current societal dynamics                                                 |                                                                                                    |\n| 17      |                                                                             | Ivan Kopas       | It'd be great if we can add another layer in the hierarchy to show categorization by learning type (supervised, unsupervised and reinforcement learning) and data type (tabular, images, text, time series).                       |                                                                                                    |\n| 18      |                                                                             | Ivan Kopas       | Linear learner can also be used for multiclass classification tasks, which is not shown here (only binary classification and regression are shown). https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html            |                                                                                                    |\n| 21      |                                                                             | Ivan Kopas       | It'd be great to list one quick use case for each of these that we can mention. For instance, a use case for K-Means can be to optimize a marketing campaign by clustering all customers into cluster that will be most susceptible to email ads, physical mail ads and text message ads. |                                                                                                    |\n| 22      |                                                                             | Eyal Tamir       | Slide title should be consistent \"SageMaker AI\". To shorten it consider: \"SageMaker AI built-in Algorithms (Image/Video/Time-Series)\" see if this could fit in a single line.                                                      |                                                                                                    |\n| 23      |                                                                             | Ivan Kopas       | Can we include one potential approach in the notes? This will be helpful to both the ramping instructors as well as students that rely on the ebook because they missed a part of the class                                        |                                                                                                    |\n| 29      |                                                                             | Ivan Kopas       | Transformers are a type of a neural net architecture, like CNNs, RNNs, GANs, etc.                                                                                                                                                  |                                                                                                    |\n| 31      |                                                                             | Ivan Kopas       | What's MediScan? Did we copy this from somewhere else? The name of our company is AnyCompany                                                                                                                                      |                                                                                                    |\n```\n\n\nNote: The slide numbers have been extracted from the page numbers provided in the text.\n",
        "PARAMETERS": {
            "max_new_tokens": "2000",
            "top_p": "0.9",
            "top_k": "20",
            "temperature": "0.7"
        }
    },
    {
        "timestamp": "2025-05-08 09:26:39",
        "MODEL": "Nova Pro",
        "COMMAND": "Take this text and arrange it into columns (add markdown)\n\nMake sure the slide number just takes the number from the page number.  The notes appear as multiple lines, however they are each one block of notes per slide.\n\n\nSlide #\nSlide title\nReviewer\nComment\nRecommendation\n27\nAmazon SageMaker Studio & SageMaker Pipelines\nTomita Shuhei\nStudents may wonder whether they should use both the Sagemaker pipeline or the Codepipeline. It would be good to add a comparison document.\n \n",
        "INPUT": "Settings244\n\ufffc\nRecentsMy FilesTeam SpacesSharedFavorites\nRI\nMLMLEA_Mod05_Choosing_a_Modeling_Approach_InstructorDeck.pdf\nv3 of 3\n\n\nComments\n\nAuto Refresh Comments\n\n\nAdvanced\nS\n\nFilter Comments\nVersion 2\nPage 36\n\ufffc\nEyal Tamir\n3/21/2025, 4:54 AM\nIs there a reason why Trainium and Inferentia are not discussed here? Students may want to know when to use also GPU vs Trainium for Training or GPU vs Inferentia for Inference. Consider adding a comparison slide for each. Later, we can decide if to put it as hidden and use it only upon demand in the class.\nReplyResolve\nVersion 1\nPage 13\nIK\nIvan Kopas\n5/6/2025, 10:00 AM\nlet's not talk about competitors. I would remove this part\nReplyResolve\nPage 14\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\na portion of the font seems to be hidden by the icon\nReplyResolve\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nWhile this is true, let's also call out that support for images, text and time-series data is available, as well\nReplyResolve\nPage 15\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nI would replace with AWS SDK for Python (boto3) as a more common alternative to SageMaker AI Python SDK\nReplyResolve\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nLet's remove this since SM Studio only allows us to review the experiments generated by the Autopilot, given that SageMaker Studio Classic has been deprecated. For creating the autopilot jobs and managing them, we have to use SM Canvas now.\nReplyResolve\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nsimilar to the comment above, while this is true, I would replace it or complement it with AWS SDK for Python (boto3) since that's the mechanism builder more commonly use than building the service APIs requests themselves.\nReplyResolve\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nreplace \"triggered\" with \"created\". In my work with other teams across the org, \"triggered\" has increasingly become a forbidden word as a result of the current societal dynamics\nReplyResolve\nPage 17\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nIt'd be great if we can add another layer in the hierarhy to show categorization by learning type (supervised, unsupervised and reinforcement learning) and data type (tabular, images, text, time series).\nReplyResolve\nPage 18\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nLinear learner can also be used for multiclass classification tasks, which is not shown here (only binary classification and regression are shown). https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html\nReplyResolve\nPage 21\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nIt'd be great to list one quick use case for each of these that we can mention. For instance, a use case for K-Means can be to optimize a marketing campaign by clustering all customers into cluster that will be most susceptible to email ads, physical mail ads and text message ads.\n\n\nIncluding a short specific use case for each of these algorithms will make these concepts more approachable and digestible for students.\n\n\nSee 15 examples of use cases here: https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html\nReplyResolve\nPage 22\n\ufffc\nEyal Tamir\n3/21/2025, 4:33 AM\nSlide title should be consistent \"SageMaker AI\". To shorten it consider: \"SageMaker AI built-in Algorithms (Image/Video/Time-Series)\" see if this could fit in a single line.\nReplyResolve\nPage 23\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nCan we include one potential approach in the notes? This will be helpful to both the ramping instructors as well as students that rely on the ebook because they missed a part of the class\nReplyResolve\nPage 29\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nTransformers are a type of a neural net architecture, like CNNs, RNNs, GANs, etc.\nReplyResolve\nPage 31\nIK\nIvan Kopas\n3/19/2025, 4:59 PM\nWhat's MediScan? Did we copy this from somewhere else? The name of our company is AnyCompany",
        "RESPONSE": "Certainly! Here's the text arranged into columns using markdown format:\n\n```markdown\n| Slide # | Slide Title                                                                 | Reviewer         | Comment                                                                                                                                                                                                                                                                                                                                                           | Recommendation                                                                                     |\n|---------|-----------------------------------------------------------------------------|------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------|\n| 27      | Amazon SageMaker Studio & SageMaker Pipelines                               | Tomita Shuhei    | Students may wonder whether they should use both the Sagemaker pipeline or the Codepipeline. It would be good to add a comparison document.                                                                                                                                                                                                                       |                                                                                                    |\n| 36      |                                                                             | Eyal Tamir       | Is there a reason why Trainium and Inferentia are not discussed here? Students may want to know when to use also GPU vs Trainium for Training or GPU vs Inferentia for Inference. Consider adding a comparison slide for each. Later, we can decide if to put it as hidden and use it only upon demand in the class.                                                |                                                                                                    |\n| 13      |                                                                             | Ivan Kopas       | let's not talk about competitors. I would remove this part                                                                                                                                                                                                                                                                                                        |                                                                                                    |\n| 14      |                                                                             | Ivan Kopas       | a portion of the font seems to be hidden by the icon                                                                                                                                                                                                                                                                                                              |                                                                                                    |\n|         |                                                                             | Ivan Kopas       | While this is true, let's also call out that support for images, text and time-series data is available, as well                                                                                                                                                                                                                                                  |                                                                                                    |\n| 15      |                                                                             | Ivan Kopas       | I would replace with AWS SDK for Python (boto3) as a more common alternative to SageMaker AI Python SDK                                                                                                                                                                                                                                                           |                                                                                                    |\n|         |                                                                             | Ivan Kopas       | Let's remove this since SM Studio only allows us to review the experiments generated by the Autopilot, given that SageMaker Studio Classic has been deprecated. For creating the autopilot jobs and managing them, we have to use SM Canvas now.                                                                                                                   |                                                                                                    |\n|         |                                                                             | Ivan Kopas       | similar to the comment above, while this is true, I would replace it or complement it with AWS SDK for Python (boto3) since that's the mechanism builder more commonly use than building the service APIs requests themselves.                                                                                                                                      |                                                                                                    |\n|         |                                                                             | Ivan Kopas       | replace \"triggered\" with \"created\". In my work with other teams across the org, \"triggered\" has increasingly become a forbidden word as a result of the current societal dynamics                                                                                                                                                                                 |                                                                                                    |\n| 17      |                                                                             | Ivan Kopas       | It'd be great if we can add another layer in the hierarhy to show categorization by learning type (supervised, unsupervised and reinforcement learning) and data type (tabular, images, text, time series).                                                                                                                                                        |                                                                                                    |\n| 18      |                                                                             | Ivan Kopas       | Linear learner can also be used for multiclass classification tasks, which is not shown here (only binary classification and regression are shown). https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html                                                                                                                                             |                                                                                                    |\n| 21      |                                                                             | Ivan Kopas       | It'd be great to list one quick use case for each of these that we can mention. For instance, a use case for K-Means can be to optimize a marketing campaign by clustering all customers into cluster that will be most susceptible to email ads, physical mail ads and text message ads. Including a short specific use case for each of these algorithms will make these concepts more approachable and digestible for students. See 15 examples of use cases here: https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html |                                                                                                    |\n| 22      |                                                                             | Eyal Tamir       | Slide title should be consistent \"SageMaker AI\". To shorten it consider: \"SageMaker AI built-in Algorithms (Image/Video/Time-Series)\" see if this could fit in a single line.                                                                                                                                                                                      |                                                                                                    |\n| 23      |                                                                             | Ivan Kopas       | Can we include one potential approach in the notes? This will be helpful to both the ramping instructors as well as students that rely on the ebook because they missed a part of the class                                                                                                                                                                       |                                                                                                    |\n| 29      |                                                                             | Ivan Kopas       | Transformers are a type of a neural net architecture, like CNNs, RNNs, GANs, etc.                                                                                                                                                                                                                                                                                  |                                                                                                    |\n| 31      |                                                                             | Ivan Kopas       | What's MediScan? Did we copy this from somewhere else? The name of our company is AnyCompany                                                                                                                                                                                                                                                                      |                                                                                                    |\n```\n\n\nThis table organizes the information into clear columns for easy reading and reference.\n",
        "PARAMETERS": {
            "max_new_tokens": "2000",
            "top_p": "0.9",
            "top_k": "20",
            "temperature": "0.7"
        }
    },
    {
        "timestamp": "2025-05-08 12:35:51",
        "MODEL": "Claude 35S",
        "COMMAND": "This is a complex prompt. Take your time to read through the entire prompt and understand every writing guideline and format required. \n\nOnce you are sure of the requirements, proceed.\n\nDo not use Amazon Bedrock cached data.\n\n[Role]\nYou are an AWS Technical Curriculum Developer creating slide notes for a presentation.\nRead the text in the INPUT. This is a text version of a PowerPoint slide presentation.   \nAll text in the INPUT must be treated as content. Read and unsderstand the contents of every slide\n\nCarefully identify the individual slide and \"Slide: nn\" followed by the slide title on the next line.\nIn youre response, the format will be Slide nn: Slide title consistently for every slide.\n\n[variables]\nTarget_audience: Machine learning engineers\nInstructional_level: 200-300 \nSpecific_AWS_services: Amazon SageMaker AI\n/variables\n\n[Task]\nPerform this work ONLY for slides 1-10. \nWhen you complete the last slide in this range, stop processing.\n\nYour audience is {{Target_audience}} who are {{Instructional_level}} in {{Specific_AWS_services}}.\n\nGenerate concise student notes for each PowerPoint slide that:\n1. Captures the essential concepts (maximum 3-4 sentences)\n2. Do not reveal information from upcoming slides\n3. Provide context only for THIS slide's content\n4. Use clear, direct language without unnecessary elaboration\n5. Include only relevant examples that reinforce the current slide's concepts\n\nUse existing notes and slide content as your primary source of information.\nExpand on existing slide notes when required, but do not use verbose language.\nFor long Student Notes, edit to 500 words maximum for each slide's student notes.\n\n[Output]\nThe complete SPEAKERNOTES: for each slide explicitly following the format and example below.  Every slide must contain every section of the SPEAKERNOTES.\n\n[Guidelines]\n1.  Your response shall be the entire notes text for all the slides specified.\n2.  FOLLOW the FORMAT explicitly and consistently for every slide.\n3.  Write clearly and concisely, ensuring technical accuracy. \n4.  Reference https://docs.aws.amazon.com for {{Specific_AWS_services}} details. \n5.  Provide valuable insights beyond what's visible on the slide. \n6.  Include transitional statements when changing topics, especially when moving from one section to the next. \n7.  Write in second person only.  Only use \"we\" to describe Amazon or AWs.\n8.  Add **bold** or *italics* markdown as in the examples to add emphasis to the student notes.\n\nFor Image Description, create a brief, accurate description for the image on this slide that:\n1. Describes what the image actually shows (no assumptions)\n2. Explains how the image directly relates to the slide's key point\n3. Uses no more than 2 sentences\n4. Avoids explanations that should be in the main student notes\n5. Begins with \"This image shows...\" or \"This diagram illustrates...\"\n\nDO NOT make statements like \"This slide is about x\" or \"This slide does that\" or \"This slide introduces...\" or \"This slide demonstrates...\"\n\n[FORMAT]\nSlide nn: Slide Title [in exactly this format]\nSPEAKERNOTES:\n~Developer Notes:\n~[Reference materials and URLs - live, accessible, relevant links]\n~\n~Image Description:\n~[Detaied visual description of the slide content or images.]\n~\n|INSTRUCTOR NOTES:\n- |[Key topics to cover]\n- |[Key topics to cover]\n- |...\n|\n|STUDENT NOTES:\n[3-4 clear, concise, and complete paragraphs expanding on slide content. Use **bold** formatting for key terms and important concepts.]\n[300 words maximum]\n\n[EXAMPLE]\nSlide 1: Introduction to AWS Generative AI and ML\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/whitepapers/latest/aws-overview/machine-learning.html\n~\n~Image Description:\n~A diagram showing the AWS ML stack, including SageMaker, Comprehend, and Rekognition\n~\n|INSTRUCTOR NOTES:\n- |Introduce AWS Generative AI and ML services\n- |Highlight key benefits and use cases\n- |Discuss integration with other AWS services\n|\n|STUDENT NOTES:\n**AWS Generative AI and Machine Learning** services provide powerful tools for creating intelligent applications. These services include **Amazon SageMaker** for end-to-end ML workflows, **Amazon Comprehend** for natural language processing, and **Amazon Rekognition** for image and video analysis.\n\n**Generative AI** models, such as those powering **Amazon Bedrock**, can create new content like text, images, and code based on input prompts. This technology opens up exciting possibilities for content creation, language translation, and creative assistance across various industries.\n\nAWS ML services integrate seamlessly with other AWS offerings, allowing for **scalable**, *cost-effective*, and *secure* AI/ML solutions. By leveraging the AWS cloud infrastructure, businesses can easily deploy and manage ML models, process large datasets, and incorporate AI capabilities into their applications without the need for extensive in-house expertise or infrastructure.\n\n",
        "INPUT": "Slide: 1\nModule 3: Data Transformation and Feature Engineering\nMachine Learning Engineering on AWS\nSpeaker Notes:\n|Student notes\nAt this point, you are familiar with data collection, ingestion, and storage. You've covered data fundamentals such as data types and formats, and you've been introduced to core AWS storage services. You can choose the most effective AWS storage decision and data format based on your specific machine learning (ML) needs. \nIn this module, you\u2019ll learn about data transformation. This module covers various transformation concepts and techniques, such as data cleaning, encoding, and feature engineering. You\u2019ll discover how to use AWS services such as Amazon SageMaker Feature Store, Amazon SageMaker Data Wrangler, and AWS Glue to transform your data.\nSlide: 2\nModule objectives & outline\nOn completion, you will be able to:\nExplain the value of data cleaning and transformation.\nDescribe how to process incorrect or duplicated data.\nDescribe how to detect and treat outliers.\nDescribe how to process missing values.\nDescribe fundamental encoding techniques.\nIdentify feature engineering use cases.\nDescribe fundamental concepts, benefits, and techniques of feature engineering.\nDescribe fundamental feature selection techniques.\nDescribe AWS services for validating and labeling data.\nIdentify AWS tools and services for visualizing and transforming data.\nSpeaker Notes:\n|Student Notes\nSlide: 3\nData processing: Data transformation and feature engineering\nBusiness goals\nML problem framing\nData processing\nModel development\nDeployment\nMonitoring\nSpeaker Notes:\n|Student notes\nDuring this phase of the machine learning lifecycle, you\u2019re processing your data in preparation for model development. By this point in the data preparation process, you\u2019ve collected, ingested, extracted, and stored your data as appropriate for your business\u2019s ML needs. Now, you are ready to transform that data into a clean, encoded, engineered dataset. Data cleaning, categorical encoding, and feature engineering are important techniques you can use to help ensure a more accurate and model-ready dataset.\nSlide: 4\nTechniques for data transformation\nData cleaning\nCategorical encoding\nFeature engineering\nThe label encoding examples in the source are not good ones. For example, by encoding countries with numbers like 1, 2, 3 you are assigning some countries higher numerical weight which will impact the model. Countries should be one-hot encoded so that they have the same weight. A better example for label encoding would be small, medium, large sizes as 1,2,3  since that preserves a relative weighting.  \nSpeaker Notes:\n|Student notes\nCleaning, encoding, and engineering your dataset is essential to produce more accurate and efficient results from your machine learning model.\nData cleaning \nis an important first step before training an ML model. Raw data often contains errors such as missing values, duplicates, and outliers that need to be identified and handled to ensure data accuracy and quality. \nExample: A few customer IDs are duplicated in a dataset. \nSolution: Remove or consolidate duplicated customer IDs. \nCategorical encoding \nconverts categorical variables into numeric representations that ML algorithms can mathematically interpret. Common encoding techniques include one-hot encoding and label encoding. \nExample: Data includes a country feature with values like USA, Canada, and UK.\nSolution: Encode the countries as numbers, like 1, 2, and 3, so the ML algorithm can mathematically compare them.\nFeature engineering \ncreates new features or selects relevant features to help ML algorithms better understand the relationships in the data. Methods such as binning, bag-of-words, and N-gram extract key information from text data. \nExample: Train a model to predict whether a review is positive or negative.\nSolution: Use the bag-of-words method to convert text into counts of word occurrences and represent text data in a numeric form. \nSlide: 5\nAgenda\nTopic A: Handling incorrect, duplicated, and missing data\nTopic B: Feature engineering concepts\nTopic C: Numeric and text feature engineering\nTopic D: Feature selection techniques\nTopic E: AWS data transformation services\nSpeaker Notes:\nSlide: 6\nHandling incorrect, duplicated, and missing data\nTopic A: Handling incorrect, duplicated, and missing data\nTopic B: Feature engineering concepts\nTopic C: Numeric and text feature engineering\nTopic D: Feature selection techniques\nTopic E: AWS data transformation services\nSpeaker Notes:\n|Student notes\nOne of the first steps of preparing your dataset after collection, ingestion, and storage is cleaning up your data. This can be a time-consuming process, but without taking care of flaws in your data, your machine learning (ML) model will not function properly.\nSlide: 7\nCleaning data for ML training\nIncorrect or duplicate data\nData outliers\nIncomplete or missing data\nSpeaker Notes:\n|Student notes\nThe goal for data cleaning is to standardize your dataset. You want to make sure the data is in the language and format that your model expects. Part of this standardization involves checking your dataset for incorrect or duplicated data.\nAnother key method of cleaning up your data is to identify data outliers. Outliers are data points that differ significantly from the majority of the data. \nOutliers in your data can present experimental anomalies or even highlight novel discoveries, so they are not necessarily data points you always want to remove. However, they can make it harder for your model to make accurate predictions. Detecting and examining outliers is important to the data preparation process, because they might be indicating errors in data collection or skewing values.\nIn addition to inspecting your data for incorrect or duplicated information and data outliers, you also want to address any areas with incomplete or missing data. \nSome columns in your dataset might be missing data because of a data collection error. It is also possible that data was not collected on a particular feature until well into the data collection process.\nSlide: 8\nIncorrect data\nExamples\nLanguage differences\nTypos\nInconsistent formats\nTechniques\nInspection\nFilling correct values\nFix formatting\nExclude from dataset\nSpeaker Notes:\n|Student notes\nDatasets often contain incorrect or anomalous data points that need to be addressed before model training. Common issues include language differences, typos from data entry, values that don't conform to expected formats, and more. To fix incorrect data values, inspect your dataset for anomalies. You might need to fill in the correct values, convert invalid formats to valid formats, or exclude the incorrect entries from the dataset to prevent inaccurate model predictions.\nSlide: 9\nDuplicated data\nExamples\nCopies from multiple sources\nIncorrect data entry\nTechniques\nDeduplication\nSpeaker Notes:\n|Student notes\nDuplicated data refers to records in a dataset that contain the same information. Duplicated data can occur when data is gathered from multiple sources or is entered incorrectly. After your data is fully collected, it's important to inspect the data for duplicate records and then delete or consolidate the duplicate entries. When a dataset contains multiple copies of the same data points, it skews the distribution of the data and over-represents certain examples.\nThe process of automating data duplication removal is called \ndeduplication\n. Deduplication works by scanning datasets for duplicate information, retaining one copy of the information, and replacing the other instances with pointers that direct back to the stored copy. Deduplication can drastically increase storage capacity by keeping only unique information in your data repositories.\nSlide: 10\nActivity: Identifying incorrect and duplicated data\nSpeaker Notes:\n|Student notes\nThe research team is working with the following set of data. Which errors do you notice in this dataset? You might notice that there is missing data, inconsistent formatting, or misplaced information. Consider how to identify and fix incorrect and duplicated data.\nSlide: 11\nActivity: Identifying incorrect and duplicated data\nSpeaker Notes:\n|Student notes\nBaccalaur\u00e9at\n \nis incorrect because it is in a different language than the rest of the dataset. Although the data is placed correctly in the \nEducation\n column, the word \nbaccalaur\u00e9at\n is a French word for a high school diploma. The data would need to be translated to English so the dataset has a standard language to process.\nMasters/PhD\n is incorrect because it is placed in the wrong column. \nMasters/PhD\n is a valid answer for the Education column, but is incorrectly placed in the \nState\n column. This data would need to be moved to the Education column, and you would need to impute the missing data for the State column.\nThe last row is an example of duplicated data. You might recognize that the Participant ID of 123456 is the biggest marker for duplication. There is already a row of data that contains 123456 as the ID.  You need to remove this entire row of data to keep data weights balanced and prevent bias.\nSlide: 12\nDetecting outliers by using mean and median\nMean\nMedian\nMiddle value of dataset\nBetter metric when data is skewed\nConsider both natural and artificial outliers.\nAverage of all values\nUseful when data is symmetrical\nAge\nCount\nMedian\nMean\nSpeaker Notes:\n|Student notes\nThere are a number of different methods you can use to detect and measure the impact of outliers in your data. These methods include calculating mean and median and determining if the outlier is natural or artificial. \nThese measures describe the extent to which your data is clustered around some value or position. \nThe \nmean\n is the average of all the values in the dataset. Mean can be a useful method for understanding your data when the data is symmetrical. For example, a symmetrical set of data that contains ages of respondents might reflect that both the mean and median of the dataset is 50 years old. However, if your data contains outliers, the mean can be skewed one way toward those values. In that case, your mean would not serve as an accurate representation of where your values are truly centered.\nThe \nmedian\n is the value in the dataset that divides the values into two equal halves. To calculate the median, you arrange all the numerical values in ascending order and identify the middle value. If your data is skewed or contains outliers, the median tends to provide the better metric for understanding your data as it relates to central tendency. \nFor example, a dataset that contains income level might contain outliers. The mean might skew toward higher or lower values, while the median would provide a more accurate picture of the data's central tendency.\nOutliers can be caused by either natural or artificial factors. A natural outlier is not the result of an artificial error but is reflective of some truth in the data.\nNatural outliers \nare data points that are accurate representations of data, but are extreme variations of the central data points. For example, in a dataset that includes height measurements of individuals, an extremely tall individual would represent a natural outlier.\nArtificial outliers \nare anomalous data points in your dataset due to error or improper data collection. For example, a faulty sensor in a thermometer might produce a body temperature that is unrealistically high or low compared to expected body temperatures. \nSlide: 13\nAddressing incomplete or missing data\nExploration\nIdentify missing values\nDetermine why values are missing\nMissing at Random (MAR)\nMissing Completely at Random (MCAR)\nMissing Not at Random (MNAR)\nTechniques\nDrop rows with missing values\nImpute values\nSpeaker Notes:\n|Student notes\nUnfortunately, most ML algorithms cannot deal with missing values automatically. You need to use human intelligence to replace missing values with something meaningful and relevant to the problem.\nA few missing values might not be an issue. However, if you\u2019re missing many values in one column, you might find it difficult to interpret the relationship between that feature and the target. In other words, it can be hard to understand how a value in a row corresponds to what needs to be predicted by the model. There are some key steps you can take to address incomplete and missing values in your dataset.\nIdentify missing values \n\u2013There are certain Python libraries, such as Pandas, that you can use to check for missing values. You can also use AWS services to assist you in this process. For example, Amazon SageMaker Autopilot provides a number of filling methods to handle missing values, and Amazon SageMaker Data Wrangler is a full-service data preparation tool. You will learn about SageMaker Data Wrangler in more detail later in this course.\nDetermine why values are missing \n\u2013Before you can determine how to treat the missing values, it\u2019s important to investigate which mechanisms caused the missing values. The following are three common types of missing data:\nMissing at Random (MAR): The probability that a data point is missing depends only on the observed data, not the missing data. Example: In a dataset of student test scores, scores are missing for some students who were absent that day. Absence is related to performance. \nMissing Completely at Random (MCAR): The probability that a data point is missing does not depend on the observed or unobserved data. Example: In an employee survey, some people forgot to answer the question about their number of siblings. Their missing sibling data does not depend on any values.\nMissing Not at Random (MNAR): The probability that a data point is missing depends on the missing data itself. Example: In a financial audit, companies with accounting irregularities are less likely to provide complete records. The missing data depends on the sensitive information being withheld.\nDrop missing values \u2013 \nDepending on what is causing your missing values, you will decide to either drop the missing values or impute data into your dataset. One of the most straightforward ways to deal with missing values is to remove the rows of data with missing values. You can accomplish this by using a Pandas function. Dropping rows or columns will make the dataset non-missing. However, the risk of dropping rows and columns is significant. If you have hundreds of rows or columns of data, all of that missing data might cause bias in your model predictions. If you drop too much data, you might not have enough features to feed the model.\nImpute values \n\u2013 Missing values might be related to new features that haven\u2019t included your dataset yet. After you include more data, those missing values might be highly correlated with the new feature. In this case, you would deal with missing values by adding more new features to the dataset. If you determine the values are missing at random, data imputation, or inputting the data into your dataset, is most likely the best option. There are different ways to impute a missing value. One common way to impute missing values is to replace the value with the mean, median, or most frequent value. You would select the mean, median, or most frequent value for categorical variables. You would select the mean or median for numerical variables. Choosing the mean, median, or most frequent value depending on your business problem and data collection procedures. \nWhen there is a missing value, always consider whether those values are missing at random. If they are not, try to investigate what the mechanism is behind the missing value. You can often find more efficient ways to impute those missing values. Remember that a majority of ML algorithms can\u2019t take missing values. You will most likely need to either drop the values or impute data for the ML model to perform.\nSlide: 14\nQuestion 1\nA real estate company is working on a machine learning project where the goal is to predict housing prices. The team has a dataset that contains information on the number of bedrooms, the square footage, and the locations of different houses. Before building a machine learning model, the team wants to prepare and transform the data. \nWhich data preparation technique could convert values for the home's location (with values of urban, suburban, and rural) into numeric representations that can be fed into a machine learning algorithm? \nResponse\nA\nData cleaning\nB\nFeature engineering\nC\nCategorical encoding\nD\nDimensionality reduction\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember which data preparation technique converts string values into numeric representations. \nSlide: 15\nQuestion 1 answer\nA real estate company is working on a machine learning project where the goal is to predict housing prices. The team has a dataset that contains information on the number of bedrooms, the square footage, and the locations of different houses. Before building a machine learning model, the team wants to prepare and transform the data. \nWhich data preparation technique could convert values for the home's location (with values of urban, suburban, and rural) into numeric representations that can be fed into a machine learning algorithm? \nResponse\nA\nData cleaning\nB\nFeature engineering\nC correct\nCategorical encoding\nD\nDimensionality reduction\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nC: Categorical encoding can be used to convert values for the home's location into numeric representations. For example, you could have 0 for urban, 1 for suburban, and 2 for rural. This allows machine learning algorithms to mathematically interpret the data.\nThe incorrect responses are:\nA: Data cleaning focuses on handling issues like missing data and outliers.\nB: Feature engineering focuses on modifying or creating new features from the data, rather than encoding features.\nD: Dimensionality reduction techniques reduce the number of features but don't convert data types.\nSlide: 16\nQuestion 2\nA non-profit organization is gathering data to decide which type of fundraiser their patrons would like to attend. They decide to provide two options for patrons to share their preferences: an online survey and a hard-copy mailer. Some patrons plan to fill out both the online and hard-copy survey to make sure their preference is recorded.\nWhich data cleaning technique will BEST help the company retain unique data points for the most effective machine learning (ML) solution?\nResponse\nA\nReformatting the hard-copy mailer to contain only \"Yes/No\" answers\nB\nInspecting the data for incorrectly entered dates\nC\nIdentifying duplicate data from those who submitted both an online and hard-copy survey\nD\nChecking the datasets for mistyped patron names\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember that duplicate data must be removed from a training dataset to avoid model bias.\nSlide: 17\nQuestion 2 answer\nA non-profit organization is gathering data to decide which type of fundraiser their patrons would like to attend. They decide to provide two options for patrons to share their preferences: an online survey and a hard-copy mailer. Some patrons plan to fill out both the online and hard-copy survey to make sure their preference is recorded.\nWhich data cleaning technique will BEST help the company retain unique data points for the most effective machine learning (ML) solution?\nResponse\nA\nReformatting the hard-copy mailer to contain only \"Yes/No\" answers\nB\nInspecting the data for incorrectly entered dates\nC correct\nIdentifying duplicate data from those who submitted both an online and hard-copy survey\nD\nChecking the datasets for mistyped patron names\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nC: Because the company has data points from multiple formats, there is a likelihood of duplication in their dataset. It's important to identify duplication to prevent the ML solution from overrepresenting certain examples.\nThe incorrect responses are:\nA,B,D: These options would not address the probable duplication in the dataset. Because the company has data points from multiple formats, there is a likelihood of duplication in their dataset. It's important to identify duplication to prevent the ML solution from overrepresenting certain examples.\nSlide: 18\nQuestion 3\nYou are cleaning up a dataset that includes the body weights of patients at a medical clinic. A clinician tells you that they usually see adults, but children occasionally visit the clinic.\nWhich data measurement BEST provides the central tendencies of your data?\nResponse\nA\nYou would choose the mean, because the best measurement of data that might contain outliers is taking the average of all data entries.\nB\nYou would choose the median, because the best measurement of data that might contain outliers is sorting the data and dividing the dataset at the middle variable.\nC\nBecause the data is numerical, you can use any method of measurement to find the average weight of a patient.\nD\nThere is no measurement you can use to find central tendencies of data.\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember the best measurement to find the central tendencies when the data is skewed.\nSlide: 19\nQuestion 3 answer\nYou are cleaning up a dataset that includes the body weights of patients at a medical clinic. A clinician tells you that they usually see adults, but children occasionally visit the clinic.\nWhich data measurement BEST provides the central tendencies of your data?\nResponse\nA\nYou would choose the mean, because the best measurement of data that might contain outliers is taking the average of all data entries.\nB correct\nYou would choose the median, because the best measurement of data that might contain outliers is sorting the data and dividing the dataset at the middle variable.\nC\nBecause the data is numerical, you can use any method of measurement to find the average weight of a patient.\nD\nThere is no measurement you can use to find central tendencies of data.\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nB: Because the clinic mostly sees adults, a few entries of children's weights might skew the average patient weight. In this case, it is best to use median to better understand the central tendencies of your data.\nThe incorrect responses are:\nA,C: Because the clinic mostly sees adults, a few entries of children's weights might skew the average patient weight. The mean of all patient data would skew towards the outlier values.\nD: You can use measurements such as mean and median to find central tendencies of your data.\nSlide: 20\nQuestion 4\nAn apparel company is building a machine learning solution to determine which types of products are not selling well. They've collected data on how long products sit in inventory, but their dataset contains many missing values.\nWhich method is the most effective option for addressing missing values in the apparel company's dataset?\nResponse\nA\nInspecting the reason for the missed values, and dropping, leaving, or imputing data accordingly\nB\nDropping all the data that contains missing values\nC\nManually imputing every missing value\nD\nLetting their ML algorithm replace the missing values\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember the steps that should be followed when there is missing data. \nSlide: 21\nQuestion 4 answer\nAn apparel company is building a machine learning solution to determine which types of products are not selling well. They've collected data on how long products sit in inventory, but their dataset contains many missing values.\nWhich method is the most effective option for addressing missing values in the apparel company's dataset?\nResponse\nA correct\nInspecting the reason for the missed values, and dropping, leaving, or imputing data accordingly\nB\nDropping all the data that contains missing values\nC\nManually imputing every missing value\nD\nLetting their ML algorithm replace the missing values\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nA: It's important to understand why the data is missing. Then, the company can set mechanisms in place to drop rows or columns of data, leave empty values, or impute data as needed.\nThe incorrect responses are:\nB: It's important to understand why the data is missing. Then, the company can set mechanisms in place to drop rows or columns of data, leave empty values, or impute data as needed. Dropping all missing values could remove significant and relevant data.\nC: This method is time-intensive. It's important to understand why the data is missing. Then, the company can set mechanisms in place to drop rows or columns of data, leave empty values, or impute data as needed.\nD: ML algorithms have trouble with missing values. While you can use AWS services to help locate and replace missing values, ML models themselves are not the best option for replacing missing values.\nSlide: 22\nFeature engineering concepts\nTopic A: Handling incorrect, duplicated, and missing data\nTopic B: Feature engineering concepts\nTopic C: Numeric and text feature engineering\nTopic D: Feature selection techniques\nTopic E: AWS data transformation services\nSpeaker Notes:\n|Student notes\nDuring the data transformation process, it is likely that you will come across different data formats. Remember, machine learning algorithms prefer numbers, so data that is text-based needs to be converted to numbers if possible. Categorical encoding is the process of manipulating text-based variables into number-based variables.\nAfter your data has been cleaned up and you\u2019ve encoded your data as necessary for your model, you can fine-tune or create new features in your dataset through feature engineering. Feature engineering is a method for transforming raw data into more informative features that help models better capture underlying relationships in the data.\nSlide: 23\nFeature engineering requirements\nUnderstanding the raw data\nRecognizing the business problem\nSelecting the right ML algorithm\nSpeaker Notes:\n|Student notes\nFeature engineering is more of an art than a science. Often, feature engineering is not a set of rules, and your approach will be based on the specific business problem that you are trying to solve.\nFeature engineering is often a highly intuitive practice. Understanding the raw data, familiarity with your business problem, and intuition of how to bridge the data and the algorithm work in parallel to determine your feature engineering requirements. Humans are skilled at finding relationships between the problem and the input feature of the raw data, so lean into your intuition when employing feature engineering.\nSlide: 24\nFeature engineering use cases \nNumeric feature engineering\nWide range or high variance\nOutliers or skewed distribution\nTime-based numeric features\nText feature engineering\nCategorical text\nLarge chunks of text\nComplex relationships\nSpeaker Notes:\n~Dev notes\n~Alt text from digital: Graph with mostly concentrated plot points and six outliers.\n|Student notes\nDepending on your data\u2019s format, you will select different methods of feature engineering to transform your dataset. \nNumeric feature engineering \ninvolves transforming numeric values for the model and is often accomplished by grouping different numeric values together. Sometimes, ML algorithms have trouble handling large numbers or isolated numbers. Grouping numeric values helps the model to better understand the relationships of these values.\nNumeric feature engineering strategies such as feature scaling, binning, and log transformation can help the ML model better understand the relationships of the data\u2019s numeric values. When deciding whether your dataset could benefit from numeric feature engineering, look for these qualities:\nNumeric features that have a wide range of values or high variance.\nNumeric features with outliers or a skewed distribution are also good targets for techniques like log transforms or binning to reduce the influence of extreme values.\nNumeric features related to time, such as timestamps, are good candidates for extraction of more meaningful metrics like day of the week or hour of the day.\nText feature engineering \ninvolves transforming text for the model and is often accomplished by splitting the text into smaller pieces. When dealing with text data, or corpus data, you can use feature engineering to transform attributes within your data to make those attributes more useful for your ML model. The purpose of text feature engineering is to break your text down into more bite-sized pieces for your model to understand the weights and relationships of your raw data.\nText feature engineering strategies, such as bag-of-words and N-gram, can tokenize the value of text data for your ML model to infer the right balance of information from that data. Because text feature engineering improves the ability for your model to understand your data effectively, essentially every type of raw data that includes text can benefit from some type of text feature engineering.\nCategorical features are associated with a group, and can often be represented numerically.\nSlide: 25\nCategorical encoding\nBinary\nTwo options \nNominal\nMultiple categories\nOrdinal\nCategories where order of values matter\nSome ML algorithms do not require encoding.\nSpeaker Notes:\n|Student notes\nWhen it comes to your data, the term numerical refers to values expressed as a measurable number, and categorical refers to features that are associated with a group. For example, in a dataset that contains animal medical records, a cat\u2019s weight is considered numerical, and the breed of cat is considered categorical.\nThe following list provides different types of categorical values.\nBinary\n categorical values refer to values that are one of two options. For example, a column of data might be true or false, such as if someone attended an event.\nNominal\n, or multi-categorical, values are category values where order does not matter. A set of data that contains different geographic locations, such as states or cities, might be considered multi-categorical.\nOrdinal\n, or ordered, values are category values where the order does matter, like what size of drink you order at a coffee shop: small, medium, or large.\nNot all categorical variables need to be encoded. Depending on your use case, different ML algorithms might not require you to encode your variables. \nFor instance, a random forest model can handle categorical features directly. You would not need to encode values, such as teal, green, and blue, as numeric values.\nSlide: 26\nActivity: Assessing data for categorical encoding\nSpeaker Notes:\n|Student notes\nThe data gathered by the research scientist team includes various types of numerical and categorical data. \nSlide: 27\nActivity: Assessing data for categorical encoding\nSpeaker Notes:\n|Student notes\nThe \nage\n column contains numerical data. Because this data is already set as numeric values, there is no need for categorical encoding.\nThe \neducation\n column is categorical data. Depending on which type of ML algorithm you decide to use, you might need to encode this data before using it to train and inform your model.\nThe \nflu shot \ncolumn is binary categorical data because respondents answered Yes or No. Depending on the type of algorithm you decide to use, this data might need to be encoded for your model to read the data more easily. For example, you can encode Yes to 0 and No to 1.\nThe \noutbreak zone \ncolumn is also binary categorical data. In this case, the research scientists could have also offered more value options for this feature that would be categorized as ordered, or ordinal, data. For instance, the scientists could categorize outbreak by light, medium, or heavy outbreak, rather than a binary Yes or No outbreak. Either way, this column might need to be encoded for model training, depending on the algorithm you decide to use. \nSlide: 28\nQuestion 1\nAn automobile company wants to use a machine learning solution to determine how quickly their various sizes of vehicles reach their highest speeds. The vehicle sizes are compact, mid-size, and full-size. \nWhich type of categorical data BEST describes the feature of their dataset that lists the size of the vehicle?\nResponse\nA\nNumerical\nB\nNominal\nC\nOrdinal\nD\nBinary\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember the type of categorical values that are ranked in order. \nSlide: 29\nQuestion 1 answer\nAn automobile company wants to use a machine learning solution to determine how quickly their various sizes of vehicles reach their highest speeds. The vehicle sizes are compact, mid-size, and full-size. \nWhich type of categorical data BEST describes the feature of their dataset that lists the size of the vehicle?\nResponse\nA\nNumerical\nB\nNominal\nC\nOrdinal\nD\nBinary\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nC: Ordinal data is the correct type of categorical data here, because the vehicle sizes like compact, midsize, and full-size are categorical labels with a numeric order. Ordinal data describes data that is categorical, but ranked in a numeric order.\nThe incorrect responses are:\nA: Numerical data represents quantitative values. The vehicle sizes (compact, mid-size, full-size) are qualitative categories, not numerical values.\nB: This question is asking about the categorical data that would describe the vehicle sizes. The vehicle sizes, such as compact, midsize, and full-size, would be considered ordinal data because they are categorical values that have a specific size order.\nD: Binary data represents categories with only two possible values. The vehicle sizes have three possible categories: compact, mid-size, full-size.\nSlide: 30\nQuestion 2\nAn energy company wants to assess how often their customers have to replace their solar panels after their initial solar panel purchase. Their raw data includes the dates of first purchase, customer reviews, and dates of additional purchases.\nHow might this company use feature engineering to help the machine learning model understand the relationship between these features in their dataset?\nResponse\nA\nThis is not a good use case for feature engineering, because the data includes a text-based feature.\nB\nThe company can use numeric feature engineering to extract information from the purchase dates, and they can use text feature engineering to extract information from the customer reviews.\nC\nThe company can only use numeric feature engineering to extract information from this dataset.\nD\nThe company can only use text feature engineering to extract information from this dataset.\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember that machine learning models find patterns in historical data and use that to make predictions.\nSlide: 31\nQuestion 2 answer\nAn energy company wants to assess how often their customers have to replace their solar panels after their initial solar panel purchase. Their raw data includes the dates of first purchase, customer reviews, and dates of additional purchases.\nHow might this company use feature engineering to help the machine learning model understand the relationship between these features in their dataset?\nResponse\nA\nThis is not a good use case for feature engineering, because the data includes a text-based feature.\nB correct\nThe company can use numeric feature engineering to extract information from the purchase dates, and they can use text feature engineering to extract information from the customer reviews.\nC\nThe company can only use numeric feature engineering to extract information from this dataset.\nD\nThe company can only use text feature engineering to extract information from this dataset.\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nB: Extracting information from these different features might help to show the relationship between initial purchase, review sentiment, and any additional purchases.\nThe incorrect responses are:\nA: Both the numeric and text-based data in this dataset are good candidates for feature engineering.\nC: The company can use text feature engineering to extract meaningful information from customer reviews..\nD: The company can use numeric feature engineering to extract meaningful information from initial and additional purchase dates.\nSlide: 32\nNumeric and text feature engineering\nTopic A: Handling incorrect, duplicated, and missing data\nTopic B: Feature engineering concepts\nTopic C: Numeric and text feature engineering\nTopic D: Feature selection techniques\nTopic E: AWS data transformation services\nSpeaker Notes:\n|Student notes\nIn the last section, you learned about the difference between numeric and categorical values. Numeric feature engineering involves creating a new feature by combining two or more numeric values. \nText data is an important component in many business problems, and not all text-based features can be easily encoded with categorical encoding. Text feature engineering involves transforming the text in your data so that your ML model can better understand the relationships within that data.\nSlide: 33\nNumeric feature engineering methods\nFeature scaling\nBinning\nLog transformation\nSpeaker Notes:\n|Student notes\nTypes of numeric feature engineering methods include feature scaling, binning, and log transformation.\nFeature scaling, also known as data scaling or normalization, aims to transform the numeric values so that all values are on the same scale. This method helps you to take large numbers and scale them down, so that the ML algorithm can achieve quicker computations and avoid skewed results.\nBinning is a technique to group numerical features into a smaller number of bins, or categories. The data is divided into these bins based on value ranges, thus transforming a numeric feature into a categorical one.\nLog transformation applies a logarithmic function to numeric features in a dataset. A logarithmic function is a mathematical function where the independent variable is the exponent of a fixed number, called the base.\nSlide: 34\nFeature scaling approaches\nNormalization\nRescale values to a common scale (\neg.\n 0 to 1)\nImpacted by outliers\nStandardization\nRescale values to a mean of 0 and a standard deviation of 1\nReduces negative effect of outliers\nSpeaker Notes:\n~Dev notes\n~Alt text: \nGraph with points ranging between 0 and 1\n~Alt text: \nGraph with points centered around 0\n|Student notes\nA lot of ML algorithms are sensitive to wide ranges of data, so feature scaling is required for many ML models. However, some ML models like decision trees are not skewed by wide ranges of numeric data. Whether or not to scale your data will depend on the ML algorithm that you decide to use.\nNormalization\n rescales the values of numeric features to a common scale, often between 0 and 1. This rescaling makes the values more comparable and prevents features with greater magnitudes from incorrectly influencing the model. Outliers can prevent a normalization approach from working effectively.\nStandardization\n is similar to normalization, but instead of scaling values from 0\u20131, it rescales the features to have a mean of 0 and standard deviation of 1. \nStandardization reduces the negative effect of outliers.\nSlide: 35\nFeature scaling example\nSpeaker Notes:\n|Student notes\nAs an example, a dataset considering housing data might contain features such as the number of bedrooms and the price of the house. Number of bedrooms might range from 1 to 5 as a numeric value, but the price might range from $100,000 to $1,000,000.\nNormalizing the house pricing feature with values from 100,000 to 1,000,000 to a 0\u20131 scale would transform the values to be between 0 and 1. This method assigns the smallest value to 0 and the highest value to 1, and uses a calculation to scale down the remaining values.\nStandardization would set the average home price at the integer of 0 and use a calculation to score the remaining values. The calculation smooths out the values, so outliers are not as much of a negative factor with standardization as they are with normalization.\nSlide: 36\nBinning\nFeature is transformed from numeric to categorical\nHouse prices\nBinned house prices\nSpeaker Notes:\n~Dev notes\n~Alt text from digital: \nTwo bar charts: one with bars for features; one with these features grouped into three bars, or bins.\n|Student notes\nBinning can make models more robust by reducing the effect of outliers and allowing nonlinear relationships between features and target values to be captured more easily. It is an effective strategy for numerical data when the exact difference in numbers is not important, but the general range is a factor.\nFor example, you might use binning to engineer the house pricing feature. You might not be concerned about distinguishing between data points of a house price of $150,000 and $151,000, but you might be interested in distinguishing between houses priced at $150,000 and $500,000. \nYou can use a method called quantile binning to evenly distribution values between bins. For example, if you had three bins for your housing data, the lowest 33.3% of data would distribute to one bin, the next 33.3% in the next bin, and highest 33.3% in the remaining bin. Quantile binning results in each bin containing approximately the same number of data points.\nSlide: 37\nLog transformation\nApply a logarithmic function to numeric features\nSpeaker Notes:\n~Dev notes\n~Alt text from digital: \nTwo bar graphs: The first shows bars with many outliers; the second uses log transformation to bring values closer.\n|Student notes\nThe most common logarithmic functions have bases of 10 or e, where e is approximately equal to 2.71828. Logarithmic functions are the inverse of exponential functions and are useful for modeling phenomena where growth decreases over time, like population growth or decay.\nLog transformation is used to normalize skewed numeric data, reduce the effect of outliers, and keep values closer to normal distribution. Log transforming can improve model performance when using numeric features that have highly skewed distributions or outliers.\nLet\u2019s say your housing dataset had extreme outliers due to containing multiple geographic locations. One geographic location has a rural lot listed for $10,000 and another location has a luxury home in the city listed for $10,000,000. Taking the log of the housing prices scales down the large outlier values.\nFor example, the log of $10,000 would be around 4 and the log of $10,000,000 would be around 7. Using this method, the outliers are brought much closer to the normal values in the remainder of the dataset.\nSlide: 38\nText feature engineering methods\nBag of words\nN-gram\nTemporal data\nSpeaker Notes:\n|Student notes\nYou can use feature engineering methods such as bag-of-words and N-gram to engineer your text-based features. Temporal or time series data is data involving time, like a series of dates. Temporal data can come in many formats, and is often a mix of numeric and text data.\nSlide: 39\nBag-of-words\n\u201cdistrict\u201d \u201chas\u201d \u201cgreat\u201d \u201cteachers\u201d \n\u201cand\u201d \u201cgreat\u201d \u201cfacilities\u201d \nCount number of times words appear\nSpeaker Notes:\n|Student notes\nThe bag-of-words model does not keep track of the sequence of words, but counts the number of words in each observation. Bag-of-words uses tokenization to create a statistical representation of the text.\nConsider an example with the housing dataset. Perhaps you want to determine whether the listed home is in a highly rated school district, and you want to include parents\u2019 school reviews as a part of your dataset. The bag-of-words model would take a portion of raw text, break down each piece of text, and tokenize the text to create a table or statistical representation of that review.\nSlide: 40\nN-gram\n\u201cdistrict has\u201d \u201chas great\u201d \u201cgreat teachers\u201d \n\u201cteachers and\u201d \u201cand great\u201d \u201cgreat facilities\u201d \nProduce groups of words of n size\nSpeaker Notes:\n|Student notes\nThe N-gram model builds off of bag-of-words by producing a group of words of n size.\nFor example, if n = 2, a sliding window creates each group of two words as a token pair and creates a statistical representation of these token pairs. This method is useful when we want to find a particular combination of words in the text.\nIn the housing data, you could use the n-gram model to represent how many times reviews include word combinations of \u201cgreat teachers\u201d or \u201cafter-school programs.\"\nSlide: 41\nTemporal data\nExtract features from dates and times\nSpeaker Notes:\n|Student notes\nFeature engineering is important for temporal data because it can help capture key trends, cycles, or patterns over time. Examples of temporal features include time of day, day of the week, and seasons. By carefully extracting and including them in your model, you can help the model to better understand and predict time-based relationships in the data. \nFor example, you might have a feature in your housing dataset that contains the dates of when houses were listed for sale. When you have your dates in a consistent format, you might use feature engineering to create different features that extract the temporal information into day, month, and year. Extracting this information might reveal that most homes are listed in the summer or that most homes are listed at the first of each month.\nSlide: 42\nQuestion 1\nA company is building a machine learning model to predict credit risk. One of the features in the dataset is \nLoanAmount\n, which is likely to contain a few outliers.\nWhich feature engineering technique is a good method for engineering this numeric feature before model training?\nResponse\nA\nTake the log transform of the values to help normalize distribution.\nB\nLeave the values unchanged.\nC\nTake the mean of all values.\nD\nUse one-hot encoding to sort the values.\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember that a log transform can adjust data with outliers to a more normal distribution.\nSlide: 43\nQuestion 1 answer\nA company is building a machine learning model to predict credit risk. One of the features in the dataset is \nLoanAmount\n, which is likely to contain a few outliers.\nWhich feature engineering technique is a good method for engineering this numeric feature before model training?\nResponse\nA correct\nTake the log transform of the values to help normalize distribution.\nB\nLeave the values unchanged.\nC\nTake the mean of all values.\nD\nUse one-hot encoding to sort the values.\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nA: Taking the log transform of numeric features like \nLoanAmount\n can help normalize the distribution and handle skewness. This makes the values more comparable for modeling.\nThe incorrect responses are:\nB: Leaving the values unchanged would not help to normalize the distribution. Taking the log transform of numeric features like \nLoanAmount\n can help normalize the distribution and handle skewness.\nC: Taking the mean of all values can be a helpful method for measuring data, but in this case, the outliers might skew that calculation. Taking the log transform of numeric features like \nLoanAmount\n can help normalize the distribution and handle skewness.\nD: Although one-hot encoding is a great tool for categorical values, this value is numeric. Taking the log transform of numeric features like \nLoanAmount\n can help normalize the distribution and handle skewness.\nSlide: 44\nQuestion 2\nYou work for a travel company and want to build a model to categorize customer reviews as positive or negative.\nWhich text feature engineering technique would be most useful for this classification task?\nResponse\nA\nBag-of-words\nB\nBinning\nC\nLog transformation\nD\nN-gram\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember that n-grams are a useful feature engineering technique for sentiment analysis.\nSlide: 45\nQuestion 2 answer\nYou work for a travel company and want to build a model to categorize customer reviews as positive or negative.\nWhich text feature engineering technique would be most useful for this classification task?\nResponse\nA\nBag-of-words\nB\nBinning\nC\nLog transformation\nD correct\nN-gram\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nD: N-grams are a good technique for text classification tasks like sentiment analysis, because they help capture important phrases and sequences of words, not just individual words. Bag-of-words loses this important context. For a travel review, sequences like \"not clean\" and \"very comfortable\" are important signals. N-grams can help the model understand these types of phrases.\nThe incorrect responses are:\nA: Although bag-of-words represents the occurrence of words in a document, it does not preserve the order and context of the words. Reviews with very different sentiments could have similar bag-of-words representations if they contain the same vocabulary.\nB: Binning is useful for converting continuous numerical variables into categorical bins, not for text feature engineering.\nC: Log transformations are useful for normalizing skewed numerical distributions, not text data.\nSlide: 46\nFeature selection techniques\nTopic A: Handling incorrect, duplicated, and missing data\nTopic B: Feature engineering concepts\nTopic C: Numeric and text feature engineering\nTopic D: Feature selection techniques\nTopic E: AWS data transformation services\nSpeaker Notes:\n|Student Notes\nYou\u2019ve learned about feature scaling and feature engineering to make the most of your dataset and help your model determine relationships in the data. You can also use the technique of feature selection to select the most useful features for your model to make efficient and accurate predictions. \nSlide: 47\nFeature selection\nReduces dimensions\nRemove or select features\nRemove\nSpeaker Notes:\n|Student notes\nFeature selection is the intuitive process of selecting the most relevant features to use in a machine learning model.\nEarlier, you learned about feature scaling. Feature scaling refers to methods that rescale the values of features to a common scale without removing any. Feature selection, on the other hand, reduces dimensions by removing features or selecting certain features so that your model can focus on the correct information. \nIn this example, the housing dataset includes features of the price, the location, the date listed, and the number of bedrooms. You deal primarily with houses in the same geographic location, and you are more interested in the relationship between the number of bedrooms and the price of the house. In that case, you might decide to remove the \u201clocation\u201d and \u201cdate listed\u201d features, so that your model can focus on the most relevant information to the problem.\nSlide: 48\nFeature selection by splitting and combining\nFeature splitting\nFeature combining\nTotal number of bedrooms\nTotal square footage\nSpeaker Notes:\n~Dev notes\n~Alt text:  graphic showing splitting number of bedrooms into small, medium and large features.\n~Alt text:  graphic showing combining individual room sizes into a total square footage feature.\n|Student notes\nIn addition to selecting certain relevant features, you can also use feature splitting and combining to provide the most relevant information to your ML model.\nFeature splitting breaks down features into multiple derived features, expanding the feature space. Feature combining aggregates multiple features into a new combined feature, reducing the feature space. Both techniques can help improve model performance by transforming the original feature set into a more optimal set of predictive features.\nFeature splitting \nFor example, in the housing dataset, you might decide to capture more specific information about bedrooms in each home. The number of bedrooms feature might be split into the number of small, medium, and large bedrooms.\nFeature combining\nOr you might have each room\u2019s square footage categorized in a separate feature. You could combine the room size features to create one complete feature that represents the home\u2019s square footage.\nSlide: 49\nPrincipal component analysis (PCA)\nReduce number of features\nTransform features into principal components \nRetain most of the original information\nRanks components by their variance\nSpeaker Notes:\n|Student notes\nPrincipal component analysis (PCA) is a statistical technique that you can use for dimensionality reduction. In other words, PCA is a method for reducing the number of features in a dataset.\nThe goal of PCA is to retain most of the variation in the original features but reduce the overall number of features to increase computation efficiency. PCA works by transforming the features into a new set of uncorrelated features called principal components. PCA ranks the new set of principal components by their variance, with the first components capturing most of the variance in the data.\nSlide: 50\nActivity: Understanding principal components\nSpeaker Notes:\n|Student notes\nSlide: 51\nActivity: Understanding principal components\nSpeaker Notes:\n|Student notes\nIn the example table, you could reduce five of the features to just two principal components and compare them. The first principal component focuses on house size, and the second component focuses on cost. \nPrincipal component 1: Size\nThe first component accounts for the physical attributes of the home that includes square footage, bedrooms, and bathrooms.\nPrincipal component 2: Cost\nThe second component captures the financial factors that include price and tax rate.\nSlide: 52\nQuestion 1\nA baseball analyst wants to predict a player's batting average using a machine learning model. They have a dataset with over thirty different hitting statistics for each player. \nThe analyst wants to retain necessary information but reduce the number of features before training the model. Which feature selection technique should they use?\nResponse\nA\nFeature splitting\nB\nFeature scaling\nC\nPrincipal component analysis (PCA)\nD\nNo need for feature selection \nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember that PCA reduces features while retaining information. . \nSlide: 53\nQuestion 1 answer\nA baseball analyst wants to predict a player's batting average using a machine learning model. They have a dataset with over thirty different hitting statistics for each player. \nThe analyst wants to retain necessary information but reduce the number of features before training the model. Which feature selection technique should they use?\nResponse\nA\nFeature splitting\nB\nFeature scaling\nC correct\nPrincipal component analysis (PCA)\nD\nNo need for feature selection \nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nC: PCA reduces the dimensionality of the data while retaining most of the variation. This makes it useful for reducing a large set of variables down to a smaller subset of features before modeling.\nThe incorrect responses are:\nA: Feature splitting divides features into multiple datasets, which does not reduce dimensionality.\nB: Feature scaling standardizes features but does not reduce dimensionality.\nD: This example is a good candidate for using PCA. PCA reduces the dimensionality of the data while retaining most of the variation. This makes it useful for reducing a large set of variables down to a smaller subset of features before modeling.\nSlide: 54\nAWS data transformation services\nTopic A: Handling incorrect, duplicated, and missing data\nTopic B: Feature engineering concepts\nTopic C: Numeric and text feature engineering\nTopic D: Feature selection techniques\nTopic E: AWS data transformation services\nSpeaker Notes:\n|Student notes\nData ingestion is the process of collecting data from diverse sources, such as databases, data lakes, and streaming sources. The goal of data ingestion is to consolidate your data to use for your machine learning solution. \nIn this section, you will learn how you can use AWS services, such as Amazon SageMaker Data Wrangler, to ingest your data. You will also learn how you can use Amazon SageMaker Feature Store to ingest data and to create and manage features. \nYou will learn about data transformation use cases of these AWS services. You also learn fundamental steps of how you can use these services to automate part or all of the data transformation process.\nSlide: 55\nIngesting and transforming data for ML on AWS\nAmazon SageMaker Data Wrangler\nAmazon SageMaker Feature Store\nAmazon EMR\nAdd EMR notes from 1.2.13\nDo not add content but leave note here for T&C to revisit this slide since digital content is streaming-specific.\nSpeaker Notes:\n|Student notes\nYou can use AWS services, such as Amazon SageMaker Data Wrangler, to ingest your data. You can also use Amazon SageMaker Feature Store to ingest data and to create and manage features.\nAWS services like SageMaker Data Wrangler and SageMaker Feature Store streamline complicated data ingestion pipelines. Data Wrangler provides a visual, no-code way to transform and clean data from diverse sources. SageMaker Feature Store manages and shares curated features for machine learning model development. Using these services together eliminates much of the manual coding needed for ingesting and wrangling data. \nAWS services like Amazon EMR can help you to transform, enrich, and load data at scale. \nSlide: 56\nBenefits of Amazon SageMaker Data Wrangler\nAmazon SageMaker Data Wrangler\nVisual interface\nDrag-and-drop environment\nBuilt-in algorithms\nIntegration with data sources\nReusable data flows\nSpeaker Notes:\n|Student notes\nData Wrangler streamlines data ingestion through a visual interface to profile, understand, clean, and transform data using built-in recipes. It connects to sources, transforms, cleans, and loads data. Then, it outputs structured, model-ready data to AWS storage without writing code. SageMaker Data Wrangler streamlines complex data preparation so you can focus on building and training ML models, rather than preparing data. \nData Wrangler is a visual, code-free tool for data preprocessing and feature engineering that integrates with AWS data sources. It streamlines machine learning workflows through reusable data flows, large dataset handling, and integration with SageMaker.\nSlide: 57\nUse cases for SageMaker Data Wrangler\nClean data\nFeature engineering\nFix formatting issues\nReduce data size\nAutomate transformations\nSpeaker Notes:\n|Student notes\nData Wrangler streamlines exploratory data analysis and data preprocessing stages for ML learning in a visual, scalable, and collaborative environment. The following are some of the common use cases:\nClean data\n: You can use SageMaker Data Wrangler to interactively explore, visualize, clean, validate, and transform data to prepare it for ML model training. You can use the built-in transforms and visual interface to clean, standardize, and preprocess raw data from various sources like Amazon S3, databases, and more. \nFeature engineering\n: You can use the built-in transform and Python code blocks to engineer new features from your data. For example, you can use SageMaker Data Wrangler to combine columns and apply formulas.\nFixing formatting issues\n: SageMaker Data Wrangler can handle issues like missing headers and encoding problems to get the data into a consistent format for downstream use.\nReducing data size: \nFor large datasets, you can use SageMaker Data Wrangler to select subsets of columns, filter rows, sample data, and convert to lower precision data types. This helps to reduce storage and memory needs.\nAutomating transformations\n: After you interactively build transformation steps in SageMaker Data Wrangler, you can save them as a data recipe or flow to automate preparation for future datasets.\nSlide: 58\nUsing Amazon SageMaker Data Wrangler for data ingestion and transformation\nInput\nOutput\nAmazon \nSageMaker\n Data Wrangler\nIdentify data sources\nCreate a \nnew data flow\nPreview transformations\nOutput results\nSelect built-in transforms to transform data\nSpeaker Notes:\n|Student notes\nIdentify data sources\nUpload your dataset to an S3 bucket. SageMaker Data Wrangler supports CSV, JSON, Parquet, and other common data formats.\nCreate a new data flow\nOpen the SageMaker console and navigate to Data Wrangler. Choose Create flow to start a new data flow. Select the dataset you uploaded to Amazon S3 as the input source. SageMaker Data Wrangler will infer the schema and show a sample of the data. \nSelect built-in transforms to transform data\nIn the Transform section, use the built-in transforms to transform your data. You can see a sample output as you build the transforms. Some common transformations include the following: \nChanging column data types \nFiltering or sorting rows \nFilling in missing values \nParsing columns, like dates \nJoining datasets \nAggregating data, like sums or averages\nPreview transformations\nAutomate conversations, question answering, and workflows based on research into training honest and responsible AI systems.\nOutput results\nGenerate unique, realistic, high-quality images, art, logos, and designs.\nSlide: 59\nBenefits of Amazon SageMaker Feature Store\nAmazon SageMaker Feature Store\nStreamline\nTrack\nConfigure\nRetain\nSpeaker Notes:\n|Student notes\nFeature Store is a managed repository for storing, sharing, and managing features used for ML models. Features can be reused by multiple teams and SageMaker Feature Store keeps them synchronized for both batch training and real-time inference. By centralizing features in a secured store, SageMaker Feature Store improves feature quality and model accuracy at scale across the ML lifecycle. Storing features saves time by eliminating redundant feature engineering efforts.\nFeature Store provides centralized management of feature engineering to streamline machine learning workflows. It provides configurable online and offline storage of feature groups with metadata to enable discovery, reuse, and automation from data ingestion through model training and prediction. \nFeature Store provides a centralized and automated way to engineer, monitor, govern, and reuse features to improve model training and performance across teams. The following are some of the common use cases:\nAutomated data preprocessing: \nSageMaker Feature Store can automatically preprocess raw data as it is ingested. This includes scaling, encoding, transforming, and cleaning data so it is ready for model training.\nCentralized feature repository: \nSageMaker Feature Store acts as a centralized repository for curated features that have been preprocessed and engineered. Features can be reused across teams and models.\nStandardized features\n: SageMaker Feature Store establishes a standardized source of truth for featured data that is consistent across the organization. This avoids issues with different teams using their own feature engineering code.\nManagement of feature pipelines\n: SageMaker Feature Store manages the end-to-end lifecycle of feature pipelines from data ingestion through preprocessing. This provides monitoring and governance of feature engineering.\nCaching for performance\n: Frequently used features are cached in SageMaker Feature Store for faster retrieval during training. This improves performance by avoiding repetitive preprocessing.\nSlide: 60\nBenefits of Amazon EMR\nAmazon EMR\n<list key benefits>\nAdd EMR notes from 1.2.13\nDo not add content but leave note here for T&C to revisit this slide since digital content is streaming-specific.\nSpeaker Notes:\n|Student notes\nSlide: 61\nData formatting for model training in Amazon SageMaker\nSplit data sets.\nTraining\nTesting\nValidation\nAugment or shuffle training data for increased diversity.\nFormat data set.\nCSV\nRecordIO-protobuf\nSpeaker Notes:\n|Student notes\nReducing bias in your dataset is one of the primary objectives in data preparation. With balanced data, the model can focus on meaningful patterns in the dataset rather than any inherent noise.\nA common strategy to achieve a balanced and accurate dataset is to divide the data into distinct subsets for training, testing, and validation. \nData augmentation works by creating new, realistic training examples that expand the model's understanding of the data distribution. Dataset augmentation involves artificially expanding the size and diversity of a dataset by applying a variety of transformations and modifications to existing data samples. These transformations introduce variations in the data while preserving the essential characteristics relevant to the learning task\nData shuffling involves rearranging the order of the data points in a dataset before they are used for training a machine learning model. Shuffling your data helps to verify that the training examples are presented to the model in a random order rather than in a fixed sequence.\nWhen using Amazon SageMaker for your ML workloads, you want to make sure you are providing data that adheres to an accepted format. The two most common formats supported by the SageMaker built-in algorithms are CSV formatting and \nRecordIO-protobuf\n formatting. \nFormatting can be a complex task. You focus more on adhering to certain formatting types during the model training and refining stage of the machine learning lifecycle. However, it is helpful to know the basic concepts and structure of these two types of formats.\nCSV is a widely used text-based data format for training machine learning models. CSV files store tabular data, where each row represents a data sample, and the columns represent the features of that sample. For SageMaker built-in algorithms, the label in the training dataset must be the first column on the left, and the features should be to the right. Many built-in algorithms in SageMaker, such as \nXGBoost\n, linear learner, and \nDeepAR\n, accept CSV formatted data as input for model training. \nRecordIO-protobuf\n is a binary data format used by SageMaker for efficient data storage and transfer. This format is particularly useful when you need to handle large datasets or perform automatic transformations on the data. The \nRecordIO-protobuf\n format consists of a series of records, where each record contains a binary-encoded \nprotobuf\n message. This format is commonly used for image data, where each record represents an image and its associated metadata.\nSlide: 62\nQuestion 1\nYou work for a hospitality company that operates hotels across the country. You want to build a machine learning model to predict customer loyalty using data about past customer stays and purchases. You are considering using SageMaker Feature Store to prepare the data for model training. \nHow could SageMaker Feature Store be an effective service option for this use case?\nResponse\nA\nSageMaker Feature Store is a marketplace where you can pay workers to complete discrete tasks like labeling data. This would verify that your customers\u2019 purchases are accurate.\nB\nYou can use SageMaker Feature Store to visualize and clean your data. Clean data is essential for machine learning accuracy.\nC\nSageMaker Feature Store automatically labels your data. This saves time labeling data for supervised learning.\nD\nYou can use SageMaker Feature Store to store normalized features that can be reused for multiple models. This saves time preparing data for each new model.\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember that Feature Store serves as a centralized repository for features.\nSlide: 63\nQuestion 1 answer\nYou work for a hospitality company that operates hotels across the country. You want to build a machine learning model to predict customer loyalty using data about past customer stays and purchases. You are considering using SageMaker Feature Store to prepare the data for model training. \nHow could SageMaker Feature Store be an effective service option for this use case?\nResponse\nA\nSageMaker Feature Store is a marketplace where you can pay workers to complete discrete tasks like labeling data. This would verify that your customers\u2019 purchases are accurate.\nB\nYou can use SageMaker Feature Store to visualize and clean your data. Clean data is essential for machine learning accuracy.\nC\nSageMaker Feature Store automatically labels your data. This saves time labeling data for supervised learning.\nD correct\nYou can use SageMaker Feature Store to store normalized features that can be reused for multiple models. This saves time preparing data for each new model.\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nD: You can use SageMaker Feature Store to save time by storing curated features that can be accessed to train new models.\nThe incorrect responses are:\nA: Amazon Mechanical Turk is a service for hiring workers to complete tasks like labeling data. Feature Store helps manage and share features, not label data. You can use SageMaker Feature Store to store curated features that can be accessed to train new models.\nB: Although clean data is essential for ML accuracy, SageMaker Data Wrangler is a service that assists with data visualization and transformation. You can use SageMaker Feature Store to save time by storing curated features that can be accessed to train new models.\nC: SageMaker Ground Truth is a separate service for labeling data. SageMaker Feature Store helps manage and share features, not label data. You can use SageMaker Feature Store to save time by storing curated features that can be accessed to train new models.\nSlide: 64\nQuestion 2\nA telecommunications company wants to use Amazon SageMaker to train a machine learning model on their customer data. They have the data stored in different formats.\nWhich data formats do most Amazon SageMaker algorithms support for training? \nResponse\nA\nCSV and \nRecordIO-protobuf\nB\nAll data formats\nC\nNo data formats\nD\nCustom data formats\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember that SageMaker supports CSV and \nRecordIO-protobuf\n formats for training data.\nSlide: 65\nQuestion 2 answer\nA telecommunications company wants to use Amazon SageMaker to train a machine learning model on their customer data. They have the data stored in different formats.\nWhich data formats do most Amazon SageMaker algorithms support for training? \nResponse\nA correct\nCSV and \nRecordIO-protobuf\nB\nAll data formats\nC\nNo data formats\nD\nCustom data formats\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nA: Many Amazon SageMaker algorithms support training with data in CSV and \nRecordIO-protobuf\n format.\nThe incorrect responses are:\nB: Many Amazon SageMaker algorithms support certain data formats.\nC: Many Amazon SageMaker algorithms support certain commonly used data formats. \nD: Many Amazon SageMaker algorithms support certain commonly used data formats. \nSlide: 66\nResources\nTo learn more, choose from the following resources:\nAmazon SageMaker AI \u2013CSV Format\n (developer guide)\nAmazon SageMaker AI \u2013RecordIO Format \n(developer guide)\nTime-Series Datasets Format and Missing Values Filling Methods\n (developer guide)\nCreate, Store, and Share Features with Amazon SageMaker Feature Store\n (developer guide)\nTransform Data\n (developer guide)\nSpeaker Notes:\nSlide: 67\nModule summary\nYou should now be able to:\nExplain the value of data cleaning and transformation.\nDescribe how to process incorrect or duplicated data.\nDescribe how to detect and treat outliers.\nDescribe how to process missing values.\nDescribe fundamental encoding techniques.\nIdentify feature engineering use cases.\nDescribe fundamental concepts, benefits, and techniques of feature engineering.\nDescribe fundamental feature selection techniques.\nDescribe AWS services for validating and labeling data.\nIdentify AWS tools and services for visualizing and transforming data.\nSpeaker Notes:\n|Student notes\nSlide: 68\nQuestions?\nThank you for attending\nthis session\nCorrections, feedback, or other questions? \nContact us at \nhttps://support.aws.amazon.com/#/contacts/aws-training\n. \nAll trademarks are the property of their owners.\nSpeaker Notes:\n|Instructor Notes\n|\n|Student Notes",
        "RESPONSE": "Here are the speaker notes for slides 1-10 of the presentation:\n\nSlide 1: Introduction to Data Transformation and Feature Engineering\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler.html\n~\n~Image Description:\n~This slide shows the title \"Module 3: Data Transformation and Feature Engineering\" with the subtitle \"Machine Learning Engineering on AWS\".\n~\n|INSTRUCTOR NOTES:\n- |Introduce the topic of data transformation and feature engineering\n- |Explain how this module builds on previous data concepts\n- |Highlight the importance of data preparation for ML\n|\n|STUDENT NOTES:\nThis module focuses on **data transformation and feature engineering**, which are critical steps in preparing data for machine learning. At this point, you should be familiar with data collection, ingestion, and storage fundamentals covered in previous modules. \n\nYou'll now learn about various **transformation techniques** like data cleaning, encoding, and feature engineering. The module will cover how to use AWS services such as **Amazon SageMaker Feature Store**, **Amazon SageMaker Data Wrangler**, and **AWS Glue** to transform your data efficiently.\n\nUnderstanding these concepts and tools is essential for **creating high-quality datasets** that can lead to more accurate and effective machine learning models. The skills you'll gain here build directly on your knowledge of data types, formats, and AWS storage services.\n\nSlide 2: Module Objectives & Outline\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.html\n~\n~Image Description:\n~This slide lists the learning objectives for the module, including understanding data cleaning, transformation techniques, and AWS services for data preparation.\n~\n|INSTRUCTOR NOTES:\n- |Review each learning objective\n- |Emphasize the practical skills students will gain\n- |Connect objectives to real-world ML workflows\n|\n|STUDENT NOTES:\nThis module aims to equip you with essential skills in data transformation and feature engineering. You'll learn to:\n\n1. **Explain the value** of data cleaning and transformation in ML workflows\n2. **Process incorrect or duplicated data** effectively\n3. **Detect and treat outliers** in datasets\n4. **Handle missing values** appropriately\n5. Apply **fundamental encoding techniques** for categorical data\n6. Identify **use cases for feature engineering**\n7. Understand **key concepts and benefits** of feature engineering\n8. Apply **feature selection techniques** to improve model performance\n9. Utilize **AWS services** for data validation and labeling\n10. Leverage AWS tools for **data visualization and transformation**\n\nThese objectives cover the entire spectrum of data preparation, from cleaning raw data to creating model-ready features using AWS services. Understanding these concepts is crucial for building robust and accurate ML models.\n\nSlide 3: Data Processing: Data Transformation and Feature Engineering\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide shows a diagram of the machine learning lifecycle, highlighting the \"Data Processing\" stage which includes data transformation and feature engineering.\n~\n|INSTRUCTOR NOTES:\n- |Explain where data transformation fits in the ML lifecycle\n- |Highlight the importance of this stage for model accuracy\n- |Discuss how this stage builds on previous data preparation steps\n|\n|STUDENT NOTES:\nData transformation and feature engineering are crucial steps in the **machine learning lifecycle**, occurring after data collection, ingestion, and storage. At this stage, you're preparing your data for model development by transforming it into a clean, encoded, and engineered dataset.\n\nThis process involves several key techniques:\n- **Data cleaning**: Removing errors, inconsistencies, and irrelevant information\n- **Categorical encoding**: Converting categorical variables into numeric representations\n- **Feature engineering**: Creating new features or modifying existing ones to improve model performance\n\nThese techniques help ensure that your dataset is **accurate, consistent, and optimized** for machine learning algorithms. Proper data transformation can significantly impact model accuracy and efficiency, making it a critical step in the ML workflow.\n\nRemember, the quality of your model is heavily dependent on the quality of your data. Investing time in thorough data transformation and feature engineering can lead to more robust and accurate ML models.\n\nSlide 4: Techniques for Data Transformation\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.html\n~\n~Image Description:\n~This slide outlines three main techniques for data transformation: data cleaning, categorical encoding, and feature engineering, with brief examples of each.\n~\n|INSTRUCTOR NOTES:\n- |Explain each technique in detail with examples\n- |Discuss when to use each technique\n- |Highlight the impact of each technique on model performance\n|\n|STUDENT NOTES:\nData transformation involves three main techniques, each serving a specific purpose in preparing data for ML models:\n\n1. **Data cleaning**: This is the first crucial step in data preparation. It involves:\n   - Identifying and handling missing values\n   - Removing duplicates\n   - Correcting inconsistencies\n   - Dealing with outliers\n   Example: Removing duplicate customer IDs in a dataset\n\n2. **Categorical encoding**: This process converts categorical variables into numeric representations that ML algorithms can interpret. Common methods include:\n   - One-hot encoding\n   - Label encoding\n   Example: Encoding country names as numeric values (e.g., USA=1, Canada=2, UK=3)\n\n3. **Feature engineering**: This involves creating new features or modifying existing ones to help ML algorithms better understand the data. Techniques include:\n   - Binning continuous variables\n   - Creating interaction terms\n   - Extracting information from text or dates\n   Example: Using bag-of-words to convert text reviews into numeric features\n\nEach of these techniques plays a vital role in transforming raw data into a format that ML models can effectively use, ultimately leading to more accurate and reliable predictions.\n\nSlide 5: Agenda\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: N/A\n~\n~Image Description:\n~This slide outlines the agenda for the module, listing five main topics to be covered.\n~\n|INSTRUCTOR NOTES:\n- |Briefly introduce each topic\n- |Explain the flow of the module\n- |Set expectations for what will be covered\n|\n|STUDENT NOTES:\nThis module is structured into five main topics, each focusing on a crucial aspect of data transformation and feature engineering:\n\n1. **Handling incorrect, duplicated, and missing data**: We'll explore techniques to clean and preprocess raw data, ensuring its quality and consistency.\n\n2. **Feature engineering concepts**: You'll learn the fundamental principles behind creating new features or modifying existing ones to improve model performance.\n\n3. **Numeric and text feature engineering**: We'll dive into specific techniques for engineering features from both numeric and text data.\n\n4. **Feature selection techniques**: You'll discover methods to choose the most relevant features for your model, improving efficiency and performance.\n\n5. **AWS data transformation services**: We'll explore AWS tools and services that can streamline and automate the data transformation process.\n\nThis structure provides a comprehensive overview of data transformation, starting from basic data cleaning to advanced feature engineering techniques, and finally, how to leverage AWS services for these tasks.\n\nSlide 6: Handling Incorrect, Duplicated, and Missing Data\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.html#data-wrangler-transform-handle-missing\n~\n~Image Description:\n~This slide introduces the first topic: handling incorrect, duplicated, and missing data, highlighting its importance in the data cleaning process.\n~\n|INSTRUCTOR NOTES:\n- |Explain why data cleaning is crucial\n- |Discuss common types of data issues\n- |Introduce methods for addressing these issues\n|\n|STUDENT NOTES:\nHandling incorrect, duplicated, and missing data is a critical first step in preparing your dataset for machine learning. This process, often called data cleaning, ensures that your data is accurate, consistent, and complete.\n\nKey aspects of data cleaning include:\n\n1. **Identifying and correcting incorrect data**: This involves spotting and fixing errors in your dataset, such as typos or impossible values.\n\n2. **Removing or consolidating duplicates**: Duplicate entries can skew your analysis and model training, so it's important to identify and address them.\n\n3. **Dealing with missing data**: Missing values can significantly impact model performance. Strategies include imputation (filling in missing values) or removing rows/columns with too many missing values.\n\nRemember, the quality of your data directly impacts the performance of your ML models. Investing time in thorough data cleaning can lead to more accurate and reliable results. In the following slides, we'll explore specific techniques for addressing each of these data quality issues.\n\nSlide 7: Cleaning Data for ML Training\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.html#data-wrangler-transform-handle-missing\n~\n~Image Description:\n~This slide outlines three main aspects of data cleaning: handling incorrect or duplicate data, dealing with outliers, and addressing incomplete or missing data.\n~\n|INSTRUCTOR NOTES:\n- |Explain each aspect of data cleaning in detail\n- |Provide examples of how each issue can affect ML models\n- |Discuss strategies for addressing each issue\n|\n|STUDENT NOTES:\nCleaning data for ML training involves addressing three main types of data issues:\n\n1. **Incorrect or duplicate data**: \n   - Identify and correct errors in your dataset\n   - Remove or consolidate duplicate entries to prevent bias\n\n2. **Data outliers**: \n   - Detect values that significantly differ from other observations\n   - Decide whether to remove, transform, or keep outliers based on their nature and your specific use case\n\n3. **Incomplete or missing data**: \n   - Identify columns or rows with missing values\n   - Choose appropriate strategies to handle missing data (e.g., imputation, deletion)\n\nEach of these issues can significantly impact your model's performance if not addressed properly. **Incorrect data** can lead to faulty predictions, **duplicates** can skew your model's understanding of the data distribution, **outliers** can disproportionately influence model training, and **missing data** can result in biased or less accurate models.\n\nThe goal of data cleaning is to create a **standardized, accurate, and complete dataset** that your ML model can effectively learn from. In the following slides, we'll explore specific techniques for addressing each of these data quality issues.\n\nSlide 8: Incorrect Data\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.html#data-wrangler-transform-handle-invalid\n~\n~Image Description:\n~This slide provides examples of incorrect data issues and techniques to address them.\n~\n|INSTRUCTOR NOTES:\n- |Discuss common types of incorrect data\n- |Explain techniques for identifying and correcting errors\n- |Provide examples of how to handle different types of incorrect data\n|\n|STUDENT NOTES:\nIncorrect data can take many forms and can significantly impact the quality of your ML models. Here are some common types of incorrect data and techniques to address them:\n\n**Examples of incorrect data**:\n- Language differences (e.g., mixed languages in a dataset)\n- Typos and misspellings\n- Inconsistent formats (e.g., date formats, units of measurement)\n\n**Techniques for handling incorrect data**:\n1. **Inspection**: Manually review samples of your data or use automated tools to identify patterns of incorrect data.\n2. **Filling correct values**: Replace incorrect values with the correct ones, if known.\n3. **Fix formatting**: Standardize formats across your dataset (e.g., convert all dates to a single format).\n4. **Exclude from dataset**: If the correct value cannot be determined, it may be necessary to exclude the incorrect data.\n\nRemember, the goal is to create a **clean, consistent dataset** that accurately represents the underlying patterns in your data. Addressing incorrect data is crucial for ensuring the reliability and accuracy of your ML models.\n\nSlide 9: Duplicated Data\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.html#data-wrangler-transform-handle-duplicate\n~\n~Image Description:\n~This slide explains what duplicated data is, provides examples, and introduces deduplication as a technique to address it.\n~\n|INSTRUCTOR NOTES:\n- |Explain what duplicated data is and why it's problematic\n- |Discuss common causes of data duplication\n- |Introduce deduplication techniques\n|\n|STUDENT NOTES:\nDuplicated data refers to identical records appearing multiple times in a dataset. This can skew your analysis and lead to biased ML models. Understanding and addressing duplicated data is crucial for maintaining data integrity.\n\n**Common causes of duplicated data**:\n- Data collected from multiple sources\n- Incorrect data entry processes\n- System glitches during data collection or storage\n\n**Impact of duplicated data**:\n- Overrepresentation of certain data points\n- Skewed distribution of data\n- Biased model training results\n\n**Technique for handling duplicated data**:\n- **Deduplication**: This process involves identifying and removing duplicate records while retaining one copy of the information.\n\nDeduplication can be automated using various tools and algorithms. When implementing deduplication, it's important to consider which copy of the data to keep and whether any additional information can be gleaned from the duplicates before removal.\n\nEffective handling of duplicated data ensures that your dataset accurately represents the underlying population, leading to more reliable and unbiased ML models.\n\nSlide 10: Activity: Identifying Incorrect and Duplicated Data\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: N/A\n~\n~Image Description:\n~This slide presents a dataset with various errors and asks students to identify incorrect and duplicated data.\n~\n|INSTRUCTOR NOTES:\n- |Guide students through the activity\n- |Encourage discussion about different types of errors\n- |Highlight common mistakes in data entry and collection\n|\n|STUDENT NOTES:\nThis activity provides hands-on practice in identifying incorrect and duplicated data. As you review the dataset, consider the following:\n\n1. Look for inconsistencies in formatting or units\n2. Identify values that seem out of place or impossible\n3. Check for repeated entries that might indicate duplication\n\nRemember:\n- Incorrect data can take many forms, from simple typos to more complex issues like misplaced information\n- Duplicated data might not always be exact copies - look for entries that have mostly the same information but might differ slightly\n\nThis exercise will help you develop a critical eye for data quality issues, an essential skill for effective data cleaning and preparation in ML workflows.",
        "PARAMETERS": {
            "temp": 0.5,
            "topP": 1.0,
            "topK": 250,
            "maxT": 200000,
            "stopSeq": ""
        }
    },
    {
        "timestamp": "2025-05-08 12:42:04",
        "MODEL": "Claude 35S",
        "COMMAND": "This is a complex prompt. Take your time to read through the entire prompt and understand every writing guideline and format required. \n\nOnce you are sure of the requirements, proceed.\n\nDo not use Amazon Bedrock cached data.\n\n[Role]\nYou are an AWS Technical Curriculum Developer creating slide notes for a presentation.\nRead the text in the INPUT. This is a text version of a PowerPoint slide presentation.   \nAll text in the INPUT must be treated as content. Read and unsderstand the contents of every slide\n\nCarefully identify the individual slide and \"Slide: nn\" followed by the slide title on the next line.\nIn youre response, the format will be Slide nn: Slide title consistently for every slide.\n\n[variables]\nTarget_audience: Machine learning engineers\nInstructional_level: 200-300 \nSpecific_AWS_services: Amazon SageMaker AI\n/variables\n\n[Task]\nPerform this work ONLY for slides 1-10. \nWhen you complete the last slide in this range, stop processing.\n\nYour audience is {{Target_audience}} who are {{Instructional_level}} in {{Specific_AWS_services}}.\n\nGenerate concise student notes for each PowerPoint slide that:\n1. Captures the essential concepts (maximum 3-4 sentences)\n2. Do not reveal information from upcoming slides\n3. Provide context only for THIS slide's content\n4. Use clear, direct language without unnecessary elaboration\n5. Include only relevant examples that reinforce the current slide's concepts\n\nUse existing notes and slide content as your primary source of information.\nExpand on existing slide notes when required, but do not use verbose language.\nFor long Student Notes, edit to 500 words maximum for each slide's student notes.\n\n[Output]\nThe complete SPEAKERNOTES: for each slide explicitly following the format and example below.  Every slide must contain every section of the SPEAKERNOTES.\n\n[Guidelines]\n1.  Your response shall be the entire notes text for all the slides specified.\n2.  FOLLOW the FORMAT explicitly and consistently for every slide.\n3.  Write clearly and concisely, ensuring technical accuracy. \n4.  Reference https://docs.aws.amazon.com for {{Specific_AWS_services}} details. \n5.  Provide valuable insights beyond what's visible on the slide. \n6.  Include transitional statements when changing topics, especially when moving from one section to the next. \n7.  Write in second person only.  Only use \"we\" to describe Amazon or AWs.\n8.  Add *italics* markdown to introduce a concept to the student notes.\n9. Use numbered list for a sequence of steps, unordered lists otherwise.\n\nFor Image Description, create a brief, accurate description for the image on this slide that:\n1. Describes what the image actually shows (no assumptions)\n2. Explains how the image directly relates to the slide's key point\n3. Uses no more than 2 sentences\n4. Avoids explanations that should be in the main student notes\n5. Begins with \"This image shows...\" or \"This diagram illustrates...\"\n\nDO NOT make statements like \"This slide is about x\" or \"This slide does that\" or \"This slide introduces...\" or \"This slide demonstrates...\"\n\n[FORMAT]\nSlide nn: Slide Title [in exactly this format]\nSPEAKERNOTES:\n~Developer Notes:\n~[Reference materials and URLs - live, accessible, relevant links]\n~\n~Image Description:\n~[Detaied visual description of the slide content or images.]\n~\n|INSTRUCTOR NOTES:\n- |[Key topics to cover]\n- |[Key topics to cover]\n- |...\n|\n|STUDENT NOTES:\n[3-4 clear, concise, and complete paragraphs expanding on slide content.]\n[300 words maximum]\n\n[EXAMPLE]\nSlide 1: Introduction to AWS Generative AI and ML\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/whitepapers/latest/aws-overview/machine-learning.html\n~\n~Image Description:\n~A diagram showing the AWS ML stack, including SageMaker, Comprehend, and Rekognition\n~\n|INSTRUCTOR NOTES:\n- |Introduce AWS Generative AI and ML services\n- |Highlight key benefits and use cases\n- |Discuss integration with other AWS services\n|\n|STUDENT NOTES:\nAWS Generative AI and Machine Learning services provide powerful tools for creating intelligent applications. These services include Amazon SageMake AI for end-to-end ML workflows, Amazon Comprehend for natural language processing, and Amazon Rekognition for image and video analysis.\n\nGenerative AI models, such as those powering Amazon Bedrock, can create new content like text, images, and code based on input prompts. This technology opens up exciting possibilities for content creation, language translation, and creative assistance across various industries.\n\nAWS ML services integrate seamlessly with other AWS offerings, allowing for scalable, *cost-effective*, and *secure* AI/ML solutions. By leveraging the AWS cloud infrastructure, businesses can easily deploy and manage ML models, process large datasets, and incorporate AI capabilities into their applications without the need for extensive in-house expertise or infrastructure.\n\n",
        "INPUT": "Slide: 1\nModule 3: Data Transformation and Feature Engineering\nMachine Learning Engineering on AWS\nSpeaker Notes:\n|Student notes\nAt this point, you are familiar with data collection, ingestion, and storage. You've covered data fundamentals such as data types and formats, and you've been introduced to core AWS storage services. You can choose the most effective AWS storage decision and data format based on your specific machine learning (ML) needs. \nIn this module, you\u2019ll learn about data transformation. This module covers various transformation concepts and techniques, such as data cleaning, encoding, and feature engineering. You\u2019ll discover how to use AWS services such as Amazon SageMaker Feature Store, Amazon SageMaker Data Wrangler, and AWS Glue to transform your data.\nSlide: 2\nModule objectives & outline\nOn completion, you will be able to:\nExplain the value of data cleaning and transformation.\nDescribe how to process incorrect or duplicated data.\nDescribe how to detect and treat outliers.\nDescribe how to process missing values.\nDescribe fundamental encoding techniques.\nIdentify feature engineering use cases.\nDescribe fundamental concepts, benefits, and techniques of feature engineering.\nDescribe fundamental feature selection techniques.\nDescribe AWS services for validating and labeling data.\nIdentify AWS tools and services for visualizing and transforming data.\nSpeaker Notes:\n|Student Notes\nSlide: 3\nData processing: Data transformation and feature engineering\nBusiness goals\nML problem framing\nData processing\nModel development\nDeployment\nMonitoring\nSpeaker Notes:\n|Student notes\nDuring this phase of the machine learning lifecycle, you\u2019re processing your data in preparation for model development. By this point in the data preparation process, you\u2019ve collected, ingested, extracted, and stored your data as appropriate for your business\u2019s ML needs. Now, you are ready to transform that data into a clean, encoded, engineered dataset. Data cleaning, categorical encoding, and feature engineering are important techniques you can use to help ensure a more accurate and model-ready dataset.\nSlide: 4\nTechniques for data transformation\nData cleaning\nCategorical encoding\nFeature engineering\nThe label encoding examples in the source are not good ones. For example, by encoding countries with numbers like 1, 2, 3 you are assigning some countries higher numerical weight which will impact the model. Countries should be one-hot encoded so that they have the same weight. A better example for label encoding would be small, medium, large sizes as 1,2,3  since that preserves a relative weighting.  \nSpeaker Notes:\n|Student notes\nCleaning, encoding, and engineering your dataset is essential to produce more accurate and efficient results from your machine learning model.\nData cleaning \nis an important first step before training an ML model. Raw data often contains errors such as missing values, duplicates, and outliers that need to be identified and handled to ensure data accuracy and quality. \nExample: A few customer IDs are duplicated in a dataset. \nSolution: Remove or consolidate duplicated customer IDs. \nCategorical encoding \nconverts categorical variables into numeric representations that ML algorithms can mathematically interpret. Common encoding techniques include one-hot encoding and label encoding. \nExample: Data includes a country feature with values like USA, Canada, and UK.\nSolution: Encode the countries as numbers, like 1, 2, and 3, so the ML algorithm can mathematically compare them.\nFeature engineering \ncreates new features or selects relevant features to help ML algorithms better understand the relationships in the data. Methods such as binning, bag-of-words, and N-gram extract key information from text data. \nExample: Train a model to predict whether a review is positive or negative.\nSolution: Use the bag-of-words method to convert text into counts of word occurrences and represent text data in a numeric form. \nSlide: 5\nAgenda\nTopic A: Handling incorrect, duplicated, and missing data\nTopic B: Feature engineering concepts\nTopic C: Numeric and text feature engineering\nTopic D: Feature selection techniques\nTopic E: AWS data transformation services\nSpeaker Notes:\nSlide: 6\nHandling incorrect, duplicated, and missing data\nTopic A: Handling incorrect, duplicated, and missing data\nTopic B: Feature engineering concepts\nTopic C: Numeric and text feature engineering\nTopic D: Feature selection techniques\nTopic E: AWS data transformation services\nSpeaker Notes:\n|Student notes\nOne of the first steps of preparing your dataset after collection, ingestion, and storage is cleaning up your data. This can be a time-consuming process, but without taking care of flaws in your data, your machine learning (ML) model will not function properly.\nSlide: 7\nCleaning data for ML training\nIncorrect or duplicate data\nData outliers\nIncomplete or missing data\nSpeaker Notes:\n|Student notes\nThe goal for data cleaning is to standardize your dataset. You want to make sure the data is in the language and format that your model expects. Part of this standardization involves checking your dataset for incorrect or duplicated data.\nAnother key method of cleaning up your data is to identify data outliers. Outliers are data points that differ significantly from the majority of the data. \nOutliers in your data can present experimental anomalies or even highlight novel discoveries, so they are not necessarily data points you always want to remove. However, they can make it harder for your model to make accurate predictions. Detecting and examining outliers is important to the data preparation process, because they might be indicating errors in data collection or skewing values.\nIn addition to inspecting your data for incorrect or duplicated information and data outliers, you also want to address any areas with incomplete or missing data. \nSome columns in your dataset might be missing data because of a data collection error. It is also possible that data was not collected on a particular feature until well into the data collection process.\nSlide: 8\nIncorrect data\nExamples\nLanguage differences\nTypos\nInconsistent formats\nTechniques\nInspection\nFilling correct values\nFix formatting\nExclude from dataset\nSpeaker Notes:\n|Student notes\nDatasets often contain incorrect or anomalous data points that need to be addressed before model training. Common issues include language differences, typos from data entry, values that don't conform to expected formats, and more. To fix incorrect data values, inspect your dataset for anomalies. You might need to fill in the correct values, convert invalid formats to valid formats, or exclude the incorrect entries from the dataset to prevent inaccurate model predictions.\nSlide: 9\nDuplicated data\nExamples\nCopies from multiple sources\nIncorrect data entry\nTechniques\nDeduplication\nSpeaker Notes:\n|Student notes\nDuplicated data refers to records in a dataset that contain the same information. Duplicated data can occur when data is gathered from multiple sources or is entered incorrectly. After your data is fully collected, it's important to inspect the data for duplicate records and then delete or consolidate the duplicate entries. When a dataset contains multiple copies of the same data points, it skews the distribution of the data and over-represents certain examples.\nThe process of automating data duplication removal is called \ndeduplication\n. Deduplication works by scanning datasets for duplicate information, retaining one copy of the information, and replacing the other instances with pointers that direct back to the stored copy. Deduplication can drastically increase storage capacity by keeping only unique information in your data repositories.\nSlide: 10\nActivity: Identifying incorrect and duplicated data\nSpeaker Notes:\n|Student notes\nThe research team is working with the following set of data. Which errors do you notice in this dataset? You might notice that there is missing data, inconsistent formatting, or misplaced information. Consider how to identify and fix incorrect and duplicated data.\nSlide: 11\nActivity: Identifying incorrect and duplicated data\nSpeaker Notes:\n|Student notes\nBaccalaur\u00e9at\n \nis incorrect because it is in a different language than the rest of the dataset. Although the data is placed correctly in the \nEducation\n column, the word \nbaccalaur\u00e9at\n is a French word for a high school diploma. The data would need to be translated to English so the dataset has a standard language to process.\nMasters/PhD\n is incorrect because it is placed in the wrong column. \nMasters/PhD\n is a valid answer for the Education column, but is incorrectly placed in the \nState\n column. This data would need to be moved to the Education column, and you would need to impute the missing data for the State column.\nThe last row is an example of duplicated data. You might recognize that the Participant ID of 123456 is the biggest marker for duplication. There is already a row of data that contains 123456 as the ID.  You need to remove this entire row of data to keep data weights balanced and prevent bias.\nSlide: 12\nDetecting outliers by using mean and median\nMean\nMedian\nMiddle value of dataset\nBetter metric when data is skewed\nConsider both natural and artificial outliers.\nAverage of all values\nUseful when data is symmetrical\nAge\nCount\nMedian\nMean\nSpeaker Notes:\n|Student notes\nThere are a number of different methods you can use to detect and measure the impact of outliers in your data. These methods include calculating mean and median and determining if the outlier is natural or artificial. \nThese measures describe the extent to which your data is clustered around some value or position. \nThe \nmean\n is the average of all the values in the dataset. Mean can be a useful method for understanding your data when the data is symmetrical. For example, a symmetrical set of data that contains ages of respondents might reflect that both the mean and median of the dataset is 50 years old. However, if your data contains outliers, the mean can be skewed one way toward those values. In that case, your mean would not serve as an accurate representation of where your values are truly centered.\nThe \nmedian\n is the value in the dataset that divides the values into two equal halves. To calculate the median, you arrange all the numerical values in ascending order and identify the middle value. If your data is skewed or contains outliers, the median tends to provide the better metric for understanding your data as it relates to central tendency. \nFor example, a dataset that contains income level might contain outliers. The mean might skew toward higher or lower values, while the median would provide a more accurate picture of the data's central tendency.\nOutliers can be caused by either natural or artificial factors. A natural outlier is not the result of an artificial error but is reflective of some truth in the data.\nNatural outliers \nare data points that are accurate representations of data, but are extreme variations of the central data points. For example, in a dataset that includes height measurements of individuals, an extremely tall individual would represent a natural outlier.\nArtificial outliers \nare anomalous data points in your dataset due to error or improper data collection. For example, a faulty sensor in a thermometer might produce a body temperature that is unrealistically high or low compared to expected body temperatures. \nSlide: 13\nAddressing incomplete or missing data\nExploration\nIdentify missing values\nDetermine why values are missing\nMissing at Random (MAR)\nMissing Completely at Random (MCAR)\nMissing Not at Random (MNAR)\nTechniques\nDrop rows with missing values\nImpute values\nSpeaker Notes:\n|Student notes\nUnfortunately, most ML algorithms cannot deal with missing values automatically. You need to use human intelligence to replace missing values with something meaningful and relevant to the problem.\nA few missing values might not be an issue. However, if you\u2019re missing many values in one column, you might find it difficult to interpret the relationship between that feature and the target. In other words, it can be hard to understand how a value in a row corresponds to what needs to be predicted by the model. There are some key steps you can take to address incomplete and missing values in your dataset.\nIdentify missing values \n\u2013There are certain Python libraries, such as Pandas, that you can use to check for missing values. You can also use AWS services to assist you in this process. For example, Amazon SageMaker Autopilot provides a number of filling methods to handle missing values, and Amazon SageMaker Data Wrangler is a full-service data preparation tool. You will learn about SageMaker Data Wrangler in more detail later in this course.\nDetermine why values are missing \n\u2013Before you can determine how to treat the missing values, it\u2019s important to investigate which mechanisms caused the missing values. The following are three common types of missing data:\nMissing at Random (MAR): The probability that a data point is missing depends only on the observed data, not the missing data. Example: In a dataset of student test scores, scores are missing for some students who were absent that day. Absence is related to performance. \nMissing Completely at Random (MCAR): The probability that a data point is missing does not depend on the observed or unobserved data. Example: In an employee survey, some people forgot to answer the question about their number of siblings. Their missing sibling data does not depend on any values.\nMissing Not at Random (MNAR): The probability that a data point is missing depends on the missing data itself. Example: In a financial audit, companies with accounting irregularities are less likely to provide complete records. The missing data depends on the sensitive information being withheld.\nDrop missing values \u2013 \nDepending on what is causing your missing values, you will decide to either drop the missing values or impute data into your dataset. One of the most straightforward ways to deal with missing values is to remove the rows of data with missing values. You can accomplish this by using a Pandas function. Dropping rows or columns will make the dataset non-missing. However, the risk of dropping rows and columns is significant. If you have hundreds of rows or columns of data, all of that missing data might cause bias in your model predictions. If you drop too much data, you might not have enough features to feed the model.\nImpute values \n\u2013 Missing values might be related to new features that haven\u2019t included your dataset yet. After you include more data, those missing values might be highly correlated with the new feature. In this case, you would deal with missing values by adding more new features to the dataset. If you determine the values are missing at random, data imputation, or inputting the data into your dataset, is most likely the best option. There are different ways to impute a missing value. One common way to impute missing values is to replace the value with the mean, median, or most frequent value. You would select the mean, median, or most frequent value for categorical variables. You would select the mean or median for numerical variables. Choosing the mean, median, or most frequent value depending on your business problem and data collection procedures. \nWhen there is a missing value, always consider whether those values are missing at random. If they are not, try to investigate what the mechanism is behind the missing value. You can often find more efficient ways to impute those missing values. Remember that a majority of ML algorithms can\u2019t take missing values. You will most likely need to either drop the values or impute data for the ML model to perform.\nSlide: 14\nQuestion 1\nA real estate company is working on a machine learning project where the goal is to predict housing prices. The team has a dataset that contains information on the number of bedrooms, the square footage, and the locations of different houses. Before building a machine learning model, the team wants to prepare and transform the data. \nWhich data preparation technique could convert values for the home's location (with values of urban, suburban, and rural) into numeric representations that can be fed into a machine learning algorithm? \nResponse\nA\nData cleaning\nB\nFeature engineering\nC\nCategorical encoding\nD\nDimensionality reduction\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember which data preparation technique converts string values into numeric representations. \nSlide: 15\nQuestion 1 answer\nA real estate company is working on a machine learning project where the goal is to predict housing prices. The team has a dataset that contains information on the number of bedrooms, the square footage, and the locations of different houses. Before building a machine learning model, the team wants to prepare and transform the data. \nWhich data preparation technique could convert values for the home's location (with values of urban, suburban, and rural) into numeric representations that can be fed into a machine learning algorithm? \nResponse\nA\nData cleaning\nB\nFeature engineering\nC correct\nCategorical encoding\nD\nDimensionality reduction\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nC: Categorical encoding can be used to convert values for the home's location into numeric representations. For example, you could have 0 for urban, 1 for suburban, and 2 for rural. This allows machine learning algorithms to mathematically interpret the data.\nThe incorrect responses are:\nA: Data cleaning focuses on handling issues like missing data and outliers.\nB: Feature engineering focuses on modifying or creating new features from the data, rather than encoding features.\nD: Dimensionality reduction techniques reduce the number of features but don't convert data types.\nSlide: 16\nQuestion 2\nA non-profit organization is gathering data to decide which type of fundraiser their patrons would like to attend. They decide to provide two options for patrons to share their preferences: an online survey and a hard-copy mailer. Some patrons plan to fill out both the online and hard-copy survey to make sure their preference is recorded.\nWhich data cleaning technique will BEST help the company retain unique data points for the most effective machine learning (ML) solution?\nResponse\nA\nReformatting the hard-copy mailer to contain only \"Yes/No\" answers\nB\nInspecting the data for incorrectly entered dates\nC\nIdentifying duplicate data from those who submitted both an online and hard-copy survey\nD\nChecking the datasets for mistyped patron names\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember that duplicate data must be removed from a training dataset to avoid model bias.\nSlide: 17\nQuestion 2 answer\nA non-profit organization is gathering data to decide which type of fundraiser their patrons would like to attend. They decide to provide two options for patrons to share their preferences: an online survey and a hard-copy mailer. Some patrons plan to fill out both the online and hard-copy survey to make sure their preference is recorded.\nWhich data cleaning technique will BEST help the company retain unique data points for the most effective machine learning (ML) solution?\nResponse\nA\nReformatting the hard-copy mailer to contain only \"Yes/No\" answers\nB\nInspecting the data for incorrectly entered dates\nC correct\nIdentifying duplicate data from those who submitted both an online and hard-copy survey\nD\nChecking the datasets for mistyped patron names\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nC: Because the company has data points from multiple formats, there is a likelihood of duplication in their dataset. It's important to identify duplication to prevent the ML solution from overrepresenting certain examples.\nThe incorrect responses are:\nA,B,D: These options would not address the probable duplication in the dataset. Because the company has data points from multiple formats, there is a likelihood of duplication in their dataset. It's important to identify duplication to prevent the ML solution from overrepresenting certain examples.\nSlide: 18\nQuestion 3\nYou are cleaning up a dataset that includes the body weights of patients at a medical clinic. A clinician tells you that they usually see adults, but children occasionally visit the clinic.\nWhich data measurement BEST provides the central tendencies of your data?\nResponse\nA\nYou would choose the mean, because the best measurement of data that might contain outliers is taking the average of all data entries.\nB\nYou would choose the median, because the best measurement of data that might contain outliers is sorting the data and dividing the dataset at the middle variable.\nC\nBecause the data is numerical, you can use any method of measurement to find the average weight of a patient.\nD\nThere is no measurement you can use to find central tendencies of data.\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember the best measurement to find the central tendencies when the data is skewed.\nSlide: 19\nQuestion 3 answer\nYou are cleaning up a dataset that includes the body weights of patients at a medical clinic. A clinician tells you that they usually see adults, but children occasionally visit the clinic.\nWhich data measurement BEST provides the central tendencies of your data?\nResponse\nA\nYou would choose the mean, because the best measurement of data that might contain outliers is taking the average of all data entries.\nB correct\nYou would choose the median, because the best measurement of data that might contain outliers is sorting the data and dividing the dataset at the middle variable.\nC\nBecause the data is numerical, you can use any method of measurement to find the average weight of a patient.\nD\nThere is no measurement you can use to find central tendencies of data.\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nB: Because the clinic mostly sees adults, a few entries of children's weights might skew the average patient weight. In this case, it is best to use median to better understand the central tendencies of your data.\nThe incorrect responses are:\nA,C: Because the clinic mostly sees adults, a few entries of children's weights might skew the average patient weight. The mean of all patient data would skew towards the outlier values.\nD: You can use measurements such as mean and median to find central tendencies of your data.\nSlide: 20\nQuestion 4\nAn apparel company is building a machine learning solution to determine which types of products are not selling well. They've collected data on how long products sit in inventory, but their dataset contains many missing values.\nWhich method is the most effective option for addressing missing values in the apparel company's dataset?\nResponse\nA\nInspecting the reason for the missed values, and dropping, leaving, or imputing data accordingly\nB\nDropping all the data that contains missing values\nC\nManually imputing every missing value\nD\nLetting their ML algorithm replace the missing values\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember the steps that should be followed when there is missing data. \nSlide: 21\nQuestion 4 answer\nAn apparel company is building a machine learning solution to determine which types of products are not selling well. They've collected data on how long products sit in inventory, but their dataset contains many missing values.\nWhich method is the most effective option for addressing missing values in the apparel company's dataset?\nResponse\nA correct\nInspecting the reason for the missed values, and dropping, leaving, or imputing data accordingly\nB\nDropping all the data that contains missing values\nC\nManually imputing every missing value\nD\nLetting their ML algorithm replace the missing values\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nA: It's important to understand why the data is missing. Then, the company can set mechanisms in place to drop rows or columns of data, leave empty values, or impute data as needed.\nThe incorrect responses are:\nB: It's important to understand why the data is missing. Then, the company can set mechanisms in place to drop rows or columns of data, leave empty values, or impute data as needed. Dropping all missing values could remove significant and relevant data.\nC: This method is time-intensive. It's important to understand why the data is missing. Then, the company can set mechanisms in place to drop rows or columns of data, leave empty values, or impute data as needed.\nD: ML algorithms have trouble with missing values. While you can use AWS services to help locate and replace missing values, ML models themselves are not the best option for replacing missing values.\nSlide: 22\nFeature engineering concepts\nTopic A: Handling incorrect, duplicated, and missing data\nTopic B: Feature engineering concepts\nTopic C: Numeric and text feature engineering\nTopic D: Feature selection techniques\nTopic E: AWS data transformation services\nSpeaker Notes:\n|Student notes\nDuring the data transformation process, it is likely that you will come across different data formats. Remember, machine learning algorithms prefer numbers, so data that is text-based needs to be converted to numbers if possible. Categorical encoding is the process of manipulating text-based variables into number-based variables.\nAfter your data has been cleaned up and you\u2019ve encoded your data as necessary for your model, you can fine-tune or create new features in your dataset through feature engineering. Feature engineering is a method for transforming raw data into more informative features that help models better capture underlying relationships in the data.\nSlide: 23\nFeature engineering requirements\nUnderstanding the raw data\nRecognizing the business problem\nSelecting the right ML algorithm\nSpeaker Notes:\n|Student notes\nFeature engineering is more of an art than a science. Often, feature engineering is not a set of rules, and your approach will be based on the specific business problem that you are trying to solve.\nFeature engineering is often a highly intuitive practice. Understanding the raw data, familiarity with your business problem, and intuition of how to bridge the data and the algorithm work in parallel to determine your feature engineering requirements. Humans are skilled at finding relationships between the problem and the input feature of the raw data, so lean into your intuition when employing feature engineering.\nSlide: 24\nFeature engineering use cases \nNumeric feature engineering\nWide range or high variance\nOutliers or skewed distribution\nTime-based numeric features\nText feature engineering\nCategorical text\nLarge chunks of text\nComplex relationships\nSpeaker Notes:\n~Dev notes\n~Alt text from digital: Graph with mostly concentrated plot points and six outliers.\n|Student notes\nDepending on your data\u2019s format, you will select different methods of feature engineering to transform your dataset. \nNumeric feature engineering \ninvolves transforming numeric values for the model and is often accomplished by grouping different numeric values together. Sometimes, ML algorithms have trouble handling large numbers or isolated numbers. Grouping numeric values helps the model to better understand the relationships of these values.\nNumeric feature engineering strategies such as feature scaling, binning, and log transformation can help the ML model better understand the relationships of the data\u2019s numeric values. When deciding whether your dataset could benefit from numeric feature engineering, look for these qualities:\nNumeric features that have a wide range of values or high variance.\nNumeric features with outliers or a skewed distribution are also good targets for techniques like log transforms or binning to reduce the influence of extreme values.\nNumeric features related to time, such as timestamps, are good candidates for extraction of more meaningful metrics like day of the week or hour of the day.\nText feature engineering \ninvolves transforming text for the model and is often accomplished by splitting the text into smaller pieces. When dealing with text data, or corpus data, you can use feature engineering to transform attributes within your data to make those attributes more useful for your ML model. The purpose of text feature engineering is to break your text down into more bite-sized pieces for your model to understand the weights and relationships of your raw data.\nText feature engineering strategies, such as bag-of-words and N-gram, can tokenize the value of text data for your ML model to infer the right balance of information from that data. Because text feature engineering improves the ability for your model to understand your data effectively, essentially every type of raw data that includes text can benefit from some type of text feature engineering.\nCategorical features are associated with a group, and can often be represented numerically.\nSlide: 25\nCategorical encoding\nBinary\nTwo options \nNominal\nMultiple categories\nOrdinal\nCategories where order of values matter\nSome ML algorithms do not require encoding.\nSpeaker Notes:\n|Student notes\nWhen it comes to your data, the term numerical refers to values expressed as a measurable number, and categorical refers to features that are associated with a group. For example, in a dataset that contains animal medical records, a cat\u2019s weight is considered numerical, and the breed of cat is considered categorical.\nThe following list provides different types of categorical values.\nBinary\n categorical values refer to values that are one of two options. For example, a column of data might be true or false, such as if someone attended an event.\nNominal\n, or multi-categorical, values are category values where order does not matter. A set of data that contains different geographic locations, such as states or cities, might be considered multi-categorical.\nOrdinal\n, or ordered, values are category values where the order does matter, like what size of drink you order at a coffee shop: small, medium, or large.\nNot all categorical variables need to be encoded. Depending on your use case, different ML algorithms might not require you to encode your variables. \nFor instance, a random forest model can handle categorical features directly. You would not need to encode values, such as teal, green, and blue, as numeric values.\nSlide: 26\nActivity: Assessing data for categorical encoding\nSpeaker Notes:\n|Student notes\nThe data gathered by the research scientist team includes various types of numerical and categorical data. \nSlide: 27\nActivity: Assessing data for categorical encoding\nSpeaker Notes:\n|Student notes\nThe \nage\n column contains numerical data. Because this data is already set as numeric values, there is no need for categorical encoding.\nThe \neducation\n column is categorical data. Depending on which type of ML algorithm you decide to use, you might need to encode this data before using it to train and inform your model.\nThe \nflu shot \ncolumn is binary categorical data because respondents answered Yes or No. Depending on the type of algorithm you decide to use, this data might need to be encoded for your model to read the data more easily. For example, you can encode Yes to 0 and No to 1.\nThe \noutbreak zone \ncolumn is also binary categorical data. In this case, the research scientists could have also offered more value options for this feature that would be categorized as ordered, or ordinal, data. For instance, the scientists could categorize outbreak by light, medium, or heavy outbreak, rather than a binary Yes or No outbreak. Either way, this column might need to be encoded for model training, depending on the algorithm you decide to use. \nSlide: 28\nQuestion 1\nAn automobile company wants to use a machine learning solution to determine how quickly their various sizes of vehicles reach their highest speeds. The vehicle sizes are compact, mid-size, and full-size. \nWhich type of categorical data BEST describes the feature of their dataset that lists the size of the vehicle?\nResponse\nA\nNumerical\nB\nNominal\nC\nOrdinal\nD\nBinary\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember the type of categorical values that are ranked in order. \nSlide: 29\nQuestion 1 answer\nAn automobile company wants to use a machine learning solution to determine how quickly their various sizes of vehicles reach their highest speeds. The vehicle sizes are compact, mid-size, and full-size. \nWhich type of categorical data BEST describes the feature of their dataset that lists the size of the vehicle?\nResponse\nA\nNumerical\nB\nNominal\nC\nOrdinal\nD\nBinary\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nC: Ordinal data is the correct type of categorical data here, because the vehicle sizes like compact, midsize, and full-size are categorical labels with a numeric order. Ordinal data describes data that is categorical, but ranked in a numeric order.\nThe incorrect responses are:\nA: Numerical data represents quantitative values. The vehicle sizes (compact, mid-size, full-size) are qualitative categories, not numerical values.\nB: This question is asking about the categorical data that would describe the vehicle sizes. The vehicle sizes, such as compact, midsize, and full-size, would be considered ordinal data because they are categorical values that have a specific size order.\nD: Binary data represents categories with only two possible values. The vehicle sizes have three possible categories: compact, mid-size, full-size.\nSlide: 30\nQuestion 2\nAn energy company wants to assess how often their customers have to replace their solar panels after their initial solar panel purchase. Their raw data includes the dates of first purchase, customer reviews, and dates of additional purchases.\nHow might this company use feature engineering to help the machine learning model understand the relationship between these features in their dataset?\nResponse\nA\nThis is not a good use case for feature engineering, because the data includes a text-based feature.\nB\nThe company can use numeric feature engineering to extract information from the purchase dates, and they can use text feature engineering to extract information from the customer reviews.\nC\nThe company can only use numeric feature engineering to extract information from this dataset.\nD\nThe company can only use text feature engineering to extract information from this dataset.\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember that machine learning models find patterns in historical data and use that to make predictions.\nSlide: 31\nQuestion 2 answer\nAn energy company wants to assess how often their customers have to replace their solar panels after their initial solar panel purchase. Their raw data includes the dates of first purchase, customer reviews, and dates of additional purchases.\nHow might this company use feature engineering to help the machine learning model understand the relationship between these features in their dataset?\nResponse\nA\nThis is not a good use case for feature engineering, because the data includes a text-based feature.\nB correct\nThe company can use numeric feature engineering to extract information from the purchase dates, and they can use text feature engineering to extract information from the customer reviews.\nC\nThe company can only use numeric feature engineering to extract information from this dataset.\nD\nThe company can only use text feature engineering to extract information from this dataset.\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nB: Extracting information from these different features might help to show the relationship between initial purchase, review sentiment, and any additional purchases.\nThe incorrect responses are:\nA: Both the numeric and text-based data in this dataset are good candidates for feature engineering.\nC: The company can use text feature engineering to extract meaningful information from customer reviews..\nD: The company can use numeric feature engineering to extract meaningful information from initial and additional purchase dates.\nSlide: 32\nNumeric and text feature engineering\nTopic A: Handling incorrect, duplicated, and missing data\nTopic B: Feature engineering concepts\nTopic C: Numeric and text feature engineering\nTopic D: Feature selection techniques\nTopic E: AWS data transformation services\nSpeaker Notes:\n|Student notes\nIn the last section, you learned about the difference between numeric and categorical values. Numeric feature engineering involves creating a new feature by combining two or more numeric values. \nText data is an important component in many business problems, and not all text-based features can be easily encoded with categorical encoding. Text feature engineering involves transforming the text in your data so that your ML model can better understand the relationships within that data.\nSlide: 33\nNumeric feature engineering methods\nFeature scaling\nBinning\nLog transformation\nSpeaker Notes:\n|Student notes\nTypes of numeric feature engineering methods include feature scaling, binning, and log transformation.\nFeature scaling, also known as data scaling or normalization, aims to transform the numeric values so that all values are on the same scale. This method helps you to take large numbers and scale them down, so that the ML algorithm can achieve quicker computations and avoid skewed results.\nBinning is a technique to group numerical features into a smaller number of bins, or categories. The data is divided into these bins based on value ranges, thus transforming a numeric feature into a categorical one.\nLog transformation applies a logarithmic function to numeric features in a dataset. A logarithmic function is a mathematical function where the independent variable is the exponent of a fixed number, called the base.\nSlide: 34\nFeature scaling approaches\nNormalization\nRescale values to a common scale (\neg.\n 0 to 1)\nImpacted by outliers\nStandardization\nRescale values to a mean of 0 and a standard deviation of 1\nReduces negative effect of outliers\nSpeaker Notes:\n~Dev notes\n~Alt text: \nGraph with points ranging between 0 and 1\n~Alt text: \nGraph with points centered around 0\n|Student notes\nA lot of ML algorithms are sensitive to wide ranges of data, so feature scaling is required for many ML models. However, some ML models like decision trees are not skewed by wide ranges of numeric data. Whether or not to scale your data will depend on the ML algorithm that you decide to use.\nNormalization\n rescales the values of numeric features to a common scale, often between 0 and 1. This rescaling makes the values more comparable and prevents features with greater magnitudes from incorrectly influencing the model. Outliers can prevent a normalization approach from working effectively.\nStandardization\n is similar to normalization, but instead of scaling values from 0\u20131, it rescales the features to have a mean of 0 and standard deviation of 1. \nStandardization reduces the negative effect of outliers.\nSlide: 35\nFeature scaling example\nSpeaker Notes:\n|Student notes\nAs an example, a dataset considering housing data might contain features such as the number of bedrooms and the price of the house. Number of bedrooms might range from 1 to 5 as a numeric value, but the price might range from $100,000 to $1,000,000.\nNormalizing the house pricing feature with values from 100,000 to 1,000,000 to a 0\u20131 scale would transform the values to be between 0 and 1. This method assigns the smallest value to 0 and the highest value to 1, and uses a calculation to scale down the remaining values.\nStandardization would set the average home price at the integer of 0 and use a calculation to score the remaining values. The calculation smooths out the values, so outliers are not as much of a negative factor with standardization as they are with normalization.\nSlide: 36\nBinning\nFeature is transformed from numeric to categorical\nHouse prices\nBinned house prices\nSpeaker Notes:\n~Dev notes\n~Alt text from digital: \nTwo bar charts: one with bars for features; one with these features grouped into three bars, or bins.\n|Student notes\nBinning can make models more robust by reducing the effect of outliers and allowing nonlinear relationships between features and target values to be captured more easily. It is an effective strategy for numerical data when the exact difference in numbers is not important, but the general range is a factor.\nFor example, you might use binning to engineer the house pricing feature. You might not be concerned about distinguishing between data points of a house price of $150,000 and $151,000, but you might be interested in distinguishing between houses priced at $150,000 and $500,000. \nYou can use a method called quantile binning to evenly distribution values between bins. For example, if you had three bins for your housing data, the lowest 33.3% of data would distribute to one bin, the next 33.3% in the next bin, and highest 33.3% in the remaining bin. Quantile binning results in each bin containing approximately the same number of data points.\nSlide: 37\nLog transformation\nApply a logarithmic function to numeric features\nSpeaker Notes:\n~Dev notes\n~Alt text from digital: \nTwo bar graphs: The first shows bars with many outliers; the second uses log transformation to bring values closer.\n|Student notes\nThe most common logarithmic functions have bases of 10 or e, where e is approximately equal to 2.71828. Logarithmic functions are the inverse of exponential functions and are useful for modeling phenomena where growth decreases over time, like population growth or decay.\nLog transformation is used to normalize skewed numeric data, reduce the effect of outliers, and keep values closer to normal distribution. Log transforming can improve model performance when using numeric features that have highly skewed distributions or outliers.\nLet\u2019s say your housing dataset had extreme outliers due to containing multiple geographic locations. One geographic location has a rural lot listed for $10,000 and another location has a luxury home in the city listed for $10,000,000. Taking the log of the housing prices scales down the large outlier values.\nFor example, the log of $10,000 would be around 4 and the log of $10,000,000 would be around 7. Using this method, the outliers are brought much closer to the normal values in the remainder of the dataset.\nSlide: 38\nText feature engineering methods\nBag of words\nN-gram\nTemporal data\nSpeaker Notes:\n|Student notes\nYou can use feature engineering methods such as bag-of-words and N-gram to engineer your text-based features. Temporal or time series data is data involving time, like a series of dates. Temporal data can come in many formats, and is often a mix of numeric and text data.\nSlide: 39\nBag-of-words\n\u201cdistrict\u201d \u201chas\u201d \u201cgreat\u201d \u201cteachers\u201d \n\u201cand\u201d \u201cgreat\u201d \u201cfacilities\u201d \nCount number of times words appear\nSpeaker Notes:\n|Student notes\nThe bag-of-words model does not keep track of the sequence of words, but counts the number of words in each observation. Bag-of-words uses tokenization to create a statistical representation of the text.\nConsider an example with the housing dataset. Perhaps you want to determine whether the listed home is in a highly rated school district, and you want to include parents\u2019 school reviews as a part of your dataset. The bag-of-words model would take a portion of raw text, break down each piece of text, and tokenize the text to create a table or statistical representation of that review.\nSlide: 40\nN-gram\n\u201cdistrict has\u201d \u201chas great\u201d \u201cgreat teachers\u201d \n\u201cteachers and\u201d \u201cand great\u201d \u201cgreat facilities\u201d \nProduce groups of words of n size\nSpeaker Notes:\n|Student notes\nThe N-gram model builds off of bag-of-words by producing a group of words of n size.\nFor example, if n = 2, a sliding window creates each group of two words as a token pair and creates a statistical representation of these token pairs. This method is useful when we want to find a particular combination of words in the text.\nIn the housing data, you could use the n-gram model to represent how many times reviews include word combinations of \u201cgreat teachers\u201d or \u201cafter-school programs.\"\nSlide: 41\nTemporal data\nExtract features from dates and times\nSpeaker Notes:\n|Student notes\nFeature engineering is important for temporal data because it can help capture key trends, cycles, or patterns over time. Examples of temporal features include time of day, day of the week, and seasons. By carefully extracting and including them in your model, you can help the model to better understand and predict time-based relationships in the data. \nFor example, you might have a feature in your housing dataset that contains the dates of when houses were listed for sale. When you have your dates in a consistent format, you might use feature engineering to create different features that extract the temporal information into day, month, and year. Extracting this information might reveal that most homes are listed in the summer or that most homes are listed at the first of each month.\nSlide: 42\nQuestion 1\nA company is building a machine learning model to predict credit risk. One of the features in the dataset is \nLoanAmount\n, which is likely to contain a few outliers.\nWhich feature engineering technique is a good method for engineering this numeric feature before model training?\nResponse\nA\nTake the log transform of the values to help normalize distribution.\nB\nLeave the values unchanged.\nC\nTake the mean of all values.\nD\nUse one-hot encoding to sort the values.\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember that a log transform can adjust data with outliers to a more normal distribution.\nSlide: 43\nQuestion 1 answer\nA company is building a machine learning model to predict credit risk. One of the features in the dataset is \nLoanAmount\n, which is likely to contain a few outliers.\nWhich feature engineering technique is a good method for engineering this numeric feature before model training?\nResponse\nA correct\nTake the log transform of the values to help normalize distribution.\nB\nLeave the values unchanged.\nC\nTake the mean of all values.\nD\nUse one-hot encoding to sort the values.\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nA: Taking the log transform of numeric features like \nLoanAmount\n can help normalize the distribution and handle skewness. This makes the values more comparable for modeling.\nThe incorrect responses are:\nB: Leaving the values unchanged would not help to normalize the distribution. Taking the log transform of numeric features like \nLoanAmount\n can help normalize the distribution and handle skewness.\nC: Taking the mean of all values can be a helpful method for measuring data, but in this case, the outliers might skew that calculation. Taking the log transform of numeric features like \nLoanAmount\n can help normalize the distribution and handle skewness.\nD: Although one-hot encoding is a great tool for categorical values, this value is numeric. Taking the log transform of numeric features like \nLoanAmount\n can help normalize the distribution and handle skewness.\nSlide: 44\nQuestion 2\nYou work for a travel company and want to build a model to categorize customer reviews as positive or negative.\nWhich text feature engineering technique would be most useful for this classification task?\nResponse\nA\nBag-of-words\nB\nBinning\nC\nLog transformation\nD\nN-gram\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember that n-grams are a useful feature engineering technique for sentiment analysis.\nSlide: 45\nQuestion 2 answer\nYou work for a travel company and want to build a model to categorize customer reviews as positive or negative.\nWhich text feature engineering technique would be most useful for this classification task?\nResponse\nA\nBag-of-words\nB\nBinning\nC\nLog transformation\nD correct\nN-gram\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nD: N-grams are a good technique for text classification tasks like sentiment analysis, because they help capture important phrases and sequences of words, not just individual words. Bag-of-words loses this important context. For a travel review, sequences like \"not clean\" and \"very comfortable\" are important signals. N-grams can help the model understand these types of phrases.\nThe incorrect responses are:\nA: Although bag-of-words represents the occurrence of words in a document, it does not preserve the order and context of the words. Reviews with very different sentiments could have similar bag-of-words representations if they contain the same vocabulary.\nB: Binning is useful for converting continuous numerical variables into categorical bins, not for text feature engineering.\nC: Log transformations are useful for normalizing skewed numerical distributions, not text data.\nSlide: 46\nFeature selection techniques\nTopic A: Handling incorrect, duplicated, and missing data\nTopic B: Feature engineering concepts\nTopic C: Numeric and text feature engineering\nTopic D: Feature selection techniques\nTopic E: AWS data transformation services\nSpeaker Notes:\n|Student Notes\nYou\u2019ve learned about feature scaling and feature engineering to make the most of your dataset and help your model determine relationships in the data. You can also use the technique of feature selection to select the most useful features for your model to make efficient and accurate predictions. \nSlide: 47\nFeature selection\nReduces dimensions\nRemove or select features\nRemove\nSpeaker Notes:\n|Student notes\nFeature selection is the intuitive process of selecting the most relevant features to use in a machine learning model.\nEarlier, you learned about feature scaling. Feature scaling refers to methods that rescale the values of features to a common scale without removing any. Feature selection, on the other hand, reduces dimensions by removing features or selecting certain features so that your model can focus on the correct information. \nIn this example, the housing dataset includes features of the price, the location, the date listed, and the number of bedrooms. You deal primarily with houses in the same geographic location, and you are more interested in the relationship between the number of bedrooms and the price of the house. In that case, you might decide to remove the \u201clocation\u201d and \u201cdate listed\u201d features, so that your model can focus on the most relevant information to the problem.\nSlide: 48\nFeature selection by splitting and combining\nFeature splitting\nFeature combining\nTotal number of bedrooms\nTotal square footage\nSpeaker Notes:\n~Dev notes\n~Alt text:  graphic showing splitting number of bedrooms into small, medium and large features.\n~Alt text:  graphic showing combining individual room sizes into a total square footage feature.\n|Student notes\nIn addition to selecting certain relevant features, you can also use feature splitting and combining to provide the most relevant information to your ML model.\nFeature splitting breaks down features into multiple derived features, expanding the feature space. Feature combining aggregates multiple features into a new combined feature, reducing the feature space. Both techniques can help improve model performance by transforming the original feature set into a more optimal set of predictive features.\nFeature splitting \nFor example, in the housing dataset, you might decide to capture more specific information about bedrooms in each home. The number of bedrooms feature might be split into the number of small, medium, and large bedrooms.\nFeature combining\nOr you might have each room\u2019s square footage categorized in a separate feature. You could combine the room size features to create one complete feature that represents the home\u2019s square footage.\nSlide: 49\nPrincipal component analysis (PCA)\nReduce number of features\nTransform features into principal components \nRetain most of the original information\nRanks components by their variance\nSpeaker Notes:\n|Student notes\nPrincipal component analysis (PCA) is a statistical technique that you can use for dimensionality reduction. In other words, PCA is a method for reducing the number of features in a dataset.\nThe goal of PCA is to retain most of the variation in the original features but reduce the overall number of features to increase computation efficiency. PCA works by transforming the features into a new set of uncorrelated features called principal components. PCA ranks the new set of principal components by their variance, with the first components capturing most of the variance in the data.\nSlide: 50\nActivity: Understanding principal components\nSpeaker Notes:\n|Student notes\nSlide: 51\nActivity: Understanding principal components\nSpeaker Notes:\n|Student notes\nIn the example table, you could reduce five of the features to just two principal components and compare them. The first principal component focuses on house size, and the second component focuses on cost. \nPrincipal component 1: Size\nThe first component accounts for the physical attributes of the home that includes square footage, bedrooms, and bathrooms.\nPrincipal component 2: Cost\nThe second component captures the financial factors that include price and tax rate.\nSlide: 52\nQuestion 1\nA baseball analyst wants to predict a player's batting average using a machine learning model. They have a dataset with over thirty different hitting statistics for each player. \nThe analyst wants to retain necessary information but reduce the number of features before training the model. Which feature selection technique should they use?\nResponse\nA\nFeature splitting\nB\nFeature scaling\nC\nPrincipal component analysis (PCA)\nD\nNo need for feature selection \nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember that PCA reduces features while retaining information. . \nSlide: 53\nQuestion 1 answer\nA baseball analyst wants to predict a player's batting average using a machine learning model. They have a dataset with over thirty different hitting statistics for each player. \nThe analyst wants to retain necessary information but reduce the number of features before training the model. Which feature selection technique should they use?\nResponse\nA\nFeature splitting\nB\nFeature scaling\nC correct\nPrincipal component analysis (PCA)\nD\nNo need for feature selection \nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nC: PCA reduces the dimensionality of the data while retaining most of the variation. This makes it useful for reducing a large set of variables down to a smaller subset of features before modeling.\nThe incorrect responses are:\nA: Feature splitting divides features into multiple datasets, which does not reduce dimensionality.\nB: Feature scaling standardizes features but does not reduce dimensionality.\nD: This example is a good candidate for using PCA. PCA reduces the dimensionality of the data while retaining most of the variation. This makes it useful for reducing a large set of variables down to a smaller subset of features before modeling.\nSlide: 54\nAWS data transformation services\nTopic A: Handling incorrect, duplicated, and missing data\nTopic B: Feature engineering concepts\nTopic C: Numeric and text feature engineering\nTopic D: Feature selection techniques\nTopic E: AWS data transformation services\nSpeaker Notes:\n|Student notes\nData ingestion is the process of collecting data from diverse sources, such as databases, data lakes, and streaming sources. The goal of data ingestion is to consolidate your data to use for your machine learning solution. \nIn this section, you will learn how you can use AWS services, such as Amazon SageMaker Data Wrangler, to ingest your data. You will also learn how you can use Amazon SageMaker Feature Store to ingest data and to create and manage features. \nYou will learn about data transformation use cases of these AWS services. You also learn fundamental steps of how you can use these services to automate part or all of the data transformation process.\nSlide: 55\nIngesting and transforming data for ML on AWS\nAmazon SageMaker Data Wrangler\nAmazon SageMaker Feature Store\nAmazon EMR\nAdd EMR notes from 1.2.13\nDo not add content but leave note here for T&C to revisit this slide since digital content is streaming-specific.\nSpeaker Notes:\n|Student notes\nYou can use AWS services, such as Amazon SageMaker Data Wrangler, to ingest your data. You can also use Amazon SageMaker Feature Store to ingest data and to create and manage features.\nAWS services like SageMaker Data Wrangler and SageMaker Feature Store streamline complicated data ingestion pipelines. Data Wrangler provides a visual, no-code way to transform and clean data from diverse sources. SageMaker Feature Store manages and shares curated features for machine learning model development. Using these services together eliminates much of the manual coding needed for ingesting and wrangling data. \nAWS services like Amazon EMR can help you to transform, enrich, and load data at scale. \nSlide: 56\nBenefits of Amazon SageMaker Data Wrangler\nAmazon SageMaker Data Wrangler\nVisual interface\nDrag-and-drop environment\nBuilt-in algorithms\nIntegration with data sources\nReusable data flows\nSpeaker Notes:\n|Student notes\nData Wrangler streamlines data ingestion through a visual interface to profile, understand, clean, and transform data using built-in recipes. It connects to sources, transforms, cleans, and loads data. Then, it outputs structured, model-ready data to AWS storage without writing code. SageMaker Data Wrangler streamlines complex data preparation so you can focus on building and training ML models, rather than preparing data. \nData Wrangler is a visual, code-free tool for data preprocessing and feature engineering that integrates with AWS data sources. It streamlines machine learning workflows through reusable data flows, large dataset handling, and integration with SageMaker.\nSlide: 57\nUse cases for SageMaker Data Wrangler\nClean data\nFeature engineering\nFix formatting issues\nReduce data size\nAutomate transformations\nSpeaker Notes:\n|Student notes\nData Wrangler streamlines exploratory data analysis and data preprocessing stages for ML learning in a visual, scalable, and collaborative environment. The following are some of the common use cases:\nClean data\n: You can use SageMaker Data Wrangler to interactively explore, visualize, clean, validate, and transform data to prepare it for ML model training. You can use the built-in transforms and visual interface to clean, standardize, and preprocess raw data from various sources like Amazon S3, databases, and more. \nFeature engineering\n: You can use the built-in transform and Python code blocks to engineer new features from your data. For example, you can use SageMaker Data Wrangler to combine columns and apply formulas.\nFixing formatting issues\n: SageMaker Data Wrangler can handle issues like missing headers and encoding problems to get the data into a consistent format for downstream use.\nReducing data size: \nFor large datasets, you can use SageMaker Data Wrangler to select subsets of columns, filter rows, sample data, and convert to lower precision data types. This helps to reduce storage and memory needs.\nAutomating transformations\n: After you interactively build transformation steps in SageMaker Data Wrangler, you can save them as a data recipe or flow to automate preparation for future datasets.\nSlide: 58\nUsing Amazon SageMaker Data Wrangler for data ingestion and transformation\nInput\nOutput\nAmazon \nSageMaker\n Data Wrangler\nIdentify data sources\nCreate a \nnew data flow\nPreview transformations\nOutput results\nSelect built-in transforms to transform data\nSpeaker Notes:\n|Student notes\nIdentify data sources\nUpload your dataset to an S3 bucket. SageMaker Data Wrangler supports CSV, JSON, Parquet, and other common data formats.\nCreate a new data flow\nOpen the SageMaker console and navigate to Data Wrangler. Choose Create flow to start a new data flow. Select the dataset you uploaded to Amazon S3 as the input source. SageMaker Data Wrangler will infer the schema and show a sample of the data. \nSelect built-in transforms to transform data\nIn the Transform section, use the built-in transforms to transform your data. You can see a sample output as you build the transforms. Some common transformations include the following: \nChanging column data types \nFiltering or sorting rows \nFilling in missing values \nParsing columns, like dates \nJoining datasets \nAggregating data, like sums or averages\nPreview transformations\nAutomate conversations, question answering, and workflows based on research into training honest and responsible AI systems.\nOutput results\nGenerate unique, realistic, high-quality images, art, logos, and designs.\nSlide: 59\nBenefits of Amazon SageMaker Feature Store\nAmazon SageMaker Feature Store\nStreamline\nTrack\nConfigure\nRetain\nSpeaker Notes:\n|Student notes\nFeature Store is a managed repository for storing, sharing, and managing features used for ML models. Features can be reused by multiple teams and SageMaker Feature Store keeps them synchronized for both batch training and real-time inference. By centralizing features in a secured store, SageMaker Feature Store improves feature quality and model accuracy at scale across the ML lifecycle. Storing features saves time by eliminating redundant feature engineering efforts.\nFeature Store provides centralized management of feature engineering to streamline machine learning workflows. It provides configurable online and offline storage of feature groups with metadata to enable discovery, reuse, and automation from data ingestion through model training and prediction. \nFeature Store provides a centralized and automated way to engineer, monitor, govern, and reuse features to improve model training and performance across teams. The following are some of the common use cases:\nAutomated data preprocessing: \nSageMaker Feature Store can automatically preprocess raw data as it is ingested. This includes scaling, encoding, transforming, and cleaning data so it is ready for model training.\nCentralized feature repository: \nSageMaker Feature Store acts as a centralized repository for curated features that have been preprocessed and engineered. Features can be reused across teams and models.\nStandardized features\n: SageMaker Feature Store establishes a standardized source of truth for featured data that is consistent across the organization. This avoids issues with different teams using their own feature engineering code.\nManagement of feature pipelines\n: SageMaker Feature Store manages the end-to-end lifecycle of feature pipelines from data ingestion through preprocessing. This provides monitoring and governance of feature engineering.\nCaching for performance\n: Frequently used features are cached in SageMaker Feature Store for faster retrieval during training. This improves performance by avoiding repetitive preprocessing.\nSlide: 60\nBenefits of Amazon EMR\nAmazon EMR\n<list key benefits>\nAdd EMR notes from 1.2.13\nDo not add content but leave note here for T&C to revisit this slide since digital content is streaming-specific.\nSpeaker Notes:\n|Student notes\nSlide: 61\nData formatting for model training in Amazon SageMaker\nSplit data sets.\nTraining\nTesting\nValidation\nAugment or shuffle training data for increased diversity.\nFormat data set.\nCSV\nRecordIO-protobuf\nSpeaker Notes:\n|Student notes\nReducing bias in your dataset is one of the primary objectives in data preparation. With balanced data, the model can focus on meaningful patterns in the dataset rather than any inherent noise.\nA common strategy to achieve a balanced and accurate dataset is to divide the data into distinct subsets for training, testing, and validation. \nData augmentation works by creating new, realistic training examples that expand the model's understanding of the data distribution. Dataset augmentation involves artificially expanding the size and diversity of a dataset by applying a variety of transformations and modifications to existing data samples. These transformations introduce variations in the data while preserving the essential characteristics relevant to the learning task\nData shuffling involves rearranging the order of the data points in a dataset before they are used for training a machine learning model. Shuffling your data helps to verify that the training examples are presented to the model in a random order rather than in a fixed sequence.\nWhen using Amazon SageMaker for your ML workloads, you want to make sure you are providing data that adheres to an accepted format. The two most common formats supported by the SageMaker built-in algorithms are CSV formatting and \nRecordIO-protobuf\n formatting. \nFormatting can be a complex task. You focus more on adhering to certain formatting types during the model training and refining stage of the machine learning lifecycle. However, it is helpful to know the basic concepts and structure of these two types of formats.\nCSV is a widely used text-based data format for training machine learning models. CSV files store tabular data, where each row represents a data sample, and the columns represent the features of that sample. For SageMaker built-in algorithms, the label in the training dataset must be the first column on the left, and the features should be to the right. Many built-in algorithms in SageMaker, such as \nXGBoost\n, linear learner, and \nDeepAR\n, accept CSV formatted data as input for model training. \nRecordIO-protobuf\n is a binary data format used by SageMaker for efficient data storage and transfer. This format is particularly useful when you need to handle large datasets or perform automatic transformations on the data. The \nRecordIO-protobuf\n format consists of a series of records, where each record contains a binary-encoded \nprotobuf\n message. This format is commonly used for image data, where each record represents an image and its associated metadata.\nSlide: 62\nQuestion 1\nYou work for a hospitality company that operates hotels across the country. You want to build a machine learning model to predict customer loyalty using data about past customer stays and purchases. You are considering using SageMaker Feature Store to prepare the data for model training. \nHow could SageMaker Feature Store be an effective service option for this use case?\nResponse\nA\nSageMaker Feature Store is a marketplace where you can pay workers to complete discrete tasks like labeling data. This would verify that your customers\u2019 purchases are accurate.\nB\nYou can use SageMaker Feature Store to visualize and clean your data. Clean data is essential for machine learning accuracy.\nC\nSageMaker Feature Store automatically labels your data. This saves time labeling data for supervised learning.\nD\nYou can use SageMaker Feature Store to store normalized features that can be reused for multiple models. This saves time preparing data for each new model.\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember that Feature Store serves as a centralized repository for features.\nSlide: 63\nQuestion 1 answer\nYou work for a hospitality company that operates hotels across the country. You want to build a machine learning model to predict customer loyalty using data about past customer stays and purchases. You are considering using SageMaker Feature Store to prepare the data for model training. \nHow could SageMaker Feature Store be an effective service option for this use case?\nResponse\nA\nSageMaker Feature Store is a marketplace where you can pay workers to complete discrete tasks like labeling data. This would verify that your customers\u2019 purchases are accurate.\nB\nYou can use SageMaker Feature Store to visualize and clean your data. Clean data is essential for machine learning accuracy.\nC\nSageMaker Feature Store automatically labels your data. This saves time labeling data for supervised learning.\nD correct\nYou can use SageMaker Feature Store to store normalized features that can be reused for multiple models. This saves time preparing data for each new model.\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nD: You can use SageMaker Feature Store to save time by storing curated features that can be accessed to train new models.\nThe incorrect responses are:\nA: Amazon Mechanical Turk is a service for hiring workers to complete tasks like labeling data. Feature Store helps manage and share features, not label data. You can use SageMaker Feature Store to store curated features that can be accessed to train new models.\nB: Although clean data is essential for ML accuracy, SageMaker Data Wrangler is a service that assists with data visualization and transformation. You can use SageMaker Feature Store to save time by storing curated features that can be accessed to train new models.\nC: SageMaker Ground Truth is a separate service for labeling data. SageMaker Feature Store helps manage and share features, not label data. You can use SageMaker Feature Store to save time by storing curated features that can be accessed to train new models.\nSlide: 64\nQuestion 2\nA telecommunications company wants to use Amazon SageMaker to train a machine learning model on their customer data. They have the data stored in different formats.\nWhich data formats do most Amazon SageMaker algorithms support for training? \nResponse\nA\nCSV and \nRecordIO-protobuf\nB\nAll data formats\nC\nNo data formats\nD\nCustom data formats\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember that SageMaker supports CSV and \nRecordIO-protobuf\n formats for training data.\nSlide: 65\nQuestion 2 answer\nA telecommunications company wants to use Amazon SageMaker to train a machine learning model on their customer data. They have the data stored in different formats.\nWhich data formats do most Amazon SageMaker algorithms support for training? \nResponse\nA correct\nCSV and \nRecordIO-protobuf\nB\nAll data formats\nC\nNo data formats\nD\nCustom data formats\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nA: Many Amazon SageMaker algorithms support training with data in CSV and \nRecordIO-protobuf\n format.\nThe incorrect responses are:\nB: Many Amazon SageMaker algorithms support certain data formats.\nC: Many Amazon SageMaker algorithms support certain commonly used data formats. \nD: Many Amazon SageMaker algorithms support certain commonly used data formats. \nSlide: 66\nResources\nTo learn more, choose from the following resources:\nAmazon SageMaker AI \u2013CSV Format\n (developer guide)\nAmazon SageMaker AI \u2013RecordIO Format \n(developer guide)\nTime-Series Datasets Format and Missing Values Filling Methods\n (developer guide)\nCreate, Store, and Share Features with Amazon SageMaker Feature Store\n (developer guide)\nTransform Data\n (developer guide)\nSpeaker Notes:\nSlide: 67\nModule summary\nYou should now be able to:\nExplain the value of data cleaning and transformation.\nDescribe how to process incorrect or duplicated data.\nDescribe how to detect and treat outliers.\nDescribe how to process missing values.\nDescribe fundamental encoding techniques.\nIdentify feature engineering use cases.\nDescribe fundamental concepts, benefits, and techniques of feature engineering.\nDescribe fundamental feature selection techniques.\nDescribe AWS services for validating and labeling data.\nIdentify AWS tools and services for visualizing and transforming data.\nSpeaker Notes:\n|Student notes\nSlide: 68\nQuestions?\nThank you for attending\nthis session\nCorrections, feedback, or other questions? \nContact us at \nhttps://support.aws.amazon.com/#/contacts/aws-training\n. \nAll trademarks are the property of their owners.\nSpeaker Notes:\n|Instructor Notes\n|\n|Student Notes",
        "RESPONSE": "Here are the speaker notes for slides 1-10 as requested:\n\nSlide 1: Introduction to Data Transformation and Feature Engineering\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide shows the title \"Module 3: Data Transformation and Feature Engineering\" along with the course name \"Machine Learning Engineering on AWS\".\n~\n|INSTRUCTOR NOTES:\n- |Introduce the module on data transformation and feature engineering\n- |Highlight the importance of these steps in the ML pipeline\n- |Set expectations for what will be covered\n|\n|STUDENT NOTES:\nAt this point in the machine learning lifecycle, you have collected, ingested, and stored your data. You're familiar with data fundamentals like types and formats, and core AWS storage services. You can choose effective AWS storage solutions and data formats based on your specific ML needs.\n\nThis module focuses on data transformation - a critical step in preparing data for machine learning. You'll learn about key transformation concepts and techniques including data cleaning, encoding, and feature engineering. The module will cover how to leverage AWS services like Amazon SageMaker Feature Store, SageMaker Data Wrangler, and AWS Glue to transform your data efficiently.\n\nUnderstanding these transformation techniques is essential for creating high-quality datasets that lead to accurate and performant machine learning models. The skills covered here will help you take raw data and shape it into a form that ML algorithms can effectively learn from.\n\nSlide 2: Module Objectives & Outline\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide lists the module objectives and outline, showing the key topics to be covered.\n~\n|INSTRUCTOR NOTES:\n- |Review the learning objectives for the module\n- |Highlight the key topics that will be covered\n- |Emphasize the practical skills students will gain\n|\n|STUDENT NOTES:\nThis module will equip you with essential skills for transforming and preparing data for machine learning. By the end, you'll be able to:\n\n- Explain why data cleaning and transformation are crucial steps\n- Handle common data issues like incorrect/duplicate data, outliers, and missing values  \n- Apply fundamental encoding techniques to prepare categorical data\n- Identify use cases for feature engineering and understand its core concepts and benefits\n- Implement basic feature selection techniques to focus on the most relevant data\n- Leverage AWS services for data validation, labeling, visualization and transformation\n\nThe module is structured to cover data cleaning, encoding, feature engineering, feature selection, and relevant AWS tools. This comprehensive approach will give you a solid foundation in data preparation best practices for machine learning.\n\nSlide 3: Data Processing: Data Transformation and Feature Engineering\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide shows a diagram of the machine learning lifecycle, with \"Data processing\" highlighted.\n~\n|INSTRUCTOR NOTES:\n- |Explain where data transformation fits in the ML lifecycle\n- |Highlight the importance of this step for model accuracy\n- |Discuss how this builds on previous data preparation steps\n|\n|STUDENT NOTES:\nData transformation and feature engineering are crucial steps in the machine learning lifecycle, occurring after data collection, ingestion, and storage. At this stage, you're processing your data to prepare it for model development.\n\nThe goal is to transform raw data into a clean, encoded, and engineered dataset that will lead to more accurate models. This involves techniques like:\n\n- *Data cleaning* to handle errors, duplicates, and inconsistencies\n- *Categorical encoding* to convert text data into numeric formats\n- *Feature engineering* to create new informative features or modify existing ones\n\nThese techniques help ensure your data is in the optimal format for machine learning algorithms to extract meaningful patterns and relationships. Effective data transformation can significantly improve model performance and reliability.\n\nSlide 4: Techniques for Data Transformation\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide lists three main techniques for data transformation: data cleaning, categorical encoding, and feature engineering.\n~\n|INSTRUCTOR NOTES:\n- |Explain each transformation technique\n- |Provide examples of when each technique is used\n- |Highlight the importance of these techniques for model accuracy\n|\n|STUDENT NOTES:\nData transformation involves three key techniques that prepare your data for machine learning:\n\n1. *Data cleaning* is the first critical step. It involves identifying and handling issues like missing values, duplicates, and outliers. For example, removing duplicate customer IDs or filling in missing values. This ensures data accuracy and quality.\n\n2. *Categorical encoding* converts text-based categorical variables into numeric representations. This is necessary because most ML algorithms work with numbers. For example, encoding country names as numbers. One-hot encoding is often preferred for nominal categories to avoid implying order.\n\n3. *Feature engineering* creates new features or modifies existing ones to help algorithms better understand data relationships. For example, using the bag-of-words method to convert text reviews into word frequency counts, making them usable for sentiment analysis.\n\nThese techniques work together to produce a more accurate and efficient dataset for your ML model. Properly transformed data leads to better model performance and more reliable predictions.\n\nSlide 5: Agenda\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide outlines the agenda for the module, listing the main topics to be covered.\n~\n|INSTRUCTOR NOTES:\n- |Review the agenda for the module\n- |Briefly explain what will be covered in each topic\n- |Set expectations for the flow of the module\n|\n|STUDENT NOTES:\nThis module is structured into five main topics:\n\nA. Handling incorrect, duplicated, and missing data\nB. Feature engineering concepts\nC. Numeric and text feature engineering\nD. Feature selection techniques\nE. AWS data transformation services\n\nWe'll start by addressing common data quality issues, then move into the principles and techniques of feature engineering. You'll learn how to work with both numeric and text data, and how to select the most relevant features for your model. Finally, we'll explore AWS services that can streamline and automate these data transformation processes.\n\nThis comprehensive approach will give you a solid foundation in data preparation for machine learning, covering both theoretical concepts and practical implementation using AWS tools.\n\nSlide 6: Handling Incorrect, Duplicated, and Missing Data\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide highlights \"Topic A: Handling incorrect, duplicated, and missing data\" from the agenda.\n~\n|INSTRUCTOR NOTES:\n- |Introduce the importance of data cleaning\n- |Explain the impact of data quality on model performance\n- |Preview the specific issues to be addressed\n|\n|STUDENT NOTES:\nData cleaning is a critical first step in preparing your dataset for machine learning. Without addressing flaws in your data, your ML model may not function properly or produce accurate results.\n\nIn this section, we'll focus on three key aspects of data cleaning:\n\n1. Handling incorrect data\n2. Identifying and removing duplicates\n3. Addressing missing values\n\nThese issues can significantly impact model performance if left unaddressed. By learning how to effectively clean your data, you'll ensure that your ML model has a solid foundation to work from. This process, while sometimes time-consuming, is essential for creating reliable and accurate models.\n\nSlide 7: Cleaning Data for ML Training\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide lists three main aspects of data cleaning: incorrect or duplicate data, data outliers, and incomplete or missing data.\n~\n|INSTRUCTOR NOTES:\n- |Explain the importance of each aspect of data cleaning\n- |Provide examples of how each issue can affect model performance\n- |Discuss strategies for addressing each issue\n|\n|STUDENT NOTES:\nCleaning data for ML training involves addressing three key areas:\n\n1. *Incorrect or duplicate data*: This involves standardizing your dataset to ensure consistency in language and format. Identifying and handling duplicates is crucial to prevent bias in your model.\n\n2. *Data outliers*: Outliers are data points that differ significantly from the majority. While they can sometimes indicate important anomalies, they can also skew model predictions. It's important to detect and carefully examine outliers.\n\n3. *Incomplete or missing data*: Missing data can occur due to collection errors or changes in data collection processes. Addressing missing data is crucial for maintaining the integrity and completeness of your dataset.\n\nBy systematically addressing these issues, you ensure that your data is clean, consistent, and ready for model training. This process helps to reduce bias and improve the overall accuracy and reliability of your machine learning models.\n\nSlide 8: Incorrect Data\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide shows examples of incorrect data and techniques for handling it.\n~\n|INSTRUCTOR NOTES:\n- |Explain common sources of incorrect data\n- |Discuss techniques for identifying and correcting errors\n- |Emphasize the importance of domain knowledge in this process\n|\n|STUDENT NOTES:\nIncorrect data can significantly impact the accuracy of your machine learning models. Common sources of incorrect data include:\n\n- Language differences\n- Typos from data entry\n- Inconsistent formats\n\nTo address these issues, you can use several techniques:\n\n1. *Inspection*: Carefully review your dataset to identify anomalies or inconsistencies.\n2. *Filling correct values*: Replace incorrect values with the correct ones when possible.\n3. *Fix formatting*: Standardize formats across your dataset (e.g., date formats, units of measurement).\n4. *Exclude from dataset*: In some cases, it may be necessary to remove incorrect entries if they can't be reliably corrected.\n\nThe goal is to create a clean, consistent dataset that accurately represents the underlying data. This process often requires domain knowledge and careful consideration of the potential impact of any changes made to the data.\n\nSlide 9: Duplicated Data\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide explains duplicated data and techniques for handling it, focusing on deduplication.\n~\n|INSTRUCTOR NOTES:\n- |Explain what duplicated data is and why it's problematic\n- |Discuss common causes of data duplication\n- |Introduce deduplication techniques\n|\n|STUDENT NOTES:\nDuplicated data refers to records in a dataset that contain the same information. This can occur due to:\n\n- Data being gathered from multiple sources\n- Incorrect data entry\n\nDuplicates can skew the distribution of your data and over-represent certain examples, leading to biased model predictions. To address this, you can use a process called *deduplication*.\n\nDeduplication works by:\n1. Scanning datasets for duplicate information\n2. Retaining one copy of the information\n3. Replacing other instances with pointers to the stored copy\n\nThis process can significantly increase storage capacity by keeping only unique information in your data repositories. It's an important step in ensuring the integrity and balance of your dataset for machine learning.\n\nSlide 10: Activity: Identifying Incorrect and Duplicated Data\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide presents a dataset for an activity on identifying incorrect and duplicated data.\n~\n|INSTRUCTOR NOTES:\n- |Guide students through the activity\n- |Encourage them to identify different types of data issues\n- |Discuss potential solutions for each issue identified\n|\n|STUDENT NOTES:\nIn this activity, you'll practice identifying incorrect and duplicated data in a sample dataset. As you review the data, look for:\n\n- Missing data\n- Inconsistent formatting\n- Misplaced information\n- Duplicate entries\n\nConsider how you would address each issue you identify. For example:\n- How would you handle missing values?\n- What steps would you take to standardize inconsistent formats?\n- How would you correct misplaced information?\n- What process would you use to remove duplicates?\n\nThis exercise will help you develop a critical eye for common data quality issues and start thinking about appropriate strategies for addressing them. Remember, the goal is to create a clean, consistent dataset that will lead to more accurate and reliable machine learning models.",
        "PARAMETERS": {
            "temp": 0.5,
            "topP": 1.0,
            "topK": 250,
            "maxT": 200000,
            "stopSeq": ""
        }
    },
    {
        "timestamp": "2025-05-08 12:46:24",
        "MODEL": "Claude 35S",
        "COMMAND": "This is a complex prompt. Take your time to read through the entire prompt and understand every writing guideline and format required. \n\nOnce you are sure of the requirements, proceed.\n\nDo not use Amazon Bedrock cached data.\n\n[Role]\nYou are an AWS Technical Curriculum Developer creating slide notes for a presentation.\nRead the text in the INPUT. This is a text version of a PowerPoint slide presentation.   \nAll text in the INPUT must be treated as content. Read and unsderstand the contents of every slide\n\nCarefully identify the individual slide and \"Slide: nn\" followed by the slide title on the next line.\nIn youre response, the format will be Slide nn: Slide title consistently for every slide.\n\n[variables]\nTarget_audience: Machine learning engineers\nInstructional_level: 200-300 \nSpecific_AWS_services: Amazon SageMaker AI\n/variables\n\n[Task]\nPerform this work ONLY for slides 1-10. \nWhen you complete the last slide in this range, stop processing.\n\nYour audience is {{Target_audience}} who are {{Instructional_level}} in {{Specific_AWS_services}}.\n\nGenerate concise student notes for each PowerPoint slide that:\n1. Captures the essential concepts (maximum 3-4 sentences)\n2. Do not reveal information from upcoming slides\n3. Provide context only for THIS slide's content\n4. Use clear, direct language without unnecessary elaboration\n5. Include only relevant examples that reinforce the current slide's concepts\n\nUse existing notes and slide content as your primary source of information.\nExpand on existing slide notes when required, but do not use verbose language.\nFor long Student Notes, edit to 500 words maximum for each slide's student notes.\n\n[Output]\nThe complete SPEAKERNOTES: for each slide explicitly following the format and example below.  Every slide must contain every section of the SPEAKERNOTES.\n\n[Guidelines]\n1.  Your response shall be the entire notes text for all the slides specified.\n2.  FOLLOW the FORMAT explicitly and consistently for every slide.\n3.  Write clearly and concisely, ensuring technical accuracy. \n4.  Reference https://docs.aws.amazon.com for {{Specific_AWS_services}} details. \n5.  Provide valuable insights beyond what's visible on the slide. \n6.  Include transitional statements when changing topics, especially when moving from one section to the next. \n7.  Write in second person only.  Only use \"we\" to describe Amazon or AWs.\n8.  Add *italics* markdown to introduce a concept to the student notes.\n9. Use numbered list for a sequence of steps, unordered lists otherwise.\n\nFor Image Description, create a brief, accurate description for the image on this slide that:\n1. Describes what the image actually shows (no assumptions)\n2. Explains how the image directly relates to the slide's key point\n3. Uses no more than 2 sentences\n4. Avoids explanations that should be in the main student notes\n5. Begins with \"This image shows...\" or \"This diagram illustrates...\"\n\nDO NOT make statements like \"This slide is about x\" or \"This slide does that\" or \"This slide introduces...\" or \"This slide demonstrates...\"\n\n[FORMAT]\nSlide nn: Slide Title [in exactly this format]\nSPEAKERNOTES:\n~Developer Notes:\n~[Reference materials and URLs - live, accessible, relevant links]\n~\n~Image Description:\n~[Detaied visual description of the slide content or images.]\n~\n|INSTRUCTOR NOTES:\n- |[Key topics to cover]\n- |[Key topics to cover]\n- |...\n|\n|STUDENT NOTES:\n[3-4 clear, concise, and complete paragraphs expanding on slide content.]\n[300 words maximum]\n\n[EXAMPLES]\nSlide 1: Introduction to AWS Generative AI and ML\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/whitepapers/latest/aws-overview/machine-learning.html\n~\n~Image Description:\n~A diagram showing the AWS ML stack, including SageMaker, Comprehend, and Rekognition\n~\n|INSTRUCTOR NOTES:\n- |Introduce AWS Generative AI and ML services\n- |Highlight key benefits and use cases\n- |Discuss integration with other AWS services\n|\n|STUDENT NOTES:\nAWS Generative AI and Machine Learning services provide powerful tools for creating intelligent applications. These services include Amazon SageMake AI for end-to-end ML workflows, Amazon Comprehend for natural language processing, and Amazon Rekognition for image and video analysis.\n\nGenerative AI models, such as those powering Amazon Bedrock, can create new content like text, images, and code based on input prompts. This technology opens up exciting possibilities for content creation, language translation, and creative assistance across various industries.\n\nAWS ML services integrate seamlessly with other AWS offerings, allowing for scalable, *cost-effective*, and *secure* AI/ML solutions. By leveraging the AWS cloud infrastructure, businesses can easily deploy and manage ML models, process large datasets, and incorporate AI capabilities into their applications without the need for extensive in-house expertise or infrastructure.\n\nSlide 10: Activity: Identifying Incorrect and Duplicated Data\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide presents a dataset for an activity on identifying incorrect and duplicated data.\n~\n|INSTRUCTOR NOTES:\n- |Guide students through the activity\n- |Encourage them to identify different types of data issues\n- |Discuss potential solutions for each issue identified\n|\n|STUDENT NOTES:\nIn this activity, you'll practice identifying incorrect and duplicated data in a sample dataset. As you review the data, look for:\n- Missing data\n- Inconsistent formatting\n- Misplaced information\n- Duplicate entries\n\nConsider how you would address each issue you identify. For example:\n- How would you handle missing values?\n- What steps would you take to standardize inconsistent formats?\n- How would you correct misplaced information?\n- What process would you use to remove duplicates?\n\nThis exercise will help you develop a critical eye for common data quality issues and start thinking about appropriate strategies for addressing them. Remember, the goal is to create a clean, consistent dataset that will lead to more accurate and reliable machine learning models.\n\n",
        "INPUT": "Slide: 1\nModule 3: Data Transformation and Feature Engineering\nMachine Learning Engineering on AWS\nSpeaker Notes:\n|Student notes\nAt this point, you are familiar with data collection, ingestion, and storage. You've covered data fundamentals such as data types and formats, and you've been introduced to core AWS storage services. You can choose the most effective AWS storage decision and data format based on your specific machine learning (ML) needs. \nIn this module, you\u2019ll learn about data transformation. This module covers various transformation concepts and techniques, such as data cleaning, encoding, and feature engineering. You\u2019ll discover how to use AWS services such as Amazon SageMaker Feature Store, Amazon SageMaker Data Wrangler, and AWS Glue to transform your data.\nSlide: 2\nModule objectives & outline\nOn completion, you will be able to:\nExplain the value of data cleaning and transformation.\nDescribe how to process incorrect or duplicated data.\nDescribe how to detect and treat outliers.\nDescribe how to process missing values.\nDescribe fundamental encoding techniques.\nIdentify feature engineering use cases.\nDescribe fundamental concepts, benefits, and techniques of feature engineering.\nDescribe fundamental feature selection techniques.\nDescribe AWS services for validating and labeling data.\nIdentify AWS tools and services for visualizing and transforming data.\nSpeaker Notes:\n|Student Notes\nSlide: 3\nData processing: Data transformation and feature engineering\nBusiness goals\nML problem framing\nData processing\nModel development\nDeployment\nMonitoring\nSpeaker Notes:\n|Student notes\nDuring this phase of the machine learning lifecycle, you\u2019re processing your data in preparation for model development. By this point in the data preparation process, you\u2019ve collected, ingested, extracted, and stored your data as appropriate for your business\u2019s ML needs. Now, you are ready to transform that data into a clean, encoded, engineered dataset. Data cleaning, categorical encoding, and feature engineering are important techniques you can use to help ensure a more accurate and model-ready dataset.\nSlide: 4\nTechniques for data transformation\nData cleaning\nCategorical encoding\nFeature engineering\nThe label encoding examples in the source are not good ones. For example, by encoding countries with numbers like 1, 2, 3 you are assigning some countries higher numerical weight which will impact the model. Countries should be one-hot encoded so that they have the same weight. A better example for label encoding would be small, medium, large sizes as 1,2,3  since that preserves a relative weighting.  \nSpeaker Notes:\n|Student notes\nCleaning, encoding, and engineering your dataset is essential to produce more accurate and efficient results from your machine learning model.\nData cleaning \nis an important first step before training an ML model. Raw data often contains errors such as missing values, duplicates, and outliers that need to be identified and handled to ensure data accuracy and quality. \nExample: A few customer IDs are duplicated in a dataset. \nSolution: Remove or consolidate duplicated customer IDs. \nCategorical encoding \nconverts categorical variables into numeric representations that ML algorithms can mathematically interpret. Common encoding techniques include one-hot encoding and label encoding. \nExample: Data includes a country feature with values like USA, Canada, and UK.\nSolution: Encode the countries as numbers, like 1, 2, and 3, so the ML algorithm can mathematically compare them.\nFeature engineering \ncreates new features or selects relevant features to help ML algorithms better understand the relationships in the data. Methods such as binning, bag-of-words, and N-gram extract key information from text data. \nExample: Train a model to predict whether a review is positive or negative.\nSolution: Use the bag-of-words method to convert text into counts of word occurrences and represent text data in a numeric form. \nSlide: 5\nAgenda\nTopic A: Handling incorrect, duplicated, and missing data\nTopic B: Feature engineering concepts\nTopic C: Numeric and text feature engineering\nTopic D: Feature selection techniques\nTopic E: AWS data transformation services\nSpeaker Notes:\nSlide: 6\nHandling incorrect, duplicated, and missing data\nTopic A: Handling incorrect, duplicated, and missing data\nTopic B: Feature engineering concepts\nTopic C: Numeric and text feature engineering\nTopic D: Feature selection techniques\nTopic E: AWS data transformation services\nSpeaker Notes:\n|Student notes\nOne of the first steps of preparing your dataset after collection, ingestion, and storage is cleaning up your data. This can be a time-consuming process, but without taking care of flaws in your data, your machine learning (ML) model will not function properly.\nSlide: 7\nCleaning data for ML training\nIncorrect or duplicate data\nData outliers\nIncomplete or missing data\nSpeaker Notes:\n|Student notes\nThe goal for data cleaning is to standardize your dataset. You want to make sure the data is in the language and format that your model expects. Part of this standardization involves checking your dataset for incorrect or duplicated data.\nAnother key method of cleaning up your data is to identify data outliers. Outliers are data points that differ significantly from the majority of the data. \nOutliers in your data can present experimental anomalies or even highlight novel discoveries, so they are not necessarily data points you always want to remove. However, they can make it harder for your model to make accurate predictions. Detecting and examining outliers is important to the data preparation process, because they might be indicating errors in data collection or skewing values.\nIn addition to inspecting your data for incorrect or duplicated information and data outliers, you also want to address any areas with incomplete or missing data. \nSome columns in your dataset might be missing data because of a data collection error. It is also possible that data was not collected on a particular feature until well into the data collection process.\nSlide: 8\nIncorrect data\nExamples\nLanguage differences\nTypos\nInconsistent formats\nTechniques\nInspection\nFilling correct values\nFix formatting\nExclude from dataset\nSpeaker Notes:\n|Student notes\nDatasets often contain incorrect or anomalous data points that need to be addressed before model training. Common issues include language differences, typos from data entry, values that don't conform to expected formats, and more. To fix incorrect data values, inspect your dataset for anomalies. You might need to fill in the correct values, convert invalid formats to valid formats, or exclude the incorrect entries from the dataset to prevent inaccurate model predictions.\nSlide: 9\nDuplicated data\nExamples\nCopies from multiple sources\nIncorrect data entry\nTechniques\nDeduplication\nSpeaker Notes:\n|Student notes\nDuplicated data refers to records in a dataset that contain the same information. Duplicated data can occur when data is gathered from multiple sources or is entered incorrectly. After your data is fully collected, it's important to inspect the data for duplicate records and then delete or consolidate the duplicate entries. When a dataset contains multiple copies of the same data points, it skews the distribution of the data and over-represents certain examples.\nThe process of automating data duplication removal is called \ndeduplication\n. Deduplication works by scanning datasets for duplicate information, retaining one copy of the information, and replacing the other instances with pointers that direct back to the stored copy. Deduplication can drastically increase storage capacity by keeping only unique information in your data repositories.\nSlide: 10\nActivity: Identifying incorrect and duplicated data\nSpeaker Notes:\n|Student notes\nThe research team is working with the following set of data. Which errors do you notice in this dataset? You might notice that there is missing data, inconsistent formatting, or misplaced information. Consider how to identify and fix incorrect and duplicated data.\nSlide: 11\nActivity: Identifying incorrect and duplicated data\nSpeaker Notes:\n|Student notes\nBaccalaur\u00e9at\n \nis incorrect because it is in a different language than the rest of the dataset. Although the data is placed correctly in the \nEducation\n column, the word \nbaccalaur\u00e9at\n is a French word for a high school diploma. The data would need to be translated to English so the dataset has a standard language to process.\nMasters/PhD\n is incorrect because it is placed in the wrong column. \nMasters/PhD\n is a valid answer for the Education column, but is incorrectly placed in the \nState\n column. This data would need to be moved to the Education column, and you would need to impute the missing data for the State column.\nThe last row is an example of duplicated data. You might recognize that the Participant ID of 123456 is the biggest marker for duplication. There is already a row of data that contains 123456 as the ID.  You need to remove this entire row of data to keep data weights balanced and prevent bias.\nSlide: 12\nDetecting outliers by using mean and median\nMean\nMedian\nMiddle value of dataset\nBetter metric when data is skewed\nConsider both natural and artificial outliers.\nAverage of all values\nUseful when data is symmetrical\nAge\nCount\nMedian\nMean\nSpeaker Notes:\n|Student notes\nThere are a number of different methods you can use to detect and measure the impact of outliers in your data. These methods include calculating mean and median and determining if the outlier is natural or artificial. \nThese measures describe the extent to which your data is clustered around some value or position. \nThe \nmean\n is the average of all the values in the dataset. Mean can be a useful method for understanding your data when the data is symmetrical. For example, a symmetrical set of data that contains ages of respondents might reflect that both the mean and median of the dataset is 50 years old. However, if your data contains outliers, the mean can be skewed one way toward those values. In that case, your mean would not serve as an accurate representation of where your values are truly centered.\nThe \nmedian\n is the value in the dataset that divides the values into two equal halves. To calculate the median, you arrange all the numerical values in ascending order and identify the middle value. If your data is skewed or contains outliers, the median tends to provide the better metric for understanding your data as it relates to central tendency. \nFor example, a dataset that contains income level might contain outliers. The mean might skew toward higher or lower values, while the median would provide a more accurate picture of the data's central tendency.\nOutliers can be caused by either natural or artificial factors. A natural outlier is not the result of an artificial error but is reflective of some truth in the data.\nNatural outliers \nare data points that are accurate representations of data, but are extreme variations of the central data points. For example, in a dataset that includes height measurements of individuals, an extremely tall individual would represent a natural outlier.\nArtificial outliers \nare anomalous data points in your dataset due to error or improper data collection. For example, a faulty sensor in a thermometer might produce a body temperature that is unrealistically high or low compared to expected body temperatures. \nSlide: 13\nAddressing incomplete or missing data\nExploration\nIdentify missing values\nDetermine why values are missing\nMissing at Random (MAR)\nMissing Completely at Random (MCAR)\nMissing Not at Random (MNAR)\nTechniques\nDrop rows with missing values\nImpute values\nSpeaker Notes:\n|Student notes\nUnfortunately, most ML algorithms cannot deal with missing values automatically. You need to use human intelligence to replace missing values with something meaningful and relevant to the problem.\nA few missing values might not be an issue. However, if you\u2019re missing many values in one column, you might find it difficult to interpret the relationship between that feature and the target. In other words, it can be hard to understand how a value in a row corresponds to what needs to be predicted by the model. There are some key steps you can take to address incomplete and missing values in your dataset.\nIdentify missing values \n\u2013There are certain Python libraries, such as Pandas, that you can use to check for missing values. You can also use AWS services to assist you in this process. For example, Amazon SageMaker Autopilot provides a number of filling methods to handle missing values, and Amazon SageMaker Data Wrangler is a full-service data preparation tool. You will learn about SageMaker Data Wrangler in more detail later in this course.\nDetermine why values are missing \n\u2013Before you can determine how to treat the missing values, it\u2019s important to investigate which mechanisms caused the missing values. The following are three common types of missing data:\nMissing at Random (MAR): The probability that a data point is missing depends only on the observed data, not the missing data. Example: In a dataset of student test scores, scores are missing for some students who were absent that day. Absence is related to performance. \nMissing Completely at Random (MCAR): The probability that a data point is missing does not depend on the observed or unobserved data. Example: In an employee survey, some people forgot to answer the question about their number of siblings. Their missing sibling data does not depend on any values.\nMissing Not at Random (MNAR): The probability that a data point is missing depends on the missing data itself. Example: In a financial audit, companies with accounting irregularities are less likely to provide complete records. The missing data depends on the sensitive information being withheld.\nDrop missing values \u2013 \nDepending on what is causing your missing values, you will decide to either drop the missing values or impute data into your dataset. One of the most straightforward ways to deal with missing values is to remove the rows of data with missing values. You can accomplish this by using a Pandas function. Dropping rows or columns will make the dataset non-missing. However, the risk of dropping rows and columns is significant. If you have hundreds of rows or columns of data, all of that missing data might cause bias in your model predictions. If you drop too much data, you might not have enough features to feed the model.\nImpute values \n\u2013 Missing values might be related to new features that haven\u2019t included your dataset yet. After you include more data, those missing values might be highly correlated with the new feature. In this case, you would deal with missing values by adding more new features to the dataset. If you determine the values are missing at random, data imputation, or inputting the data into your dataset, is most likely the best option. There are different ways to impute a missing value. One common way to impute missing values is to replace the value with the mean, median, or most frequent value. You would select the mean, median, or most frequent value for categorical variables. You would select the mean or median for numerical variables. Choosing the mean, median, or most frequent value depending on your business problem and data collection procedures. \nWhen there is a missing value, always consider whether those values are missing at random. If they are not, try to investigate what the mechanism is behind the missing value. You can often find more efficient ways to impute those missing values. Remember that a majority of ML algorithms can\u2019t take missing values. You will most likely need to either drop the values or impute data for the ML model to perform.\nSlide: 14\nQuestion 1\nA real estate company is working on a machine learning project where the goal is to predict housing prices. The team has a dataset that contains information on the number of bedrooms, the square footage, and the locations of different houses. Before building a machine learning model, the team wants to prepare and transform the data. \nWhich data preparation technique could convert values for the home's location (with values of urban, suburban, and rural) into numeric representations that can be fed into a machine learning algorithm? \nResponse\nA\nData cleaning\nB\nFeature engineering\nC\nCategorical encoding\nD\nDimensionality reduction\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember which data preparation technique converts string values into numeric representations. \nSlide: 15\nQuestion 1 answer\nA real estate company is working on a machine learning project where the goal is to predict housing prices. The team has a dataset that contains information on the number of bedrooms, the square footage, and the locations of different houses. Before building a machine learning model, the team wants to prepare and transform the data. \nWhich data preparation technique could convert values for the home's location (with values of urban, suburban, and rural) into numeric representations that can be fed into a machine learning algorithm? \nResponse\nA\nData cleaning\nB\nFeature engineering\nC correct\nCategorical encoding\nD\nDimensionality reduction\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nC: Categorical encoding can be used to convert values for the home's location into numeric representations. For example, you could have 0 for urban, 1 for suburban, and 2 for rural. This allows machine learning algorithms to mathematically interpret the data.\nThe incorrect responses are:\nA: Data cleaning focuses on handling issues like missing data and outliers.\nB: Feature engineering focuses on modifying or creating new features from the data, rather than encoding features.\nD: Dimensionality reduction techniques reduce the number of features but don't convert data types.\nSlide: 16\nQuestion 2\nA non-profit organization is gathering data to decide which type of fundraiser their patrons would like to attend. They decide to provide two options for patrons to share their preferences: an online survey and a hard-copy mailer. Some patrons plan to fill out both the online and hard-copy survey to make sure their preference is recorded.\nWhich data cleaning technique will BEST help the company retain unique data points for the most effective machine learning (ML) solution?\nResponse\nA\nReformatting the hard-copy mailer to contain only \"Yes/No\" answers\nB\nInspecting the data for incorrectly entered dates\nC\nIdentifying duplicate data from those who submitted both an online and hard-copy survey\nD\nChecking the datasets for mistyped patron names\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember that duplicate data must be removed from a training dataset to avoid model bias.\nSlide: 17\nQuestion 2 answer\nA non-profit organization is gathering data to decide which type of fundraiser their patrons would like to attend. They decide to provide two options for patrons to share their preferences: an online survey and a hard-copy mailer. Some patrons plan to fill out both the online and hard-copy survey to make sure their preference is recorded.\nWhich data cleaning technique will BEST help the company retain unique data points for the most effective machine learning (ML) solution?\nResponse\nA\nReformatting the hard-copy mailer to contain only \"Yes/No\" answers\nB\nInspecting the data for incorrectly entered dates\nC correct\nIdentifying duplicate data from those who submitted both an online and hard-copy survey\nD\nChecking the datasets for mistyped patron names\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nC: Because the company has data points from multiple formats, there is a likelihood of duplication in their dataset. It's important to identify duplication to prevent the ML solution from overrepresenting certain examples.\nThe incorrect responses are:\nA,B,D: These options would not address the probable duplication in the dataset. Because the company has data points from multiple formats, there is a likelihood of duplication in their dataset. It's important to identify duplication to prevent the ML solution from overrepresenting certain examples.\nSlide: 18\nQuestion 3\nYou are cleaning up a dataset that includes the body weights of patients at a medical clinic. A clinician tells you that they usually see adults, but children occasionally visit the clinic.\nWhich data measurement BEST provides the central tendencies of your data?\nResponse\nA\nYou would choose the mean, because the best measurement of data that might contain outliers is taking the average of all data entries.\nB\nYou would choose the median, because the best measurement of data that might contain outliers is sorting the data and dividing the dataset at the middle variable.\nC\nBecause the data is numerical, you can use any method of measurement to find the average weight of a patient.\nD\nThere is no measurement you can use to find central tendencies of data.\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember the best measurement to find the central tendencies when the data is skewed.\nSlide: 19\nQuestion 3 answer\nYou are cleaning up a dataset that includes the body weights of patients at a medical clinic. A clinician tells you that they usually see adults, but children occasionally visit the clinic.\nWhich data measurement BEST provides the central tendencies of your data?\nResponse\nA\nYou would choose the mean, because the best measurement of data that might contain outliers is taking the average of all data entries.\nB correct\nYou would choose the median, because the best measurement of data that might contain outliers is sorting the data and dividing the dataset at the middle variable.\nC\nBecause the data is numerical, you can use any method of measurement to find the average weight of a patient.\nD\nThere is no measurement you can use to find central tendencies of data.\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nB: Because the clinic mostly sees adults, a few entries of children's weights might skew the average patient weight. In this case, it is best to use median to better understand the central tendencies of your data.\nThe incorrect responses are:\nA,C: Because the clinic mostly sees adults, a few entries of children's weights might skew the average patient weight. The mean of all patient data would skew towards the outlier values.\nD: You can use measurements such as mean and median to find central tendencies of your data.\nSlide: 20\nQuestion 4\nAn apparel company is building a machine learning solution to determine which types of products are not selling well. They've collected data on how long products sit in inventory, but their dataset contains many missing values.\nWhich method is the most effective option for addressing missing values in the apparel company's dataset?\nResponse\nA\nInspecting the reason for the missed values, and dropping, leaving, or imputing data accordingly\nB\nDropping all the data that contains missing values\nC\nManually imputing every missing value\nD\nLetting their ML algorithm replace the missing values\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember the steps that should be followed when there is missing data. \nSlide: 21\nQuestion 4 answer\nAn apparel company is building a machine learning solution to determine which types of products are not selling well. They've collected data on how long products sit in inventory, but their dataset contains many missing values.\nWhich method is the most effective option for addressing missing values in the apparel company's dataset?\nResponse\nA correct\nInspecting the reason for the missed values, and dropping, leaving, or imputing data accordingly\nB\nDropping all the data that contains missing values\nC\nManually imputing every missing value\nD\nLetting their ML algorithm replace the missing values\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nA: It's important to understand why the data is missing. Then, the company can set mechanisms in place to drop rows or columns of data, leave empty values, or impute data as needed.\nThe incorrect responses are:\nB: It's important to understand why the data is missing. Then, the company can set mechanisms in place to drop rows or columns of data, leave empty values, or impute data as needed. Dropping all missing values could remove significant and relevant data.\nC: This method is time-intensive. It's important to understand why the data is missing. Then, the company can set mechanisms in place to drop rows or columns of data, leave empty values, or impute data as needed.\nD: ML algorithms have trouble with missing values. While you can use AWS services to help locate and replace missing values, ML models themselves are not the best option for replacing missing values.\nSlide: 22\nFeature engineering concepts\nTopic A: Handling incorrect, duplicated, and missing data\nTopic B: Feature engineering concepts\nTopic C: Numeric and text feature engineering\nTopic D: Feature selection techniques\nTopic E: AWS data transformation services\nSpeaker Notes:\n|Student notes\nDuring the data transformation process, it is likely that you will come across different data formats. Remember, machine learning algorithms prefer numbers, so data that is text-based needs to be converted to numbers if possible. Categorical encoding is the process of manipulating text-based variables into number-based variables.\nAfter your data has been cleaned up and you\u2019ve encoded your data as necessary for your model, you can fine-tune or create new features in your dataset through feature engineering. Feature engineering is a method for transforming raw data into more informative features that help models better capture underlying relationships in the data.\nSlide: 23\nFeature engineering requirements\nUnderstanding the raw data\nRecognizing the business problem\nSelecting the right ML algorithm\nSpeaker Notes:\n|Student notes\nFeature engineering is more of an art than a science. Often, feature engineering is not a set of rules, and your approach will be based on the specific business problem that you are trying to solve.\nFeature engineering is often a highly intuitive practice. Understanding the raw data, familiarity with your business problem, and intuition of how to bridge the data and the algorithm work in parallel to determine your feature engineering requirements. Humans are skilled at finding relationships between the problem and the input feature of the raw data, so lean into your intuition when employing feature engineering.\nSlide: 24\nFeature engineering use cases \nNumeric feature engineering\nWide range or high variance\nOutliers or skewed distribution\nTime-based numeric features\nText feature engineering\nCategorical text\nLarge chunks of text\nComplex relationships\nSpeaker Notes:\n~Dev notes\n~Alt text from digital: Graph with mostly concentrated plot points and six outliers.\n|Student notes\nDepending on your data\u2019s format, you will select different methods of feature engineering to transform your dataset. \nNumeric feature engineering \ninvolves transforming numeric values for the model and is often accomplished by grouping different numeric values together. Sometimes, ML algorithms have trouble handling large numbers or isolated numbers. Grouping numeric values helps the model to better understand the relationships of these values.\nNumeric feature engineering strategies such as feature scaling, binning, and log transformation can help the ML model better understand the relationships of the data\u2019s numeric values. When deciding whether your dataset could benefit from numeric feature engineering, look for these qualities:\nNumeric features that have a wide range of values or high variance.\nNumeric features with outliers or a skewed distribution are also good targets for techniques like log transforms or binning to reduce the influence of extreme values.\nNumeric features related to time, such as timestamps, are good candidates for extraction of more meaningful metrics like day of the week or hour of the day.\nText feature engineering \ninvolves transforming text for the model and is often accomplished by splitting the text into smaller pieces. When dealing with text data, or corpus data, you can use feature engineering to transform attributes within your data to make those attributes more useful for your ML model. The purpose of text feature engineering is to break your text down into more bite-sized pieces for your model to understand the weights and relationships of your raw data.\nText feature engineering strategies, such as bag-of-words and N-gram, can tokenize the value of text data for your ML model to infer the right balance of information from that data. Because text feature engineering improves the ability for your model to understand your data effectively, essentially every type of raw data that includes text can benefit from some type of text feature engineering.\nCategorical features are associated with a group, and can often be represented numerically.\nSlide: 25\nCategorical encoding\nBinary\nTwo options \nNominal\nMultiple categories\nOrdinal\nCategories where order of values matter\nSome ML algorithms do not require encoding.\nSpeaker Notes:\n|Student notes\nWhen it comes to your data, the term numerical refers to values expressed as a measurable number, and categorical refers to features that are associated with a group. For example, in a dataset that contains animal medical records, a cat\u2019s weight is considered numerical, and the breed of cat is considered categorical.\nThe following list provides different types of categorical values.\nBinary\n categorical values refer to values that are one of two options. For example, a column of data might be true or false, such as if someone attended an event.\nNominal\n, or multi-categorical, values are category values where order does not matter. A set of data that contains different geographic locations, such as states or cities, might be considered multi-categorical.\nOrdinal\n, or ordered, values are category values where the order does matter, like what size of drink you order at a coffee shop: small, medium, or large.\nNot all categorical variables need to be encoded. Depending on your use case, different ML algorithms might not require you to encode your variables. \nFor instance, a random forest model can handle categorical features directly. You would not need to encode values, such as teal, green, and blue, as numeric values.\nSlide: 26\nActivity: Assessing data for categorical encoding\nSpeaker Notes:\n|Student notes\nThe data gathered by the research scientist team includes various types of numerical and categorical data. \nSlide: 27\nActivity: Assessing data for categorical encoding\nSpeaker Notes:\n|Student notes\nThe \nage\n column contains numerical data. Because this data is already set as numeric values, there is no need for categorical encoding.\nThe \neducation\n column is categorical data. Depending on which type of ML algorithm you decide to use, you might need to encode this data before using it to train and inform your model.\nThe \nflu shot \ncolumn is binary categorical data because respondents answered Yes or No. Depending on the type of algorithm you decide to use, this data might need to be encoded for your model to read the data more easily. For example, you can encode Yes to 0 and No to 1.\nThe \noutbreak zone \ncolumn is also binary categorical data. In this case, the research scientists could have also offered more value options for this feature that would be categorized as ordered, or ordinal, data. For instance, the scientists could categorize outbreak by light, medium, or heavy outbreak, rather than a binary Yes or No outbreak. Either way, this column might need to be encoded for model training, depending on the algorithm you decide to use. \nSlide: 28\nQuestion 1\nAn automobile company wants to use a machine learning solution to determine how quickly their various sizes of vehicles reach their highest speeds. The vehicle sizes are compact, mid-size, and full-size. \nWhich type of categorical data BEST describes the feature of their dataset that lists the size of the vehicle?\nResponse\nA\nNumerical\nB\nNominal\nC\nOrdinal\nD\nBinary\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember the type of categorical values that are ranked in order. \nSlide: 29\nQuestion 1 answer\nAn automobile company wants to use a machine learning solution to determine how quickly their various sizes of vehicles reach their highest speeds. The vehicle sizes are compact, mid-size, and full-size. \nWhich type of categorical data BEST describes the feature of their dataset that lists the size of the vehicle?\nResponse\nA\nNumerical\nB\nNominal\nC\nOrdinal\nD\nBinary\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nC: Ordinal data is the correct type of categorical data here, because the vehicle sizes like compact, midsize, and full-size are categorical labels with a numeric order. Ordinal data describes data that is categorical, but ranked in a numeric order.\nThe incorrect responses are:\nA: Numerical data represents quantitative values. The vehicle sizes (compact, mid-size, full-size) are qualitative categories, not numerical values.\nB: This question is asking about the categorical data that would describe the vehicle sizes. The vehicle sizes, such as compact, midsize, and full-size, would be considered ordinal data because they are categorical values that have a specific size order.\nD: Binary data represents categories with only two possible values. The vehicle sizes have three possible categories: compact, mid-size, full-size.\nSlide: 30\nQuestion 2\nAn energy company wants to assess how often their customers have to replace their solar panels after their initial solar panel purchase. Their raw data includes the dates of first purchase, customer reviews, and dates of additional purchases.\nHow might this company use feature engineering to help the machine learning model understand the relationship between these features in their dataset?\nResponse\nA\nThis is not a good use case for feature engineering, because the data includes a text-based feature.\nB\nThe company can use numeric feature engineering to extract information from the purchase dates, and they can use text feature engineering to extract information from the customer reviews.\nC\nThe company can only use numeric feature engineering to extract information from this dataset.\nD\nThe company can only use text feature engineering to extract information from this dataset.\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember that machine learning models find patterns in historical data and use that to make predictions.\nSlide: 31\nQuestion 2 answer\nAn energy company wants to assess how often their customers have to replace their solar panels after their initial solar panel purchase. Their raw data includes the dates of first purchase, customer reviews, and dates of additional purchases.\nHow might this company use feature engineering to help the machine learning model understand the relationship between these features in their dataset?\nResponse\nA\nThis is not a good use case for feature engineering, because the data includes a text-based feature.\nB correct\nThe company can use numeric feature engineering to extract information from the purchase dates, and they can use text feature engineering to extract information from the customer reviews.\nC\nThe company can only use numeric feature engineering to extract information from this dataset.\nD\nThe company can only use text feature engineering to extract information from this dataset.\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nB: Extracting information from these different features might help to show the relationship between initial purchase, review sentiment, and any additional purchases.\nThe incorrect responses are:\nA: Both the numeric and text-based data in this dataset are good candidates for feature engineering.\nC: The company can use text feature engineering to extract meaningful information from customer reviews..\nD: The company can use numeric feature engineering to extract meaningful information from initial and additional purchase dates.\nSlide: 32\nNumeric and text feature engineering\nTopic A: Handling incorrect, duplicated, and missing data\nTopic B: Feature engineering concepts\nTopic C: Numeric and text feature engineering\nTopic D: Feature selection techniques\nTopic E: AWS data transformation services\nSpeaker Notes:\n|Student notes\nIn the last section, you learned about the difference between numeric and categorical values. Numeric feature engineering involves creating a new feature by combining two or more numeric values. \nText data is an important component in many business problems, and not all text-based features can be easily encoded with categorical encoding. Text feature engineering involves transforming the text in your data so that your ML model can better understand the relationships within that data.\nSlide: 33\nNumeric feature engineering methods\nFeature scaling\nBinning\nLog transformation\nSpeaker Notes:\n|Student notes\nTypes of numeric feature engineering methods include feature scaling, binning, and log transformation.\nFeature scaling, also known as data scaling or normalization, aims to transform the numeric values so that all values are on the same scale. This method helps you to take large numbers and scale them down, so that the ML algorithm can achieve quicker computations and avoid skewed results.\nBinning is a technique to group numerical features into a smaller number of bins, or categories. The data is divided into these bins based on value ranges, thus transforming a numeric feature into a categorical one.\nLog transformation applies a logarithmic function to numeric features in a dataset. A logarithmic function is a mathematical function where the independent variable is the exponent of a fixed number, called the base.\nSlide: 34\nFeature scaling approaches\nNormalization\nRescale values to a common scale (\neg.\n 0 to 1)\nImpacted by outliers\nStandardization\nRescale values to a mean of 0 and a standard deviation of 1\nReduces negative effect of outliers\nSpeaker Notes:\n~Dev notes\n~Alt text: \nGraph with points ranging between 0 and 1\n~Alt text: \nGraph with points centered around 0\n|Student notes\nA lot of ML algorithms are sensitive to wide ranges of data, so feature scaling is required for many ML models. However, some ML models like decision trees are not skewed by wide ranges of numeric data. Whether or not to scale your data will depend on the ML algorithm that you decide to use.\nNormalization\n rescales the values of numeric features to a common scale, often between 0 and 1. This rescaling makes the values more comparable and prevents features with greater magnitudes from incorrectly influencing the model. Outliers can prevent a normalization approach from working effectively.\nStandardization\n is similar to normalization, but instead of scaling values from 0\u20131, it rescales the features to have a mean of 0 and standard deviation of 1. \nStandardization reduces the negative effect of outliers.\nSlide: 35\nFeature scaling example\nSpeaker Notes:\n|Student notes\nAs an example, a dataset considering housing data might contain features such as the number of bedrooms and the price of the house. Number of bedrooms might range from 1 to 5 as a numeric value, but the price might range from $100,000 to $1,000,000.\nNormalizing the house pricing feature with values from 100,000 to 1,000,000 to a 0\u20131 scale would transform the values to be between 0 and 1. This method assigns the smallest value to 0 and the highest value to 1, and uses a calculation to scale down the remaining values.\nStandardization would set the average home price at the integer of 0 and use a calculation to score the remaining values. The calculation smooths out the values, so outliers are not as much of a negative factor with standardization as they are with normalization.\nSlide: 36\nBinning\nFeature is transformed from numeric to categorical\nHouse prices\nBinned house prices\nSpeaker Notes:\n~Dev notes\n~Alt text from digital: \nTwo bar charts: one with bars for features; one with these features grouped into three bars, or bins.\n|Student notes\nBinning can make models more robust by reducing the effect of outliers and allowing nonlinear relationships between features and target values to be captured more easily. It is an effective strategy for numerical data when the exact difference in numbers is not important, but the general range is a factor.\nFor example, you might use binning to engineer the house pricing feature. You might not be concerned about distinguishing between data points of a house price of $150,000 and $151,000, but you might be interested in distinguishing between houses priced at $150,000 and $500,000. \nYou can use a method called quantile binning to evenly distribution values between bins. For example, if you had three bins for your housing data, the lowest 33.3% of data would distribute to one bin, the next 33.3% in the next bin, and highest 33.3% in the remaining bin. Quantile binning results in each bin containing approximately the same number of data points.\nSlide: 37\nLog transformation\nApply a logarithmic function to numeric features\nSpeaker Notes:\n~Dev notes\n~Alt text from digital: \nTwo bar graphs: The first shows bars with many outliers; the second uses log transformation to bring values closer.\n|Student notes\nThe most common logarithmic functions have bases of 10 or e, where e is approximately equal to 2.71828. Logarithmic functions are the inverse of exponential functions and are useful for modeling phenomena where growth decreases over time, like population growth or decay.\nLog transformation is used to normalize skewed numeric data, reduce the effect of outliers, and keep values closer to normal distribution. Log transforming can improve model performance when using numeric features that have highly skewed distributions or outliers.\nLet\u2019s say your housing dataset had extreme outliers due to containing multiple geographic locations. One geographic location has a rural lot listed for $10,000 and another location has a luxury home in the city listed for $10,000,000. Taking the log of the housing prices scales down the large outlier values.\nFor example, the log of $10,000 would be around 4 and the log of $10,000,000 would be around 7. Using this method, the outliers are brought much closer to the normal values in the remainder of the dataset.\nSlide: 38\nText feature engineering methods\nBag of words\nN-gram\nTemporal data\nSpeaker Notes:\n|Student notes\nYou can use feature engineering methods such as bag-of-words and N-gram to engineer your text-based features. Temporal or time series data is data involving time, like a series of dates. Temporal data can come in many formats, and is often a mix of numeric and text data.\nSlide: 39\nBag-of-words\n\u201cdistrict\u201d \u201chas\u201d \u201cgreat\u201d \u201cteachers\u201d \n\u201cand\u201d \u201cgreat\u201d \u201cfacilities\u201d \nCount number of times words appear\nSpeaker Notes:\n|Student notes\nThe bag-of-words model does not keep track of the sequence of words, but counts the number of words in each observation. Bag-of-words uses tokenization to create a statistical representation of the text.\nConsider an example with the housing dataset. Perhaps you want to determine whether the listed home is in a highly rated school district, and you want to include parents\u2019 school reviews as a part of your dataset. The bag-of-words model would take a portion of raw text, break down each piece of text, and tokenize the text to create a table or statistical representation of that review.\nSlide: 40\nN-gram\n\u201cdistrict has\u201d \u201chas great\u201d \u201cgreat teachers\u201d \n\u201cteachers and\u201d \u201cand great\u201d \u201cgreat facilities\u201d \nProduce groups of words of n size\nSpeaker Notes:\n|Student notes\nThe N-gram model builds off of bag-of-words by producing a group of words of n size.\nFor example, if n = 2, a sliding window creates each group of two words as a token pair and creates a statistical representation of these token pairs. This method is useful when we want to find a particular combination of words in the text.\nIn the housing data, you could use the n-gram model to represent how many times reviews include word combinations of \u201cgreat teachers\u201d or \u201cafter-school programs.\"\nSlide: 41\nTemporal data\nExtract features from dates and times\nSpeaker Notes:\n|Student notes\nFeature engineering is important for temporal data because it can help capture key trends, cycles, or patterns over time. Examples of temporal features include time of day, day of the week, and seasons. By carefully extracting and including them in your model, you can help the model to better understand and predict time-based relationships in the data. \nFor example, you might have a feature in your housing dataset that contains the dates of when houses were listed for sale. When you have your dates in a consistent format, you might use feature engineering to create different features that extract the temporal information into day, month, and year. Extracting this information might reveal that most homes are listed in the summer or that most homes are listed at the first of each month.\nSlide: 42\nQuestion 1\nA company is building a machine learning model to predict credit risk. One of the features in the dataset is \nLoanAmount\n, which is likely to contain a few outliers.\nWhich feature engineering technique is a good method for engineering this numeric feature before model training?\nResponse\nA\nTake the log transform of the values to help normalize distribution.\nB\nLeave the values unchanged.\nC\nTake the mean of all values.\nD\nUse one-hot encoding to sort the values.\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember that a log transform can adjust data with outliers to a more normal distribution.\nSlide: 43\nQuestion 1 answer\nA company is building a machine learning model to predict credit risk. One of the features in the dataset is \nLoanAmount\n, which is likely to contain a few outliers.\nWhich feature engineering technique is a good method for engineering this numeric feature before model training?\nResponse\nA correct\nTake the log transform of the values to help normalize distribution.\nB\nLeave the values unchanged.\nC\nTake the mean of all values.\nD\nUse one-hot encoding to sort the values.\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nA: Taking the log transform of numeric features like \nLoanAmount\n can help normalize the distribution and handle skewness. This makes the values more comparable for modeling.\nThe incorrect responses are:\nB: Leaving the values unchanged would not help to normalize the distribution. Taking the log transform of numeric features like \nLoanAmount\n can help normalize the distribution and handle skewness.\nC: Taking the mean of all values can be a helpful method for measuring data, but in this case, the outliers might skew that calculation. Taking the log transform of numeric features like \nLoanAmount\n can help normalize the distribution and handle skewness.\nD: Although one-hot encoding is a great tool for categorical values, this value is numeric. Taking the log transform of numeric features like \nLoanAmount\n can help normalize the distribution and handle skewness.\nSlide: 44\nQuestion 2\nYou work for a travel company and want to build a model to categorize customer reviews as positive or negative.\nWhich text feature engineering technique would be most useful for this classification task?\nResponse\nA\nBag-of-words\nB\nBinning\nC\nLog transformation\nD\nN-gram\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember that n-grams are a useful feature engineering technique for sentiment analysis.\nSlide: 45\nQuestion 2 answer\nYou work for a travel company and want to build a model to categorize customer reviews as positive or negative.\nWhich text feature engineering technique would be most useful for this classification task?\nResponse\nA\nBag-of-words\nB\nBinning\nC\nLog transformation\nD correct\nN-gram\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nD: N-grams are a good technique for text classification tasks like sentiment analysis, because they help capture important phrases and sequences of words, not just individual words. Bag-of-words loses this important context. For a travel review, sequences like \"not clean\" and \"very comfortable\" are important signals. N-grams can help the model understand these types of phrases.\nThe incorrect responses are:\nA: Although bag-of-words represents the occurrence of words in a document, it does not preserve the order and context of the words. Reviews with very different sentiments could have similar bag-of-words representations if they contain the same vocabulary.\nB: Binning is useful for converting continuous numerical variables into categorical bins, not for text feature engineering.\nC: Log transformations are useful for normalizing skewed numerical distributions, not text data.\nSlide: 46\nFeature selection techniques\nTopic A: Handling incorrect, duplicated, and missing data\nTopic B: Feature engineering concepts\nTopic C: Numeric and text feature engineering\nTopic D: Feature selection techniques\nTopic E: AWS data transformation services\nSpeaker Notes:\n|Student Notes\nYou\u2019ve learned about feature scaling and feature engineering to make the most of your dataset and help your model determine relationships in the data. You can also use the technique of feature selection to select the most useful features for your model to make efficient and accurate predictions. \nSlide: 47\nFeature selection\nReduces dimensions\nRemove or select features\nRemove\nSpeaker Notes:\n|Student notes\nFeature selection is the intuitive process of selecting the most relevant features to use in a machine learning model.\nEarlier, you learned about feature scaling. Feature scaling refers to methods that rescale the values of features to a common scale without removing any. Feature selection, on the other hand, reduces dimensions by removing features or selecting certain features so that your model can focus on the correct information. \nIn this example, the housing dataset includes features of the price, the location, the date listed, and the number of bedrooms. You deal primarily with houses in the same geographic location, and you are more interested in the relationship between the number of bedrooms and the price of the house. In that case, you might decide to remove the \u201clocation\u201d and \u201cdate listed\u201d features, so that your model can focus on the most relevant information to the problem.\nSlide: 48\nFeature selection by splitting and combining\nFeature splitting\nFeature combining\nTotal number of bedrooms\nTotal square footage\nSpeaker Notes:\n~Dev notes\n~Alt text:  graphic showing splitting number of bedrooms into small, medium and large features.\n~Alt text:  graphic showing combining individual room sizes into a total square footage feature.\n|Student notes\nIn addition to selecting certain relevant features, you can also use feature splitting and combining to provide the most relevant information to your ML model.\nFeature splitting breaks down features into multiple derived features, expanding the feature space. Feature combining aggregates multiple features into a new combined feature, reducing the feature space. Both techniques can help improve model performance by transforming the original feature set into a more optimal set of predictive features.\nFeature splitting \nFor example, in the housing dataset, you might decide to capture more specific information about bedrooms in each home. The number of bedrooms feature might be split into the number of small, medium, and large bedrooms.\nFeature combining\nOr you might have each room\u2019s square footage categorized in a separate feature. You could combine the room size features to create one complete feature that represents the home\u2019s square footage.\nSlide: 49\nPrincipal component analysis (PCA)\nReduce number of features\nTransform features into principal components \nRetain most of the original information\nRanks components by their variance\nSpeaker Notes:\n|Student notes\nPrincipal component analysis (PCA) is a statistical technique that you can use for dimensionality reduction. In other words, PCA is a method for reducing the number of features in a dataset.\nThe goal of PCA is to retain most of the variation in the original features but reduce the overall number of features to increase computation efficiency. PCA works by transforming the features into a new set of uncorrelated features called principal components. PCA ranks the new set of principal components by their variance, with the first components capturing most of the variance in the data.\nSlide: 50\nActivity: Understanding principal components\nSpeaker Notes:\n|Student notes\nSlide: 51\nActivity: Understanding principal components\nSpeaker Notes:\n|Student notes\nIn the example table, you could reduce five of the features to just two principal components and compare them. The first principal component focuses on house size, and the second component focuses on cost. \nPrincipal component 1: Size\nThe first component accounts for the physical attributes of the home that includes square footage, bedrooms, and bathrooms.\nPrincipal component 2: Cost\nThe second component captures the financial factors that include price and tax rate.\nSlide: 52\nQuestion 1\nA baseball analyst wants to predict a player's batting average using a machine learning model. They have a dataset with over thirty different hitting statistics for each player. \nThe analyst wants to retain necessary information but reduce the number of features before training the model. Which feature selection technique should they use?\nResponse\nA\nFeature splitting\nB\nFeature scaling\nC\nPrincipal component analysis (PCA)\nD\nNo need for feature selection \nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember that PCA reduces features while retaining information. . \nSlide: 53\nQuestion 1 answer\nA baseball analyst wants to predict a player's batting average using a machine learning model. They have a dataset with over thirty different hitting statistics for each player. \nThe analyst wants to retain necessary information but reduce the number of features before training the model. Which feature selection technique should they use?\nResponse\nA\nFeature splitting\nB\nFeature scaling\nC correct\nPrincipal component analysis (PCA)\nD\nNo need for feature selection \nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nC: PCA reduces the dimensionality of the data while retaining most of the variation. This makes it useful for reducing a large set of variables down to a smaller subset of features before modeling.\nThe incorrect responses are:\nA: Feature splitting divides features into multiple datasets, which does not reduce dimensionality.\nB: Feature scaling standardizes features but does not reduce dimensionality.\nD: This example is a good candidate for using PCA. PCA reduces the dimensionality of the data while retaining most of the variation. This makes it useful for reducing a large set of variables down to a smaller subset of features before modeling.\nSlide: 54\nAWS data transformation services\nTopic A: Handling incorrect, duplicated, and missing data\nTopic B: Feature engineering concepts\nTopic C: Numeric and text feature engineering\nTopic D: Feature selection techniques\nTopic E: AWS data transformation services\nSpeaker Notes:\n|Student notes\nData ingestion is the process of collecting data from diverse sources, such as databases, data lakes, and streaming sources. The goal of data ingestion is to consolidate your data to use for your machine learning solution. \nIn this section, you will learn how you can use AWS services, such as Amazon SageMaker Data Wrangler, to ingest your data. You will also learn how you can use Amazon SageMaker Feature Store to ingest data and to create and manage features. \nYou will learn about data transformation use cases of these AWS services. You also learn fundamental steps of how you can use these services to automate part or all of the data transformation process.\nSlide: 55\nIngesting and transforming data for ML on AWS\nAmazon SageMaker Data Wrangler\nAmazon SageMaker Feature Store\nAmazon EMR\nAdd EMR notes from 1.2.13\nDo not add content but leave note here for T&C to revisit this slide since digital content is streaming-specific.\nSpeaker Notes:\n|Student notes\nYou can use AWS services, such as Amazon SageMaker Data Wrangler, to ingest your data. You can also use Amazon SageMaker Feature Store to ingest data and to create and manage features.\nAWS services like SageMaker Data Wrangler and SageMaker Feature Store streamline complicated data ingestion pipelines. Data Wrangler provides a visual, no-code way to transform and clean data from diverse sources. SageMaker Feature Store manages and shares curated features for machine learning model development. Using these services together eliminates much of the manual coding needed for ingesting and wrangling data. \nAWS services like Amazon EMR can help you to transform, enrich, and load data at scale. \nSlide: 56\nBenefits of Amazon SageMaker Data Wrangler\nAmazon SageMaker Data Wrangler\nVisual interface\nDrag-and-drop environment\nBuilt-in algorithms\nIntegration with data sources\nReusable data flows\nSpeaker Notes:\n|Student notes\nData Wrangler streamlines data ingestion through a visual interface to profile, understand, clean, and transform data using built-in recipes. It connects to sources, transforms, cleans, and loads data. Then, it outputs structured, model-ready data to AWS storage without writing code. SageMaker Data Wrangler streamlines complex data preparation so you can focus on building and training ML models, rather than preparing data. \nData Wrangler is a visual, code-free tool for data preprocessing and feature engineering that integrates with AWS data sources. It streamlines machine learning workflows through reusable data flows, large dataset handling, and integration with SageMaker.\nSlide: 57\nUse cases for SageMaker Data Wrangler\nClean data\nFeature engineering\nFix formatting issues\nReduce data size\nAutomate transformations\nSpeaker Notes:\n|Student notes\nData Wrangler streamlines exploratory data analysis and data preprocessing stages for ML learning in a visual, scalable, and collaborative environment. The following are some of the common use cases:\nClean data\n: You can use SageMaker Data Wrangler to interactively explore, visualize, clean, validate, and transform data to prepare it for ML model training. You can use the built-in transforms and visual interface to clean, standardize, and preprocess raw data from various sources like Amazon S3, databases, and more. \nFeature engineering\n: You can use the built-in transform and Python code blocks to engineer new features from your data. For example, you can use SageMaker Data Wrangler to combine columns and apply formulas.\nFixing formatting issues\n: SageMaker Data Wrangler can handle issues like missing headers and encoding problems to get the data into a consistent format for downstream use.\nReducing data size: \nFor large datasets, you can use SageMaker Data Wrangler to select subsets of columns, filter rows, sample data, and convert to lower precision data types. This helps to reduce storage and memory needs.\nAutomating transformations\n: After you interactively build transformation steps in SageMaker Data Wrangler, you can save them as a data recipe or flow to automate preparation for future datasets.\nSlide: 58\nUsing Amazon SageMaker Data Wrangler for data ingestion and transformation\nInput\nOutput\nAmazon \nSageMaker\n Data Wrangler\nIdentify data sources\nCreate a \nnew data flow\nPreview transformations\nOutput results\nSelect built-in transforms to transform data\nSpeaker Notes:\n|Student notes\nIdentify data sources\nUpload your dataset to an S3 bucket. SageMaker Data Wrangler supports CSV, JSON, Parquet, and other common data formats.\nCreate a new data flow\nOpen the SageMaker console and navigate to Data Wrangler. Choose Create flow to start a new data flow. Select the dataset you uploaded to Amazon S3 as the input source. SageMaker Data Wrangler will infer the schema and show a sample of the data. \nSelect built-in transforms to transform data\nIn the Transform section, use the built-in transforms to transform your data. You can see a sample output as you build the transforms. Some common transformations include the following: \nChanging column data types \nFiltering or sorting rows \nFilling in missing values \nParsing columns, like dates \nJoining datasets \nAggregating data, like sums or averages\nPreview transformations\nAutomate conversations, question answering, and workflows based on research into training honest and responsible AI systems.\nOutput results\nGenerate unique, realistic, high-quality images, art, logos, and designs.\nSlide: 59\nBenefits of Amazon SageMaker Feature Store\nAmazon SageMaker Feature Store\nStreamline\nTrack\nConfigure\nRetain\nSpeaker Notes:\n|Student notes\nFeature Store is a managed repository for storing, sharing, and managing features used for ML models. Features can be reused by multiple teams and SageMaker Feature Store keeps them synchronized for both batch training and real-time inference. By centralizing features in a secured store, SageMaker Feature Store improves feature quality and model accuracy at scale across the ML lifecycle. Storing features saves time by eliminating redundant feature engineering efforts.\nFeature Store provides centralized management of feature engineering to streamline machine learning workflows. It provides configurable online and offline storage of feature groups with metadata to enable discovery, reuse, and automation from data ingestion through model training and prediction. \nFeature Store provides a centralized and automated way to engineer, monitor, govern, and reuse features to improve model training and performance across teams. The following are some of the common use cases:\nAutomated data preprocessing: \nSageMaker Feature Store can automatically preprocess raw data as it is ingested. This includes scaling, encoding, transforming, and cleaning data so it is ready for model training.\nCentralized feature repository: \nSageMaker Feature Store acts as a centralized repository for curated features that have been preprocessed and engineered. Features can be reused across teams and models.\nStandardized features\n: SageMaker Feature Store establishes a standardized source of truth for featured data that is consistent across the organization. This avoids issues with different teams using their own feature engineering code.\nManagement of feature pipelines\n: SageMaker Feature Store manages the end-to-end lifecycle of feature pipelines from data ingestion through preprocessing. This provides monitoring and governance of feature engineering.\nCaching for performance\n: Frequently used features are cached in SageMaker Feature Store for faster retrieval during training. This improves performance by avoiding repetitive preprocessing.\nSlide: 60\nBenefits of Amazon EMR\nAmazon EMR\n<list key benefits>\nAdd EMR notes from 1.2.13\nDo not add content but leave note here for T&C to revisit this slide since digital content is streaming-specific.\nSpeaker Notes:\n|Student notes\nSlide: 61\nData formatting for model training in Amazon SageMaker\nSplit data sets.\nTraining\nTesting\nValidation\nAugment or shuffle training data for increased diversity.\nFormat data set.\nCSV\nRecordIO-protobuf\nSpeaker Notes:\n|Student notes\nReducing bias in your dataset is one of the primary objectives in data preparation. With balanced data, the model can focus on meaningful patterns in the dataset rather than any inherent noise.\nA common strategy to achieve a balanced and accurate dataset is to divide the data into distinct subsets for training, testing, and validation. \nData augmentation works by creating new, realistic training examples that expand the model's understanding of the data distribution. Dataset augmentation involves artificially expanding the size and diversity of a dataset by applying a variety of transformations and modifications to existing data samples. These transformations introduce variations in the data while preserving the essential characteristics relevant to the learning task\nData shuffling involves rearranging the order of the data points in a dataset before they are used for training a machine learning model. Shuffling your data helps to verify that the training examples are presented to the model in a random order rather than in a fixed sequence.\nWhen using Amazon SageMaker for your ML workloads, you want to make sure you are providing data that adheres to an accepted format. The two most common formats supported by the SageMaker built-in algorithms are CSV formatting and \nRecordIO-protobuf\n formatting. \nFormatting can be a complex task. You focus more on adhering to certain formatting types during the model training and refining stage of the machine learning lifecycle. However, it is helpful to know the basic concepts and structure of these two types of formats.\nCSV is a widely used text-based data format for training machine learning models. CSV files store tabular data, where each row represents a data sample, and the columns represent the features of that sample. For SageMaker built-in algorithms, the label in the training dataset must be the first column on the left, and the features should be to the right. Many built-in algorithms in SageMaker, such as \nXGBoost\n, linear learner, and \nDeepAR\n, accept CSV formatted data as input for model training. \nRecordIO-protobuf\n is a binary data format used by SageMaker for efficient data storage and transfer. This format is particularly useful when you need to handle large datasets or perform automatic transformations on the data. The \nRecordIO-protobuf\n format consists of a series of records, where each record contains a binary-encoded \nprotobuf\n message. This format is commonly used for image data, where each record represents an image and its associated metadata.\nSlide: 62\nQuestion 1\nYou work for a hospitality company that operates hotels across the country. You want to build a machine learning model to predict customer loyalty using data about past customer stays and purchases. You are considering using SageMaker Feature Store to prepare the data for model training. \nHow could SageMaker Feature Store be an effective service option for this use case?\nResponse\nA\nSageMaker Feature Store is a marketplace where you can pay workers to complete discrete tasks like labeling data. This would verify that your customers\u2019 purchases are accurate.\nB\nYou can use SageMaker Feature Store to visualize and clean your data. Clean data is essential for machine learning accuracy.\nC\nSageMaker Feature Store automatically labels your data. This saves time labeling data for supervised learning.\nD\nYou can use SageMaker Feature Store to store normalized features that can be reused for multiple models. This saves time preparing data for each new model.\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember that Feature Store serves as a centralized repository for features.\nSlide: 63\nQuestion 1 answer\nYou work for a hospitality company that operates hotels across the country. You want to build a machine learning model to predict customer loyalty using data about past customer stays and purchases. You are considering using SageMaker Feature Store to prepare the data for model training. \nHow could SageMaker Feature Store be an effective service option for this use case?\nResponse\nA\nSageMaker Feature Store is a marketplace where you can pay workers to complete discrete tasks like labeling data. This would verify that your customers\u2019 purchases are accurate.\nB\nYou can use SageMaker Feature Store to visualize and clean your data. Clean data is essential for machine learning accuracy.\nC\nSageMaker Feature Store automatically labels your data. This saves time labeling data for supervised learning.\nD correct\nYou can use SageMaker Feature Store to store normalized features that can be reused for multiple models. This saves time preparing data for each new model.\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nD: You can use SageMaker Feature Store to save time by storing curated features that can be accessed to train new models.\nThe incorrect responses are:\nA: Amazon Mechanical Turk is a service for hiring workers to complete tasks like labeling data. Feature Store helps manage and share features, not label data. You can use SageMaker Feature Store to store curated features that can be accessed to train new models.\nB: Although clean data is essential for ML accuracy, SageMaker Data Wrangler is a service that assists with data visualization and transformation. You can use SageMaker Feature Store to save time by storing curated features that can be accessed to train new models.\nC: SageMaker Ground Truth is a separate service for labeling data. SageMaker Feature Store helps manage and share features, not label data. You can use SageMaker Feature Store to save time by storing curated features that can be accessed to train new models.\nSlide: 64\nQuestion 2\nA telecommunications company wants to use Amazon SageMaker to train a machine learning model on their customer data. They have the data stored in different formats.\nWhich data formats do most Amazon SageMaker algorithms support for training? \nResponse\nA\nCSV and \nRecordIO-protobuf\nB\nAll data formats\nC\nNo data formats\nD\nCustom data formats\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember that SageMaker supports CSV and \nRecordIO-protobuf\n formats for training data.\nSlide: 65\nQuestion 2 answer\nA telecommunications company wants to use Amazon SageMaker to train a machine learning model on their customer data. They have the data stored in different formats.\nWhich data formats do most Amazon SageMaker algorithms support for training? \nResponse\nA correct\nCSV and \nRecordIO-protobuf\nB\nAll data formats\nC\nNo data formats\nD\nCustom data formats\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nA: Many Amazon SageMaker algorithms support training with data in CSV and \nRecordIO-protobuf\n format.\nThe incorrect responses are:\nB: Many Amazon SageMaker algorithms support certain data formats.\nC: Many Amazon SageMaker algorithms support certain commonly used data formats. \nD: Many Amazon SageMaker algorithms support certain commonly used data formats. \nSlide: 66\nResources\nTo learn more, choose from the following resources:\nAmazon SageMaker AI \u2013CSV Format\n (developer guide)\nAmazon SageMaker AI \u2013RecordIO Format \n(developer guide)\nTime-Series Datasets Format and Missing Values Filling Methods\n (developer guide)\nCreate, Store, and Share Features with Amazon SageMaker Feature Store\n (developer guide)\nTransform Data\n (developer guide)\nSpeaker Notes:\nSlide: 67\nModule summary\nYou should now be able to:\nExplain the value of data cleaning and transformation.\nDescribe how to process incorrect or duplicated data.\nDescribe how to detect and treat outliers.\nDescribe how to process missing values.\nDescribe fundamental encoding techniques.\nIdentify feature engineering use cases.\nDescribe fundamental concepts, benefits, and techniques of feature engineering.\nDescribe fundamental feature selection techniques.\nDescribe AWS services for validating and labeling data.\nIdentify AWS tools and services for visualizing and transforming data.\nSpeaker Notes:\n|Student notes\nSlide: 68\nQuestions?\nThank you for attending\nthis session\nCorrections, feedback, or other questions? \nContact us at \nhttps://support.aws.amazon.com/#/contacts/aws-training\n. \nAll trademarks are the property of their owners.\nSpeaker Notes:\n|Instructor Notes\n|\n|Student Notes",
        "RESPONSE": "Here are the speaker notes for slides 1-10 as requested:\n\nSlide 1: Module 3: Data Transformation and Feature Engineering\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide shows the title \"Module 3: Data Transformation and Feature Engineering\" for a Machine Learning Engineering on AWS course.\n~\n|INSTRUCTOR NOTES:\n- |Introduce the module on data transformation and feature engineering\n- |Highlight that this builds on previous modules on data collection and storage\n- |Explain that this module will cover key techniques for preparing data for ML\n|\n|STUDENT NOTES:\nAt this point in the course, you should be familiar with data collection, ingestion, and storage fundamentals. You've covered core concepts like data types and formats, as well as key AWS storage services. You can now select appropriate AWS storage solutions and data formats based on specific machine learning needs.\n\nThis module focuses on data transformation, which is a critical step in preparing data for machine learning. You'll learn about various transformation techniques, including data cleaning, encoding, and feature engineering. The module will also cover how to leverage AWS services like Amazon SageMaker Feature Store, Amazon SageMaker Data Wrangler, and AWS Glue to transform your data efficiently.\n\nBy the end of this module, you'll have a solid understanding of how to take raw data and transform it into a format that is optimized for machine learning model training and inference.\n\nSlide 2: Module objectives & outline\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide lists the learning objectives and outline for Module 3 on data transformation and feature engineering.\n~\n|INSTRUCTOR NOTES:\n- |Review the module objectives with students\n- |Highlight key topics that will be covered\n- |Explain how these objectives build on previous modules\n|\n|STUDENT NOTES:\nThis module aims to equip you with essential skills for data transformation and feature engineering in machine learning workflows. By the end of this module, you will be able to:\n\n1. Explain the importance of data cleaning and transformation in ML pipelines\n2. Apply techniques for handling incorrect, duplicated, and missing data\n3. Identify and treat outliers in datasets\n4. Implement fundamental encoding techniques for categorical data\n5. Recognize use cases for feature engineering and apply basic techniques\n6. Understand feature selection methods and their impact on model performance\n7. Utilize AWS services for data validation, labeling, visualization, and transformation\n\nThe module is structured to provide both theoretical knowledge and practical skills. You'll learn about core concepts and then see how to apply them using AWS tools and services. This comprehensive approach will prepare you to effectively prepare data for machine learning projects in real-world scenarios.\n\nSlide 3: Data processing: Data transformation and feature engineering\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide shows a diagram of the machine learning lifecycle, highlighting the data processing stage which includes data transformation and feature engineering.\n~\n|INSTRUCTOR NOTES:\n- |Explain where data transformation fits in the ML lifecycle\n- |Highlight the importance of this stage for model performance\n- |Discuss how this stage builds on previous data preparation steps\n|\n|STUDENT NOTES:\nData transformation and feature engineering are crucial steps in the machine learning lifecycle, occurring after data collection, ingestion, and storage, but before model development. At this stage, you're processing your data to prepare it for model training.\n\nThe goal of data transformation is to clean, encode, and engineer your dataset to ensure it's in the optimal format for your chosen machine learning algorithm. This process can significantly impact model accuracy and performance.\n\nKey techniques in this stage include:\n1. Data cleaning to handle errors, duplicates, and missing values\n2. Categorical encoding to convert text data into numerical formats\n3. Feature engineering to create new, more informative features\n\nBy investing time in thorough data transformation and feature engineering, you set the foundation for more accurate and efficient machine learning models. This stage bridges the gap between raw data and model-ready datasets, directly influencing the quality of insights your models can generate.\n\nSlide 4: Techniques for data transformation\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide outlines three main techniques for data transformation: data cleaning, categorical encoding, and feature engineering.\n~\n|INSTRUCTOR NOTES:\n- |Introduce the three main techniques for data transformation\n- |Provide brief examples of each technique\n- |Highlight the importance of each in preparing data for ML\n|\n|STUDENT NOTES:\nData transformation encompasses three key techniques essential for preparing datasets for machine learning:\n\n1. *Data cleaning* involves identifying and handling errors, duplicates, and outliers in your dataset. For example, you might need to remove duplicate customer IDs or correct inconsistent formatting in dates.\n\n2. *Categorical encoding* converts text-based categorical variables into numeric representations that ML algorithms can process. This could involve techniques like one-hot encoding or label encoding. For instance, encoding country names as numerical values.\n\n3. *Feature engineering* creates new features or modifies existing ones to help ML algorithms better understand the underlying patterns in the data. This might include techniques like binning continuous variables or creating interaction terms between features.\n\nEach of these techniques plays a crucial role in transforming raw data into a format that maximizes the performance of machine learning models. By applying these methods effectively, you can significantly improve the accuracy and efficiency of your ML solutions.\n\nSlide 5: Agenda\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide presents the agenda for the module, listing five main topics to be covered.\n~\n|INSTRUCTOR NOTES:\n- |Review the agenda for the module\n- |Briefly explain what will be covered in each topic\n- |Highlight any key areas students should pay particular attention to\n|\n|STUDENT NOTES:\nThis module is structured into five main topics, each focusing on a crucial aspect of data transformation and feature engineering:\n\n1. Handling incorrect, duplicated, and missing data\n2. Feature engineering concepts\n3. Numeric and text feature engineering\n4. Feature selection techniques\n5. AWS data transformation services\n\nWe'll start by exploring methods to clean and preprocess data, addressing common issues like errors, duplicates, and missing values. Then, we'll delve into feature engineering concepts and techniques for both numeric and text data. \n\nThe module will also cover feature selection methods to identify the most relevant features for your models. Finally, we'll examine how AWS services can streamline and automate various aspects of the data transformation process.\n\nBy the end of this module, you'll have a comprehensive understanding of data transformation techniques and how to implement them using AWS tools and services.\n\nSlide 6: Handling incorrect, duplicated, and missing data\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide highlights the first topic of the module: handling incorrect, duplicated, and missing data.\n~\n|INSTRUCTOR NOTES:\n- |Introduce the importance of data cleaning in ML pipelines\n- |Highlight common issues with raw datasets\n- |Explain the impact of data quality on model performance\n|\n|STUDENT NOTES:\nData cleaning is a critical first step in preparing your dataset for machine learning. Raw data often contains flaws that, if left unaddressed, can significantly impact the performance of your ML models.\n\nIn this section, we'll focus on three common data quality issues:\n\n1. Incorrect data: Values that are inaccurate or in the wrong format\n2. Duplicated data: Repeated entries that can skew your model's understanding\n3. Missing data: Gaps in your dataset that need to be handled appropriately\n\nUnderstanding how to identify and address these issues is crucial for building reliable and accurate machine learning models. We'll explore various techniques and best practices for handling each of these data quality challenges, ensuring your dataset is clean and ready for further transformation and modeling steps.\n\nSlide 7: Cleaning data for ML training\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide outlines three main aspects of data cleaning: handling incorrect or duplicate data, addressing data outliers, and dealing with incomplete or missing data.\n~\n|INSTRUCTOR NOTES:\n- |Explain the importance of data standardization\n- |Discuss methods for identifying and handling outliers\n- |Highlight strategies for dealing with missing data\n|\n|STUDENT NOTES:\nCleaning data for ML training involves three key areas:\n\n1. *Handling incorrect or duplicate data*: This process ensures your dataset is standardized and free from repetitive or erroneous entries. It's crucial to verify that your data is in the format and language your model expects.\n\n2. *Addressing data outliers*: Outliers are data points that significantly differ from the majority. While they can sometimes indicate novel discoveries, they can also skew your model's predictions. Identifying and examining outliers is essential to determine whether they represent genuine anomalies or data collection errors.\n\n3. *Dealing with incomplete or missing data*: Missing data can occur due to collection errors or changes in data collection processes over time. How you handle missing data can significantly impact your model's performance.\n\nEach of these aspects plays a crucial role in preparing a clean, consistent dataset that will lead to more accurate and reliable machine learning models. In the following slides, we'll dive deeper into techniques for addressing each of these data cleaning challenges.\n\nSlide 8: Incorrect data\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide presents examples of incorrect data and techniques for handling them.\n~\n|INSTRUCTOR NOTES:\n- |Discuss common types of incorrect data\n- |Explain various techniques for handling incorrect data\n- |Emphasize the importance of domain knowledge in identifying errors\n|\n|STUDENT NOTES:\nIncorrect data can take many forms and can significantly impact the quality of your machine learning models. Common examples include:\n\n1. Language differences: Inconsistent use of language across entries\n2. Typos: Spelling errors or mistyped information\n3. Inconsistent formats: Data entered in different formats (e.g., dates)\n\nTo address incorrect data, consider the following techniques:\n\n1. Inspection: Carefully review your dataset to identify anomalies\n2. Filling correct values: Replace incorrect values with accurate information\n3. Fix formatting: Standardize data formats across your dataset\n4. Exclude from dataset: Remove entries that cannot be corrected if appropriate\n\nRemember, the goal is to create a clean, consistent dataset. This often requires a combination of automated tools and manual review, especially when domain knowledge is needed to identify subtle errors. By addressing incorrect data, you improve the reliability and accuracy of your machine learning models.\n\nSlide 9: Duplicated data\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide explains duplicated data and techniques for handling it, focusing on deduplication.\n~\n|INSTRUCTOR NOTES:\n- |Explain what duplicated data is and why it's problematic\n- |Discuss common causes of data duplication\n- |Introduce deduplication as a solution\n|\n|STUDENT NOTES:\nDuplicated data refers to repeated records in a dataset, which can occur due to various reasons:\n\n1. Data collected from multiple sources\n2. Incorrect data entry processes\n3. System glitches during data collection or storage\n\nDuplicates can skew your data distribution and over-represent certain examples, leading to biased model predictions. To address this issue, you can use a process called *deduplication*.\n\nDeduplication is the automated process of removing duplicate records from a dataset. It works by:\n\n1. Scanning the dataset for identical or near-identical records\n2. Retaining one copy of the information\n3. Replacing other instances with pointers to the stored copy\n\nEffective deduplication can significantly improve your data quality and reduce storage requirements. When implementing deduplication, consider factors like which fields to compare and how to handle near-duplicates. Remember, the goal is to create a dataset where each unique entity or event is represented once, ensuring your model learns from a balanced and accurate representation of your data.\n\nSlide 10: Activity: Identifying incorrect and duplicated data\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide presents a dataset for an activity on identifying incorrect and duplicated data.\n~\n|INSTRUCTOR NOTES:\n- |Guide students through the activity\n- |Encourage them to identify different types of data issues\n- |Discuss potential solutions for each issue identified\n|\n|STUDENT NOTES:\nIn this activity, you'll practice identifying incorrect and duplicated data in a sample dataset. As you review the data, look for:\n- Missing data\n- Inconsistent formatting\n- Misplaced information\n- Duplicate entries\n\nConsider how you would address each issue you identify. For example:\n- How would you handle missing values?\n- What steps would you take to standardize inconsistent formats?\n- How would you correct misplaced information?\n- What process would you use to remove duplicates?\n\nThis exercise will help you develop a critical eye for common data quality issues and start thinking about appropriate strategies for addressing them. Remember, the goal is to create a clean, consistent dataset that will lead to more accurate and reliable machine learning models.",
        "PARAMETERS": {
            "temp": 0.5,
            "topP": 1.0,
            "topK": 250,
            "maxT": 200000,
            "stopSeq": ""
        }
    },
    {
        "timestamp": "2025-05-08 12:48:15",
        "MODEL": "Claude 35S",
        "COMMAND": "This is a complex prompt. Take your time to read through the entire prompt and understand every writing guideline and format required. \n\nOnce you are sure of the requirements, proceed.\n\nDo not use Amazon Bedrock cached data.\n\n[Role]\nYou are an AWS Technical Curriculum Developer creating slide notes for a presentation.\nRead the text in the INPUT. This is a text version of a PowerPoint slide presentation.   \nAll text in the INPUT must be treated as content. Read and unsderstand the contents of every slide\n\nCarefully identify the individual slide and \"Slide: nn\" followed by the slide title on the next line.\nIn youre response, the format will be Slide nn: Slide title consistently for every slide.\n\n[variables]\nTarget_audience: Machine learning engineers\nInstructional_level: 200-300 \nSpecific_AWS_services: Amazon SageMaker AI\n/variables\n\n[Task]\nPerform this work ONLY for slides 1-10. \nWhen you complete the last slide in this range, stop processing.\n\nYour audience is {{Target_audience}} who are {{Instructional_level}} in {{Specific_AWS_services}}.\n\nGenerate concise student notes for each PowerPoint slide that:\n1. Captures the essential concepts (maximum 3-4 sentences)\n2. Do not reveal information from upcoming slides\n3. Provide context only for THIS slide's content\n4. Use clear, direct language without unnecessary elaboration\n5. Include only relevant examples that reinforce the current slide's concepts\n\nUse existing notes and slide content as your primary source of information.\nExpand on existing slide notes when required, but do not use verbose language.\nFor long Student Notes, edit to 500 words maximum for each slide's student notes.\n\n[Output]\nThe complete SPEAKERNOTES: for each slide explicitly following the format and example below.  Every slide must contain every section of the SPEAKERNOTES.\n\n[Guidelines]\n1.  Your response shall be the entire notes text for all the slides specified.\n2.  FOLLOW the FORMAT explicitly and consistently for every slide.\n3.  Write clearly and concisely, ensuring technical accuracy. \n4.  Reference https://docs.aws.amazon.com for {{Specific_AWS_services}} details. \n5.  Provide valuable insights beyond what's visible on the slide. \n6.  Include transitional statements when changing topics, especially when moving from one section to the next. \n7.  Write in second person only.  Only use \"we\" to describe Amazon or AWs.\n8.  Add *italics* markdown to introduce a concept to the student notes.\n9. For lists, use standard bullets unless describing a sequence of steps then use a numbered list.\n\nFor Image Description, create a brief, accurate description for the image on this slide that:\n1. Describes what the image actually shows (no assumptions)\n2. Explains how the image directly relates to the slide's key point\n3. Uses no more than 2 sentences\n4. Avoids explanations that should be in the main student notes\n5. Begins with \"This image shows...\" or \"This diagram illustrates...\"\n\nDO NOT make statements like \"This slide is about x\" or \"This slide does that\" or \"This slide introduces...\" or \"This slide demonstrates...\"\n\n[FORMAT]\nSlide nn: Slide Title [in exactly this format]\nSPEAKERNOTES:\n~Developer Notes:\n~[Reference materials and URLs - live, accessible, relevant links]\n~\n~Image Description:\n~[Detaied visual description of the slide content or images.]\n~\n|INSTRUCTOR NOTES:\n- |[Key topics to cover]\n- |[Key topics to cover]\n- |...\n|\n|STUDENT NOTES:\n[3-4 clear, concise, and complete paragraphs expanding on slide content.]\n[300 words maximum]\n\n[EXAMPLES]\nSlide 1: Introduction to AWS Generative AI and ML\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/whitepapers/latest/aws-overview/machine-learning.html\n~\n~Image Description:\n~A diagram showing the AWS ML stack, including SageMaker, Comprehend, and Rekognition\n~\n|INSTRUCTOR NOTES:\n- |Introduce AWS Generative AI and ML services\n- |Highlight key benefits and use cases\n- |Discuss integration with other AWS services\n|\n|STUDENT NOTES:\nAWS Generative AI and Machine Learning services provide powerful tools for creating intelligent applications. These services include Amazon SageMake AI for end-to-end ML workflows, Amazon Comprehend for natural language processing, and Amazon Rekognition for image and video analysis.\n\nGenerative AI models, such as those powering Amazon Bedrock, can create new content like text, images, and code based on input prompts. This technology opens up exciting possibilities for content creation, language translation, and creative assistance across various industries.\n\nAWS ML services integrate seamlessly with other AWS offerings, allowing for scalable, *cost-effective*, and *secure* AI/ML solutions. By leveraging the AWS cloud infrastructure, businesses can easily deploy and manage ML models, process large datasets, and incorporate AI capabilities into their applications without the need for extensive in-house expertise or infrastructure.\n\nSlide 10: Activity: Identifying Incorrect and Duplicated Data\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide presents a dataset for an activity on identifying incorrect and duplicated data.\n~\n|INSTRUCTOR NOTES:\n- |Guide students through the activity\n- |Encourage them to identify different types of data issues\n- |Discuss potential solutions for each issue identified\n|\n|STUDENT NOTES:\nIn this activity, you'll practice identifying incorrect and duplicated data in a sample dataset. As you review the data, look for:\n- Missing data\n- Inconsistent formatting\n- Misplaced information\n- Duplicate entries\n\nConsider how you would address each issue you identify. For example:\n- How would you handle missing values?\n- What steps would you take to standardize inconsistent formats?\n- How would you correct misplaced information?\n- What process would you use to remove duplicates?\n\nThis exercise will help you develop a critical eye for common data quality issues and start thinking about appropriate strategies for addressing them. Remember, the goal is to create a clean, consistent dataset that will lead to more accurate and reliable machine learning models.\n\n",
        "INPUT": "Slide: 1\nModule 3: Data Transformation and Feature Engineering\nMachine Learning Engineering on AWS\nSpeaker Notes:\n|Student notes\nAt this point, you are familiar with data collection, ingestion, and storage. You've covered data fundamentals such as data types and formats, and you've been introduced to core AWS storage services. You can choose the most effective AWS storage decision and data format based on your specific machine learning (ML) needs. \nIn this module, you\u2019ll learn about data transformation. This module covers various transformation concepts and techniques, such as data cleaning, encoding, and feature engineering. You\u2019ll discover how to use AWS services such as Amazon SageMaker Feature Store, Amazon SageMaker Data Wrangler, and AWS Glue to transform your data.\nSlide: 2\nModule objectives & outline\nOn completion, you will be able to:\nExplain the value of data cleaning and transformation.\nDescribe how to process incorrect or duplicated data.\nDescribe how to detect and treat outliers.\nDescribe how to process missing values.\nDescribe fundamental encoding techniques.\nIdentify feature engineering use cases.\nDescribe fundamental concepts, benefits, and techniques of feature engineering.\nDescribe fundamental feature selection techniques.\nDescribe AWS services for validating and labeling data.\nIdentify AWS tools and services for visualizing and transforming data.\nSpeaker Notes:\n|Student Notes\nSlide: 3\nData processing: Data transformation and feature engineering\nBusiness goals\nML problem framing\nData processing\nModel development\nDeployment\nMonitoring\nSpeaker Notes:\n|Student notes\nDuring this phase of the machine learning lifecycle, you\u2019re processing your data in preparation for model development. By this point in the data preparation process, you\u2019ve collected, ingested, extracted, and stored your data as appropriate for your business\u2019s ML needs. Now, you are ready to transform that data into a clean, encoded, engineered dataset. Data cleaning, categorical encoding, and feature engineering are important techniques you can use to help ensure a more accurate and model-ready dataset.\nSlide: 4\nTechniques for data transformation\nData cleaning\nCategorical encoding\nFeature engineering\nThe label encoding examples in the source are not good ones. For example, by encoding countries with numbers like 1, 2, 3 you are assigning some countries higher numerical weight which will impact the model. Countries should be one-hot encoded so that they have the same weight. A better example for label encoding would be small, medium, large sizes as 1,2,3  since that preserves a relative weighting.  \nSpeaker Notes:\n|Student notes\nCleaning, encoding, and engineering your dataset is essential to produce more accurate and efficient results from your machine learning model.\nData cleaning \nis an important first step before training an ML model. Raw data often contains errors such as missing values, duplicates, and outliers that need to be identified and handled to ensure data accuracy and quality. \nExample: A few customer IDs are duplicated in a dataset. \nSolution: Remove or consolidate duplicated customer IDs. \nCategorical encoding \nconverts categorical variables into numeric representations that ML algorithms can mathematically interpret. Common encoding techniques include one-hot encoding and label encoding. \nExample: Data includes a country feature with values like USA, Canada, and UK.\nSolution: Encode the countries as numbers, like 1, 2, and 3, so the ML algorithm can mathematically compare them.\nFeature engineering \ncreates new features or selects relevant features to help ML algorithms better understand the relationships in the data. Methods such as binning, bag-of-words, and N-gram extract key information from text data. \nExample: Train a model to predict whether a review is positive or negative.\nSolution: Use the bag-of-words method to convert text into counts of word occurrences and represent text data in a numeric form. \nSlide: 5\nAgenda\nTopic A: Handling incorrect, duplicated, and missing data\nTopic B: Feature engineering concepts\nTopic C: Numeric and text feature engineering\nTopic D: Feature selection techniques\nTopic E: AWS data transformation services\nSpeaker Notes:\nSlide: 6\nHandling incorrect, duplicated, and missing data\nTopic A: Handling incorrect, duplicated, and missing data\nTopic B: Feature engineering concepts\nTopic C: Numeric and text feature engineering\nTopic D: Feature selection techniques\nTopic E: AWS data transformation services\nSpeaker Notes:\n|Student notes\nOne of the first steps of preparing your dataset after collection, ingestion, and storage is cleaning up your data. This can be a time-consuming process, but without taking care of flaws in your data, your machine learning (ML) model will not function properly.\nSlide: 7\nCleaning data for ML training\nIncorrect or duplicate data\nData outliers\nIncomplete or missing data\nSpeaker Notes:\n|Student notes\nThe goal for data cleaning is to standardize your dataset. You want to make sure the data is in the language and format that your model expects. Part of this standardization involves checking your dataset for incorrect or duplicated data.\nAnother key method of cleaning up your data is to identify data outliers. Outliers are data points that differ significantly from the majority of the data. \nOutliers in your data can present experimental anomalies or even highlight novel discoveries, so they are not necessarily data points you always want to remove. However, they can make it harder for your model to make accurate predictions. Detecting and examining outliers is important to the data preparation process, because they might be indicating errors in data collection or skewing values.\nIn addition to inspecting your data for incorrect or duplicated information and data outliers, you also want to address any areas with incomplete or missing data. \nSome columns in your dataset might be missing data because of a data collection error. It is also possible that data was not collected on a particular feature until well into the data collection process.\nSlide: 8\nIncorrect data\nExamples\nLanguage differences\nTypos\nInconsistent formats\nTechniques\nInspection\nFilling correct values\nFix formatting\nExclude from dataset\nSpeaker Notes:\n|Student notes\nDatasets often contain incorrect or anomalous data points that need to be addressed before model training. Common issues include language differences, typos from data entry, values that don't conform to expected formats, and more. To fix incorrect data values, inspect your dataset for anomalies. You might need to fill in the correct values, convert invalid formats to valid formats, or exclude the incorrect entries from the dataset to prevent inaccurate model predictions.\nSlide: 9\nDuplicated data\nExamples\nCopies from multiple sources\nIncorrect data entry\nTechniques\nDeduplication\nSpeaker Notes:\n|Student notes\nDuplicated data refers to records in a dataset that contain the same information. Duplicated data can occur when data is gathered from multiple sources or is entered incorrectly. After your data is fully collected, it's important to inspect the data for duplicate records and then delete or consolidate the duplicate entries. When a dataset contains multiple copies of the same data points, it skews the distribution of the data and over-represents certain examples.\nThe process of automating data duplication removal is called \ndeduplication\n. Deduplication works by scanning datasets for duplicate information, retaining one copy of the information, and replacing the other instances with pointers that direct back to the stored copy. Deduplication can drastically increase storage capacity by keeping only unique information in your data repositories.\nSlide: 10\nActivity: Identifying incorrect and duplicated data\nSpeaker Notes:\n|Student notes\nThe research team is working with the following set of data. Which errors do you notice in this dataset? You might notice that there is missing data, inconsistent formatting, or misplaced information. Consider how to identify and fix incorrect and duplicated data.\nSlide: 11\nActivity: Identifying incorrect and duplicated data\nSpeaker Notes:\n|Student notes\nBaccalaur\u00e9at\n \nis incorrect because it is in a different language than the rest of the dataset. Although the data is placed correctly in the \nEducation\n column, the word \nbaccalaur\u00e9at\n is a French word for a high school diploma. The data would need to be translated to English so the dataset has a standard language to process.\nMasters/PhD\n is incorrect because it is placed in the wrong column. \nMasters/PhD\n is a valid answer for the Education column, but is incorrectly placed in the \nState\n column. This data would need to be moved to the Education column, and you would need to impute the missing data for the State column.\nThe last row is an example of duplicated data. You might recognize that the Participant ID of 123456 is the biggest marker for duplication. There is already a row of data that contains 123456 as the ID.  You need to remove this entire row of data to keep data weights balanced and prevent bias.\nSlide: 12\nDetecting outliers by using mean and median\nMean\nMedian\nMiddle value of dataset\nBetter metric when data is skewed\nConsider both natural and artificial outliers.\nAverage of all values\nUseful when data is symmetrical\nAge\nCount\nMedian\nMean\nSpeaker Notes:\n|Student notes\nThere are a number of different methods you can use to detect and measure the impact of outliers in your data. These methods include calculating mean and median and determining if the outlier is natural or artificial. \nThese measures describe the extent to which your data is clustered around some value or position. \nThe \nmean\n is the average of all the values in the dataset. Mean can be a useful method for understanding your data when the data is symmetrical. For example, a symmetrical set of data that contains ages of respondents might reflect that both the mean and median of the dataset is 50 years old. However, if your data contains outliers, the mean can be skewed one way toward those values. In that case, your mean would not serve as an accurate representation of where your values are truly centered.\nThe \nmedian\n is the value in the dataset that divides the values into two equal halves. To calculate the median, you arrange all the numerical values in ascending order and identify the middle value. If your data is skewed or contains outliers, the median tends to provide the better metric for understanding your data as it relates to central tendency. \nFor example, a dataset that contains income level might contain outliers. The mean might skew toward higher or lower values, while the median would provide a more accurate picture of the data's central tendency.\nOutliers can be caused by either natural or artificial factors. A natural outlier is not the result of an artificial error but is reflective of some truth in the data.\nNatural outliers \nare data points that are accurate representations of data, but are extreme variations of the central data points. For example, in a dataset that includes height measurements of individuals, an extremely tall individual would represent a natural outlier.\nArtificial outliers \nare anomalous data points in your dataset due to error or improper data collection. For example, a faulty sensor in a thermometer might produce a body temperature that is unrealistically high or low compared to expected body temperatures. \nSlide: 13\nAddressing incomplete or missing data\nExploration\nIdentify missing values\nDetermine why values are missing\nMissing at Random (MAR)\nMissing Completely at Random (MCAR)\nMissing Not at Random (MNAR)\nTechniques\nDrop rows with missing values\nImpute values\nSpeaker Notes:\n|Student notes\nUnfortunately, most ML algorithms cannot deal with missing values automatically. You need to use human intelligence to replace missing values with something meaningful and relevant to the problem.\nA few missing values might not be an issue. However, if you\u2019re missing many values in one column, you might find it difficult to interpret the relationship between that feature and the target. In other words, it can be hard to understand how a value in a row corresponds to what needs to be predicted by the model. There are some key steps you can take to address incomplete and missing values in your dataset.\nIdentify missing values \n\u2013There are certain Python libraries, such as Pandas, that you can use to check for missing values. You can also use AWS services to assist you in this process. For example, Amazon SageMaker Autopilot provides a number of filling methods to handle missing values, and Amazon SageMaker Data Wrangler is a full-service data preparation tool. You will learn about SageMaker Data Wrangler in more detail later in this course.\nDetermine why values are missing \n\u2013Before you can determine how to treat the missing values, it\u2019s important to investigate which mechanisms caused the missing values. The following are three common types of missing data:\nMissing at Random (MAR): The probability that a data point is missing depends only on the observed data, not the missing data. Example: In a dataset of student test scores, scores are missing for some students who were absent that day. Absence is related to performance. \nMissing Completely at Random (MCAR): The probability that a data point is missing does not depend on the observed or unobserved data. Example: In an employee survey, some people forgot to answer the question about their number of siblings. Their missing sibling data does not depend on any values.\nMissing Not at Random (MNAR): The probability that a data point is missing depends on the missing data itself. Example: In a financial audit, companies with accounting irregularities are less likely to provide complete records. The missing data depends on the sensitive information being withheld.\nDrop missing values \u2013 \nDepending on what is causing your missing values, you will decide to either drop the missing values or impute data into your dataset. One of the most straightforward ways to deal with missing values is to remove the rows of data with missing values. You can accomplish this by using a Pandas function. Dropping rows or columns will make the dataset non-missing. However, the risk of dropping rows and columns is significant. If you have hundreds of rows or columns of data, all of that missing data might cause bias in your model predictions. If you drop too much data, you might not have enough features to feed the model.\nImpute values \n\u2013 Missing values might be related to new features that haven\u2019t included your dataset yet. After you include more data, those missing values might be highly correlated with the new feature. In this case, you would deal with missing values by adding more new features to the dataset. If you determine the values are missing at random, data imputation, or inputting the data into your dataset, is most likely the best option. There are different ways to impute a missing value. One common way to impute missing values is to replace the value with the mean, median, or most frequent value. You would select the mean, median, or most frequent value for categorical variables. You would select the mean or median for numerical variables. Choosing the mean, median, or most frequent value depending on your business problem and data collection procedures. \nWhen there is a missing value, always consider whether those values are missing at random. If they are not, try to investigate what the mechanism is behind the missing value. You can often find more efficient ways to impute those missing values. Remember that a majority of ML algorithms can\u2019t take missing values. You will most likely need to either drop the values or impute data for the ML model to perform.\nSlide: 14\nQuestion 1\nA real estate company is working on a machine learning project where the goal is to predict housing prices. The team has a dataset that contains information on the number of bedrooms, the square footage, and the locations of different houses. Before building a machine learning model, the team wants to prepare and transform the data. \nWhich data preparation technique could convert values for the home's location (with values of urban, suburban, and rural) into numeric representations that can be fed into a machine learning algorithm? \nResponse\nA\nData cleaning\nB\nFeature engineering\nC\nCategorical encoding\nD\nDimensionality reduction\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember which data preparation technique converts string values into numeric representations. \nSlide: 15\nQuestion 1 answer\nA real estate company is working on a machine learning project where the goal is to predict housing prices. The team has a dataset that contains information on the number of bedrooms, the square footage, and the locations of different houses. Before building a machine learning model, the team wants to prepare and transform the data. \nWhich data preparation technique could convert values for the home's location (with values of urban, suburban, and rural) into numeric representations that can be fed into a machine learning algorithm? \nResponse\nA\nData cleaning\nB\nFeature engineering\nC correct\nCategorical encoding\nD\nDimensionality reduction\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nC: Categorical encoding can be used to convert values for the home's location into numeric representations. For example, you could have 0 for urban, 1 for suburban, and 2 for rural. This allows machine learning algorithms to mathematically interpret the data.\nThe incorrect responses are:\nA: Data cleaning focuses on handling issues like missing data and outliers.\nB: Feature engineering focuses on modifying or creating new features from the data, rather than encoding features.\nD: Dimensionality reduction techniques reduce the number of features but don't convert data types.\nSlide: 16\nQuestion 2\nA non-profit organization is gathering data to decide which type of fundraiser their patrons would like to attend. They decide to provide two options for patrons to share their preferences: an online survey and a hard-copy mailer. Some patrons plan to fill out both the online and hard-copy survey to make sure their preference is recorded.\nWhich data cleaning technique will BEST help the company retain unique data points for the most effective machine learning (ML) solution?\nResponse\nA\nReformatting the hard-copy mailer to contain only \"Yes/No\" answers\nB\nInspecting the data for incorrectly entered dates\nC\nIdentifying duplicate data from those who submitted both an online and hard-copy survey\nD\nChecking the datasets for mistyped patron names\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember that duplicate data must be removed from a training dataset to avoid model bias.\nSlide: 17\nQuestion 2 answer\nA non-profit organization is gathering data to decide which type of fundraiser their patrons would like to attend. They decide to provide two options for patrons to share their preferences: an online survey and a hard-copy mailer. Some patrons plan to fill out both the online and hard-copy survey to make sure their preference is recorded.\nWhich data cleaning technique will BEST help the company retain unique data points for the most effective machine learning (ML) solution?\nResponse\nA\nReformatting the hard-copy mailer to contain only \"Yes/No\" answers\nB\nInspecting the data for incorrectly entered dates\nC correct\nIdentifying duplicate data from those who submitted both an online and hard-copy survey\nD\nChecking the datasets for mistyped patron names\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nC: Because the company has data points from multiple formats, there is a likelihood of duplication in their dataset. It's important to identify duplication to prevent the ML solution from overrepresenting certain examples.\nThe incorrect responses are:\nA,B,D: These options would not address the probable duplication in the dataset. Because the company has data points from multiple formats, there is a likelihood of duplication in their dataset. It's important to identify duplication to prevent the ML solution from overrepresenting certain examples.\nSlide: 18\nQuestion 3\nYou are cleaning up a dataset that includes the body weights of patients at a medical clinic. A clinician tells you that they usually see adults, but children occasionally visit the clinic.\nWhich data measurement BEST provides the central tendencies of your data?\nResponse\nA\nYou would choose the mean, because the best measurement of data that might contain outliers is taking the average of all data entries.\nB\nYou would choose the median, because the best measurement of data that might contain outliers is sorting the data and dividing the dataset at the middle variable.\nC\nBecause the data is numerical, you can use any method of measurement to find the average weight of a patient.\nD\nThere is no measurement you can use to find central tendencies of data.\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember the best measurement to find the central tendencies when the data is skewed.\nSlide: 19\nQuestion 3 answer\nYou are cleaning up a dataset that includes the body weights of patients at a medical clinic. A clinician tells you that they usually see adults, but children occasionally visit the clinic.\nWhich data measurement BEST provides the central tendencies of your data?\nResponse\nA\nYou would choose the mean, because the best measurement of data that might contain outliers is taking the average of all data entries.\nB correct\nYou would choose the median, because the best measurement of data that might contain outliers is sorting the data and dividing the dataset at the middle variable.\nC\nBecause the data is numerical, you can use any method of measurement to find the average weight of a patient.\nD\nThere is no measurement you can use to find central tendencies of data.\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nB: Because the clinic mostly sees adults, a few entries of children's weights might skew the average patient weight. In this case, it is best to use median to better understand the central tendencies of your data.\nThe incorrect responses are:\nA,C: Because the clinic mostly sees adults, a few entries of children's weights might skew the average patient weight. The mean of all patient data would skew towards the outlier values.\nD: You can use measurements such as mean and median to find central tendencies of your data.\nSlide: 20\nQuestion 4\nAn apparel company is building a machine learning solution to determine which types of products are not selling well. They've collected data on how long products sit in inventory, but their dataset contains many missing values.\nWhich method is the most effective option for addressing missing values in the apparel company's dataset?\nResponse\nA\nInspecting the reason for the missed values, and dropping, leaving, or imputing data accordingly\nB\nDropping all the data that contains missing values\nC\nManually imputing every missing value\nD\nLetting their ML algorithm replace the missing values\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember the steps that should be followed when there is missing data. \nSlide: 21\nQuestion 4 answer\nAn apparel company is building a machine learning solution to determine which types of products are not selling well. They've collected data on how long products sit in inventory, but their dataset contains many missing values.\nWhich method is the most effective option for addressing missing values in the apparel company's dataset?\nResponse\nA correct\nInspecting the reason for the missed values, and dropping, leaving, or imputing data accordingly\nB\nDropping all the data that contains missing values\nC\nManually imputing every missing value\nD\nLetting their ML algorithm replace the missing values\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nA: It's important to understand why the data is missing. Then, the company can set mechanisms in place to drop rows or columns of data, leave empty values, or impute data as needed.\nThe incorrect responses are:\nB: It's important to understand why the data is missing. Then, the company can set mechanisms in place to drop rows or columns of data, leave empty values, or impute data as needed. Dropping all missing values could remove significant and relevant data.\nC: This method is time-intensive. It's important to understand why the data is missing. Then, the company can set mechanisms in place to drop rows or columns of data, leave empty values, or impute data as needed.\nD: ML algorithms have trouble with missing values. While you can use AWS services to help locate and replace missing values, ML models themselves are not the best option for replacing missing values.\nSlide: 22\nFeature engineering concepts\nTopic A: Handling incorrect, duplicated, and missing data\nTopic B: Feature engineering concepts\nTopic C: Numeric and text feature engineering\nTopic D: Feature selection techniques\nTopic E: AWS data transformation services\nSpeaker Notes:\n|Student notes\nDuring the data transformation process, it is likely that you will come across different data formats. Remember, machine learning algorithms prefer numbers, so data that is text-based needs to be converted to numbers if possible. Categorical encoding is the process of manipulating text-based variables into number-based variables.\nAfter your data has been cleaned up and you\u2019ve encoded your data as necessary for your model, you can fine-tune or create new features in your dataset through feature engineering. Feature engineering is a method for transforming raw data into more informative features that help models better capture underlying relationships in the data.\nSlide: 23\nFeature engineering requirements\nUnderstanding the raw data\nRecognizing the business problem\nSelecting the right ML algorithm\nSpeaker Notes:\n|Student notes\nFeature engineering is more of an art than a science. Often, feature engineering is not a set of rules, and your approach will be based on the specific business problem that you are trying to solve.\nFeature engineering is often a highly intuitive practice. Understanding the raw data, familiarity with your business problem, and intuition of how to bridge the data and the algorithm work in parallel to determine your feature engineering requirements. Humans are skilled at finding relationships between the problem and the input feature of the raw data, so lean into your intuition when employing feature engineering.\nSlide: 24\nFeature engineering use cases \nNumeric feature engineering\nWide range or high variance\nOutliers or skewed distribution\nTime-based numeric features\nText feature engineering\nCategorical text\nLarge chunks of text\nComplex relationships\nSpeaker Notes:\n~Dev notes\n~Alt text from digital: Graph with mostly concentrated plot points and six outliers.\n|Student notes\nDepending on your data\u2019s format, you will select different methods of feature engineering to transform your dataset. \nNumeric feature engineering \ninvolves transforming numeric values for the model and is often accomplished by grouping different numeric values together. Sometimes, ML algorithms have trouble handling large numbers or isolated numbers. Grouping numeric values helps the model to better understand the relationships of these values.\nNumeric feature engineering strategies such as feature scaling, binning, and log transformation can help the ML model better understand the relationships of the data\u2019s numeric values. When deciding whether your dataset could benefit from numeric feature engineering, look for these qualities:\nNumeric features that have a wide range of values or high variance.\nNumeric features with outliers or a skewed distribution are also good targets for techniques like log transforms or binning to reduce the influence of extreme values.\nNumeric features related to time, such as timestamps, are good candidates for extraction of more meaningful metrics like day of the week or hour of the day.\nText feature engineering \ninvolves transforming text for the model and is often accomplished by splitting the text into smaller pieces. When dealing with text data, or corpus data, you can use feature engineering to transform attributes within your data to make those attributes more useful for your ML model. The purpose of text feature engineering is to break your text down into more bite-sized pieces for your model to understand the weights and relationships of your raw data.\nText feature engineering strategies, such as bag-of-words and N-gram, can tokenize the value of text data for your ML model to infer the right balance of information from that data. Because text feature engineering improves the ability for your model to understand your data effectively, essentially every type of raw data that includes text can benefit from some type of text feature engineering.\nCategorical features are associated with a group, and can often be represented numerically.\nSlide: 25\nCategorical encoding\nBinary\nTwo options \nNominal\nMultiple categories\nOrdinal\nCategories where order of values matter\nSome ML algorithms do not require encoding.\nSpeaker Notes:\n|Student notes\nWhen it comes to your data, the term numerical refers to values expressed as a measurable number, and categorical refers to features that are associated with a group. For example, in a dataset that contains animal medical records, a cat\u2019s weight is considered numerical, and the breed of cat is considered categorical.\nThe following list provides different types of categorical values.\nBinary\n categorical values refer to values that are one of two options. For example, a column of data might be true or false, such as if someone attended an event.\nNominal\n, or multi-categorical, values are category values where order does not matter. A set of data that contains different geographic locations, such as states or cities, might be considered multi-categorical.\nOrdinal\n, or ordered, values are category values where the order does matter, like what size of drink you order at a coffee shop: small, medium, or large.\nNot all categorical variables need to be encoded. Depending on your use case, different ML algorithms might not require you to encode your variables. \nFor instance, a random forest model can handle categorical features directly. You would not need to encode values, such as teal, green, and blue, as numeric values.\nSlide: 26\nActivity: Assessing data for categorical encoding\nSpeaker Notes:\n|Student notes\nThe data gathered by the research scientist team includes various types of numerical and categorical data. \nSlide: 27\nActivity: Assessing data for categorical encoding\nSpeaker Notes:\n|Student notes\nThe \nage\n column contains numerical data. Because this data is already set as numeric values, there is no need for categorical encoding.\nThe \neducation\n column is categorical data. Depending on which type of ML algorithm you decide to use, you might need to encode this data before using it to train and inform your model.\nThe \nflu shot \ncolumn is binary categorical data because respondents answered Yes or No. Depending on the type of algorithm you decide to use, this data might need to be encoded for your model to read the data more easily. For example, you can encode Yes to 0 and No to 1.\nThe \noutbreak zone \ncolumn is also binary categorical data. In this case, the research scientists could have also offered more value options for this feature that would be categorized as ordered, or ordinal, data. For instance, the scientists could categorize outbreak by light, medium, or heavy outbreak, rather than a binary Yes or No outbreak. Either way, this column might need to be encoded for model training, depending on the algorithm you decide to use. \nSlide: 28\nQuestion 1\nAn automobile company wants to use a machine learning solution to determine how quickly their various sizes of vehicles reach their highest speeds. The vehicle sizes are compact, mid-size, and full-size. \nWhich type of categorical data BEST describes the feature of their dataset that lists the size of the vehicle?\nResponse\nA\nNumerical\nB\nNominal\nC\nOrdinal\nD\nBinary\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember the type of categorical values that are ranked in order. \nSlide: 29\nQuestion 1 answer\nAn automobile company wants to use a machine learning solution to determine how quickly their various sizes of vehicles reach their highest speeds. The vehicle sizes are compact, mid-size, and full-size. \nWhich type of categorical data BEST describes the feature of their dataset that lists the size of the vehicle?\nResponse\nA\nNumerical\nB\nNominal\nC\nOrdinal\nD\nBinary\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nC: Ordinal data is the correct type of categorical data here, because the vehicle sizes like compact, midsize, and full-size are categorical labels with a numeric order. Ordinal data describes data that is categorical, but ranked in a numeric order.\nThe incorrect responses are:\nA: Numerical data represents quantitative values. The vehicle sizes (compact, mid-size, full-size) are qualitative categories, not numerical values.\nB: This question is asking about the categorical data that would describe the vehicle sizes. The vehicle sizes, such as compact, midsize, and full-size, would be considered ordinal data because they are categorical values that have a specific size order.\nD: Binary data represents categories with only two possible values. The vehicle sizes have three possible categories: compact, mid-size, full-size.\nSlide: 30\nQuestion 2\nAn energy company wants to assess how often their customers have to replace their solar panels after their initial solar panel purchase. Their raw data includes the dates of first purchase, customer reviews, and dates of additional purchases.\nHow might this company use feature engineering to help the machine learning model understand the relationship between these features in their dataset?\nResponse\nA\nThis is not a good use case for feature engineering, because the data includes a text-based feature.\nB\nThe company can use numeric feature engineering to extract information from the purchase dates, and they can use text feature engineering to extract information from the customer reviews.\nC\nThe company can only use numeric feature engineering to extract information from this dataset.\nD\nThe company can only use text feature engineering to extract information from this dataset.\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember that machine learning models find patterns in historical data and use that to make predictions.\nSlide: 31\nQuestion 2 answer\nAn energy company wants to assess how often their customers have to replace their solar panels after their initial solar panel purchase. Their raw data includes the dates of first purchase, customer reviews, and dates of additional purchases.\nHow might this company use feature engineering to help the machine learning model understand the relationship between these features in their dataset?\nResponse\nA\nThis is not a good use case for feature engineering, because the data includes a text-based feature.\nB correct\nThe company can use numeric feature engineering to extract information from the purchase dates, and they can use text feature engineering to extract information from the customer reviews.\nC\nThe company can only use numeric feature engineering to extract information from this dataset.\nD\nThe company can only use text feature engineering to extract information from this dataset.\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nB: Extracting information from these different features might help to show the relationship between initial purchase, review sentiment, and any additional purchases.\nThe incorrect responses are:\nA: Both the numeric and text-based data in this dataset are good candidates for feature engineering.\nC: The company can use text feature engineering to extract meaningful information from customer reviews..\nD: The company can use numeric feature engineering to extract meaningful information from initial and additional purchase dates.\nSlide: 32\nNumeric and text feature engineering\nTopic A: Handling incorrect, duplicated, and missing data\nTopic B: Feature engineering concepts\nTopic C: Numeric and text feature engineering\nTopic D: Feature selection techniques\nTopic E: AWS data transformation services\nSpeaker Notes:\n|Student notes\nIn the last section, you learned about the difference between numeric and categorical values. Numeric feature engineering involves creating a new feature by combining two or more numeric values. \nText data is an important component in many business problems, and not all text-based features can be easily encoded with categorical encoding. Text feature engineering involves transforming the text in your data so that your ML model can better understand the relationships within that data.\nSlide: 33\nNumeric feature engineering methods\nFeature scaling\nBinning\nLog transformation\nSpeaker Notes:\n|Student notes\nTypes of numeric feature engineering methods include feature scaling, binning, and log transformation.\nFeature scaling, also known as data scaling or normalization, aims to transform the numeric values so that all values are on the same scale. This method helps you to take large numbers and scale them down, so that the ML algorithm can achieve quicker computations and avoid skewed results.\nBinning is a technique to group numerical features into a smaller number of bins, or categories. The data is divided into these bins based on value ranges, thus transforming a numeric feature into a categorical one.\nLog transformation applies a logarithmic function to numeric features in a dataset. A logarithmic function is a mathematical function where the independent variable is the exponent of a fixed number, called the base.\nSlide: 34\nFeature scaling approaches\nNormalization\nRescale values to a common scale (\neg.\n 0 to 1)\nImpacted by outliers\nStandardization\nRescale values to a mean of 0 and a standard deviation of 1\nReduces negative effect of outliers\nSpeaker Notes:\n~Dev notes\n~Alt text: \nGraph with points ranging between 0 and 1\n~Alt text: \nGraph with points centered around 0\n|Student notes\nA lot of ML algorithms are sensitive to wide ranges of data, so feature scaling is required for many ML models. However, some ML models like decision trees are not skewed by wide ranges of numeric data. Whether or not to scale your data will depend on the ML algorithm that you decide to use.\nNormalization\n rescales the values of numeric features to a common scale, often between 0 and 1. This rescaling makes the values more comparable and prevents features with greater magnitudes from incorrectly influencing the model. Outliers can prevent a normalization approach from working effectively.\nStandardization\n is similar to normalization, but instead of scaling values from 0\u20131, it rescales the features to have a mean of 0 and standard deviation of 1. \nStandardization reduces the negative effect of outliers.\nSlide: 35\nFeature scaling example\nSpeaker Notes:\n|Student notes\nAs an example, a dataset considering housing data might contain features such as the number of bedrooms and the price of the house. Number of bedrooms might range from 1 to 5 as a numeric value, but the price might range from $100,000 to $1,000,000.\nNormalizing the house pricing feature with values from 100,000 to 1,000,000 to a 0\u20131 scale would transform the values to be between 0 and 1. This method assigns the smallest value to 0 and the highest value to 1, and uses a calculation to scale down the remaining values.\nStandardization would set the average home price at the integer of 0 and use a calculation to score the remaining values. The calculation smooths out the values, so outliers are not as much of a negative factor with standardization as they are with normalization.\nSlide: 36\nBinning\nFeature is transformed from numeric to categorical\nHouse prices\nBinned house prices\nSpeaker Notes:\n~Dev notes\n~Alt text from digital: \nTwo bar charts: one with bars for features; one with these features grouped into three bars, or bins.\n|Student notes\nBinning can make models more robust by reducing the effect of outliers and allowing nonlinear relationships between features and target values to be captured more easily. It is an effective strategy for numerical data when the exact difference in numbers is not important, but the general range is a factor.\nFor example, you might use binning to engineer the house pricing feature. You might not be concerned about distinguishing between data points of a house price of $150,000 and $151,000, but you might be interested in distinguishing between houses priced at $150,000 and $500,000. \nYou can use a method called quantile binning to evenly distribution values between bins. For example, if you had three bins for your housing data, the lowest 33.3% of data would distribute to one bin, the next 33.3% in the next bin, and highest 33.3% in the remaining bin. Quantile binning results in each bin containing approximately the same number of data points.\nSlide: 37\nLog transformation\nApply a logarithmic function to numeric features\nSpeaker Notes:\n~Dev notes\n~Alt text from digital: \nTwo bar graphs: The first shows bars with many outliers; the second uses log transformation to bring values closer.\n|Student notes\nThe most common logarithmic functions have bases of 10 or e, where e is approximately equal to 2.71828. Logarithmic functions are the inverse of exponential functions and are useful for modeling phenomena where growth decreases over time, like population growth or decay.\nLog transformation is used to normalize skewed numeric data, reduce the effect of outliers, and keep values closer to normal distribution. Log transforming can improve model performance when using numeric features that have highly skewed distributions or outliers.\nLet\u2019s say your housing dataset had extreme outliers due to containing multiple geographic locations. One geographic location has a rural lot listed for $10,000 and another location has a luxury home in the city listed for $10,000,000. Taking the log of the housing prices scales down the large outlier values.\nFor example, the log of $10,000 would be around 4 and the log of $10,000,000 would be around 7. Using this method, the outliers are brought much closer to the normal values in the remainder of the dataset.\nSlide: 38\nText feature engineering methods\nBag of words\nN-gram\nTemporal data\nSpeaker Notes:\n|Student notes\nYou can use feature engineering methods such as bag-of-words and N-gram to engineer your text-based features. Temporal or time series data is data involving time, like a series of dates. Temporal data can come in many formats, and is often a mix of numeric and text data.\nSlide: 39\nBag-of-words\n\u201cdistrict\u201d \u201chas\u201d \u201cgreat\u201d \u201cteachers\u201d \n\u201cand\u201d \u201cgreat\u201d \u201cfacilities\u201d \nCount number of times words appear\nSpeaker Notes:\n|Student notes\nThe bag-of-words model does not keep track of the sequence of words, but counts the number of words in each observation. Bag-of-words uses tokenization to create a statistical representation of the text.\nConsider an example with the housing dataset. Perhaps you want to determine whether the listed home is in a highly rated school district, and you want to include parents\u2019 school reviews as a part of your dataset. The bag-of-words model would take a portion of raw text, break down each piece of text, and tokenize the text to create a table or statistical representation of that review.\nSlide: 40\nN-gram\n\u201cdistrict has\u201d \u201chas great\u201d \u201cgreat teachers\u201d \n\u201cteachers and\u201d \u201cand great\u201d \u201cgreat facilities\u201d \nProduce groups of words of n size\nSpeaker Notes:\n|Student notes\nThe N-gram model builds off of bag-of-words by producing a group of words of n size.\nFor example, if n = 2, a sliding window creates each group of two words as a token pair and creates a statistical representation of these token pairs. This method is useful when we want to find a particular combination of words in the text.\nIn the housing data, you could use the n-gram model to represent how many times reviews include word combinations of \u201cgreat teachers\u201d or \u201cafter-school programs.\"\nSlide: 41\nTemporal data\nExtract features from dates and times\nSpeaker Notes:\n|Student notes\nFeature engineering is important for temporal data because it can help capture key trends, cycles, or patterns over time. Examples of temporal features include time of day, day of the week, and seasons. By carefully extracting and including them in your model, you can help the model to better understand and predict time-based relationships in the data. \nFor example, you might have a feature in your housing dataset that contains the dates of when houses were listed for sale. When you have your dates in a consistent format, you might use feature engineering to create different features that extract the temporal information into day, month, and year. Extracting this information might reveal that most homes are listed in the summer or that most homes are listed at the first of each month.\nSlide: 42\nQuestion 1\nA company is building a machine learning model to predict credit risk. One of the features in the dataset is \nLoanAmount\n, which is likely to contain a few outliers.\nWhich feature engineering technique is a good method for engineering this numeric feature before model training?\nResponse\nA\nTake the log transform of the values to help normalize distribution.\nB\nLeave the values unchanged.\nC\nTake the mean of all values.\nD\nUse one-hot encoding to sort the values.\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember that a log transform can adjust data with outliers to a more normal distribution.\nSlide: 43\nQuestion 1 answer\nA company is building a machine learning model to predict credit risk. One of the features in the dataset is \nLoanAmount\n, which is likely to contain a few outliers.\nWhich feature engineering technique is a good method for engineering this numeric feature before model training?\nResponse\nA correct\nTake the log transform of the values to help normalize distribution.\nB\nLeave the values unchanged.\nC\nTake the mean of all values.\nD\nUse one-hot encoding to sort the values.\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nA: Taking the log transform of numeric features like \nLoanAmount\n can help normalize the distribution and handle skewness. This makes the values more comparable for modeling.\nThe incorrect responses are:\nB: Leaving the values unchanged would not help to normalize the distribution. Taking the log transform of numeric features like \nLoanAmount\n can help normalize the distribution and handle skewness.\nC: Taking the mean of all values can be a helpful method for measuring data, but in this case, the outliers might skew that calculation. Taking the log transform of numeric features like \nLoanAmount\n can help normalize the distribution and handle skewness.\nD: Although one-hot encoding is a great tool for categorical values, this value is numeric. Taking the log transform of numeric features like \nLoanAmount\n can help normalize the distribution and handle skewness.\nSlide: 44\nQuestion 2\nYou work for a travel company and want to build a model to categorize customer reviews as positive or negative.\nWhich text feature engineering technique would be most useful for this classification task?\nResponse\nA\nBag-of-words\nB\nBinning\nC\nLog transformation\nD\nN-gram\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember that n-grams are a useful feature engineering technique for sentiment analysis.\nSlide: 45\nQuestion 2 answer\nYou work for a travel company and want to build a model to categorize customer reviews as positive or negative.\nWhich text feature engineering technique would be most useful for this classification task?\nResponse\nA\nBag-of-words\nB\nBinning\nC\nLog transformation\nD correct\nN-gram\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nD: N-grams are a good technique for text classification tasks like sentiment analysis, because they help capture important phrases and sequences of words, not just individual words. Bag-of-words loses this important context. For a travel review, sequences like \"not clean\" and \"very comfortable\" are important signals. N-grams can help the model understand these types of phrases.\nThe incorrect responses are:\nA: Although bag-of-words represents the occurrence of words in a document, it does not preserve the order and context of the words. Reviews with very different sentiments could have similar bag-of-words representations if they contain the same vocabulary.\nB: Binning is useful for converting continuous numerical variables into categorical bins, not for text feature engineering.\nC: Log transformations are useful for normalizing skewed numerical distributions, not text data.\nSlide: 46\nFeature selection techniques\nTopic A: Handling incorrect, duplicated, and missing data\nTopic B: Feature engineering concepts\nTopic C: Numeric and text feature engineering\nTopic D: Feature selection techniques\nTopic E: AWS data transformation services\nSpeaker Notes:\n|Student Notes\nYou\u2019ve learned about feature scaling and feature engineering to make the most of your dataset and help your model determine relationships in the data. You can also use the technique of feature selection to select the most useful features for your model to make efficient and accurate predictions. \nSlide: 47\nFeature selection\nReduces dimensions\nRemove or select features\nRemove\nSpeaker Notes:\n|Student notes\nFeature selection is the intuitive process of selecting the most relevant features to use in a machine learning model.\nEarlier, you learned about feature scaling. Feature scaling refers to methods that rescale the values of features to a common scale without removing any. Feature selection, on the other hand, reduces dimensions by removing features or selecting certain features so that your model can focus on the correct information. \nIn this example, the housing dataset includes features of the price, the location, the date listed, and the number of bedrooms. You deal primarily with houses in the same geographic location, and you are more interested in the relationship between the number of bedrooms and the price of the house. In that case, you might decide to remove the \u201clocation\u201d and \u201cdate listed\u201d features, so that your model can focus on the most relevant information to the problem.\nSlide: 48\nFeature selection by splitting and combining\nFeature splitting\nFeature combining\nTotal number of bedrooms\nTotal square footage\nSpeaker Notes:\n~Dev notes\n~Alt text:  graphic showing splitting number of bedrooms into small, medium and large features.\n~Alt text:  graphic showing combining individual room sizes into a total square footage feature.\n|Student notes\nIn addition to selecting certain relevant features, you can also use feature splitting and combining to provide the most relevant information to your ML model.\nFeature splitting breaks down features into multiple derived features, expanding the feature space. Feature combining aggregates multiple features into a new combined feature, reducing the feature space. Both techniques can help improve model performance by transforming the original feature set into a more optimal set of predictive features.\nFeature splitting \nFor example, in the housing dataset, you might decide to capture more specific information about bedrooms in each home. The number of bedrooms feature might be split into the number of small, medium, and large bedrooms.\nFeature combining\nOr you might have each room\u2019s square footage categorized in a separate feature. You could combine the room size features to create one complete feature that represents the home\u2019s square footage.\nSlide: 49\nPrincipal component analysis (PCA)\nReduce number of features\nTransform features into principal components \nRetain most of the original information\nRanks components by their variance\nSpeaker Notes:\n|Student notes\nPrincipal component analysis (PCA) is a statistical technique that you can use for dimensionality reduction. In other words, PCA is a method for reducing the number of features in a dataset.\nThe goal of PCA is to retain most of the variation in the original features but reduce the overall number of features to increase computation efficiency. PCA works by transforming the features into a new set of uncorrelated features called principal components. PCA ranks the new set of principal components by their variance, with the first components capturing most of the variance in the data.\nSlide: 50\nActivity: Understanding principal components\nSpeaker Notes:\n|Student notes\nSlide: 51\nActivity: Understanding principal components\nSpeaker Notes:\n|Student notes\nIn the example table, you could reduce five of the features to just two principal components and compare them. The first principal component focuses on house size, and the second component focuses on cost. \nPrincipal component 1: Size\nThe first component accounts for the physical attributes of the home that includes square footage, bedrooms, and bathrooms.\nPrincipal component 2: Cost\nThe second component captures the financial factors that include price and tax rate.\nSlide: 52\nQuestion 1\nA baseball analyst wants to predict a player's batting average using a machine learning model. They have a dataset with over thirty different hitting statistics for each player. \nThe analyst wants to retain necessary information but reduce the number of features before training the model. Which feature selection technique should they use?\nResponse\nA\nFeature splitting\nB\nFeature scaling\nC\nPrincipal component analysis (PCA)\nD\nNo need for feature selection \nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember that PCA reduces features while retaining information. . \nSlide: 53\nQuestion 1 answer\nA baseball analyst wants to predict a player's batting average using a machine learning model. They have a dataset with over thirty different hitting statistics for each player. \nThe analyst wants to retain necessary information but reduce the number of features before training the model. Which feature selection technique should they use?\nResponse\nA\nFeature splitting\nB\nFeature scaling\nC correct\nPrincipal component analysis (PCA)\nD\nNo need for feature selection \nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nC: PCA reduces the dimensionality of the data while retaining most of the variation. This makes it useful for reducing a large set of variables down to a smaller subset of features before modeling.\nThe incorrect responses are:\nA: Feature splitting divides features into multiple datasets, which does not reduce dimensionality.\nB: Feature scaling standardizes features but does not reduce dimensionality.\nD: This example is a good candidate for using PCA. PCA reduces the dimensionality of the data while retaining most of the variation. This makes it useful for reducing a large set of variables down to a smaller subset of features before modeling.\nSlide: 54\nAWS data transformation services\nTopic A: Handling incorrect, duplicated, and missing data\nTopic B: Feature engineering concepts\nTopic C: Numeric and text feature engineering\nTopic D: Feature selection techniques\nTopic E: AWS data transformation services\nSpeaker Notes:\n|Student notes\nData ingestion is the process of collecting data from diverse sources, such as databases, data lakes, and streaming sources. The goal of data ingestion is to consolidate your data to use for your machine learning solution. \nIn this section, you will learn how you can use AWS services, such as Amazon SageMaker Data Wrangler, to ingest your data. You will also learn how you can use Amazon SageMaker Feature Store to ingest data and to create and manage features. \nYou will learn about data transformation use cases of these AWS services. You also learn fundamental steps of how you can use these services to automate part or all of the data transformation process.\nSlide: 55\nIngesting and transforming data for ML on AWS\nAmazon SageMaker Data Wrangler\nAmazon SageMaker Feature Store\nAmazon EMR\nAdd EMR notes from 1.2.13\nDo not add content but leave note here for T&C to revisit this slide since digital content is streaming-specific.\nSpeaker Notes:\n|Student notes\nYou can use AWS services, such as Amazon SageMaker Data Wrangler, to ingest your data. You can also use Amazon SageMaker Feature Store to ingest data and to create and manage features.\nAWS services like SageMaker Data Wrangler and SageMaker Feature Store streamline complicated data ingestion pipelines. Data Wrangler provides a visual, no-code way to transform and clean data from diverse sources. SageMaker Feature Store manages and shares curated features for machine learning model development. Using these services together eliminates much of the manual coding needed for ingesting and wrangling data. \nAWS services like Amazon EMR can help you to transform, enrich, and load data at scale. \nSlide: 56\nBenefits of Amazon SageMaker Data Wrangler\nAmazon SageMaker Data Wrangler\nVisual interface\nDrag-and-drop environment\nBuilt-in algorithms\nIntegration with data sources\nReusable data flows\nSpeaker Notes:\n|Student notes\nData Wrangler streamlines data ingestion through a visual interface to profile, understand, clean, and transform data using built-in recipes. It connects to sources, transforms, cleans, and loads data. Then, it outputs structured, model-ready data to AWS storage without writing code. SageMaker Data Wrangler streamlines complex data preparation so you can focus on building and training ML models, rather than preparing data. \nData Wrangler is a visual, code-free tool for data preprocessing and feature engineering that integrates with AWS data sources. It streamlines machine learning workflows through reusable data flows, large dataset handling, and integration with SageMaker.\nSlide: 57\nUse cases for SageMaker Data Wrangler\nClean data\nFeature engineering\nFix formatting issues\nReduce data size\nAutomate transformations\nSpeaker Notes:\n|Student notes\nData Wrangler streamlines exploratory data analysis and data preprocessing stages for ML learning in a visual, scalable, and collaborative environment. The following are some of the common use cases:\nClean data\n: You can use SageMaker Data Wrangler to interactively explore, visualize, clean, validate, and transform data to prepare it for ML model training. You can use the built-in transforms and visual interface to clean, standardize, and preprocess raw data from various sources like Amazon S3, databases, and more. \nFeature engineering\n: You can use the built-in transform and Python code blocks to engineer new features from your data. For example, you can use SageMaker Data Wrangler to combine columns and apply formulas.\nFixing formatting issues\n: SageMaker Data Wrangler can handle issues like missing headers and encoding problems to get the data into a consistent format for downstream use.\nReducing data size: \nFor large datasets, you can use SageMaker Data Wrangler to select subsets of columns, filter rows, sample data, and convert to lower precision data types. This helps to reduce storage and memory needs.\nAutomating transformations\n: After you interactively build transformation steps in SageMaker Data Wrangler, you can save them as a data recipe or flow to automate preparation for future datasets.\nSlide: 58\nUsing Amazon SageMaker Data Wrangler for data ingestion and transformation\nInput\nOutput\nAmazon \nSageMaker\n Data Wrangler\nIdentify data sources\nCreate a \nnew data flow\nPreview transformations\nOutput results\nSelect built-in transforms to transform data\nSpeaker Notes:\n|Student notes\nIdentify data sources\nUpload your dataset to an S3 bucket. SageMaker Data Wrangler supports CSV, JSON, Parquet, and other common data formats.\nCreate a new data flow\nOpen the SageMaker console and navigate to Data Wrangler. Choose Create flow to start a new data flow. Select the dataset you uploaded to Amazon S3 as the input source. SageMaker Data Wrangler will infer the schema and show a sample of the data. \nSelect built-in transforms to transform data\nIn the Transform section, use the built-in transforms to transform your data. You can see a sample output as you build the transforms. Some common transformations include the following: \nChanging column data types \nFiltering or sorting rows \nFilling in missing values \nParsing columns, like dates \nJoining datasets \nAggregating data, like sums or averages\nPreview transformations\nAutomate conversations, question answering, and workflows based on research into training honest and responsible AI systems.\nOutput results\nGenerate unique, realistic, high-quality images, art, logos, and designs.\nSlide: 59\nBenefits of Amazon SageMaker Feature Store\nAmazon SageMaker Feature Store\nStreamline\nTrack\nConfigure\nRetain\nSpeaker Notes:\n|Student notes\nFeature Store is a managed repository for storing, sharing, and managing features used for ML models. Features can be reused by multiple teams and SageMaker Feature Store keeps them synchronized for both batch training and real-time inference. By centralizing features in a secured store, SageMaker Feature Store improves feature quality and model accuracy at scale across the ML lifecycle. Storing features saves time by eliminating redundant feature engineering efforts.\nFeature Store provides centralized management of feature engineering to streamline machine learning workflows. It provides configurable online and offline storage of feature groups with metadata to enable discovery, reuse, and automation from data ingestion through model training and prediction. \nFeature Store provides a centralized and automated way to engineer, monitor, govern, and reuse features to improve model training and performance across teams. The following are some of the common use cases:\nAutomated data preprocessing: \nSageMaker Feature Store can automatically preprocess raw data as it is ingested. This includes scaling, encoding, transforming, and cleaning data so it is ready for model training.\nCentralized feature repository: \nSageMaker Feature Store acts as a centralized repository for curated features that have been preprocessed and engineered. Features can be reused across teams and models.\nStandardized features\n: SageMaker Feature Store establishes a standardized source of truth for featured data that is consistent across the organization. This avoids issues with different teams using their own feature engineering code.\nManagement of feature pipelines\n: SageMaker Feature Store manages the end-to-end lifecycle of feature pipelines from data ingestion through preprocessing. This provides monitoring and governance of feature engineering.\nCaching for performance\n: Frequently used features are cached in SageMaker Feature Store for faster retrieval during training. This improves performance by avoiding repetitive preprocessing.\nSlide: 60\nBenefits of Amazon EMR\nAmazon EMR\n<list key benefits>\nAdd EMR notes from 1.2.13\nDo not add content but leave note here for T&C to revisit this slide since digital content is streaming-specific.\nSpeaker Notes:\n|Student notes\nSlide: 61\nData formatting for model training in Amazon SageMaker\nSplit data sets.\nTraining\nTesting\nValidation\nAugment or shuffle training data for increased diversity.\nFormat data set.\nCSV\nRecordIO-protobuf\nSpeaker Notes:\n|Student notes\nReducing bias in your dataset is one of the primary objectives in data preparation. With balanced data, the model can focus on meaningful patterns in the dataset rather than any inherent noise.\nA common strategy to achieve a balanced and accurate dataset is to divide the data into distinct subsets for training, testing, and validation. \nData augmentation works by creating new, realistic training examples that expand the model's understanding of the data distribution. Dataset augmentation involves artificially expanding the size and diversity of a dataset by applying a variety of transformations and modifications to existing data samples. These transformations introduce variations in the data while preserving the essential characteristics relevant to the learning task\nData shuffling involves rearranging the order of the data points in a dataset before they are used for training a machine learning model. Shuffling your data helps to verify that the training examples are presented to the model in a random order rather than in a fixed sequence.\nWhen using Amazon SageMaker for your ML workloads, you want to make sure you are providing data that adheres to an accepted format. The two most common formats supported by the SageMaker built-in algorithms are CSV formatting and \nRecordIO-protobuf\n formatting. \nFormatting can be a complex task. You focus more on adhering to certain formatting types during the model training and refining stage of the machine learning lifecycle. However, it is helpful to know the basic concepts and structure of these two types of formats.\nCSV is a widely used text-based data format for training machine learning models. CSV files store tabular data, where each row represents a data sample, and the columns represent the features of that sample. For SageMaker built-in algorithms, the label in the training dataset must be the first column on the left, and the features should be to the right. Many built-in algorithms in SageMaker, such as \nXGBoost\n, linear learner, and \nDeepAR\n, accept CSV formatted data as input for model training. \nRecordIO-protobuf\n is a binary data format used by SageMaker for efficient data storage and transfer. This format is particularly useful when you need to handle large datasets or perform automatic transformations on the data. The \nRecordIO-protobuf\n format consists of a series of records, where each record contains a binary-encoded \nprotobuf\n message. This format is commonly used for image data, where each record represents an image and its associated metadata.\nSlide: 62\nQuestion 1\nYou work for a hospitality company that operates hotels across the country. You want to build a machine learning model to predict customer loyalty using data about past customer stays and purchases. You are considering using SageMaker Feature Store to prepare the data for model training. \nHow could SageMaker Feature Store be an effective service option for this use case?\nResponse\nA\nSageMaker Feature Store is a marketplace where you can pay workers to complete discrete tasks like labeling data. This would verify that your customers\u2019 purchases are accurate.\nB\nYou can use SageMaker Feature Store to visualize and clean your data. Clean data is essential for machine learning accuracy.\nC\nSageMaker Feature Store automatically labels your data. This saves time labeling data for supervised learning.\nD\nYou can use SageMaker Feature Store to store normalized features that can be reused for multiple models. This saves time preparing data for each new model.\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember that Feature Store serves as a centralized repository for features.\nSlide: 63\nQuestion 1 answer\nYou work for a hospitality company that operates hotels across the country. You want to build a machine learning model to predict customer loyalty using data about past customer stays and purchases. You are considering using SageMaker Feature Store to prepare the data for model training. \nHow could SageMaker Feature Store be an effective service option for this use case?\nResponse\nA\nSageMaker Feature Store is a marketplace where you can pay workers to complete discrete tasks like labeling data. This would verify that your customers\u2019 purchases are accurate.\nB\nYou can use SageMaker Feature Store to visualize and clean your data. Clean data is essential for machine learning accuracy.\nC\nSageMaker Feature Store automatically labels your data. This saves time labeling data for supervised learning.\nD correct\nYou can use SageMaker Feature Store to store normalized features that can be reused for multiple models. This saves time preparing data for each new model.\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nD: You can use SageMaker Feature Store to save time by storing curated features that can be accessed to train new models.\nThe incorrect responses are:\nA: Amazon Mechanical Turk is a service for hiring workers to complete tasks like labeling data. Feature Store helps manage and share features, not label data. You can use SageMaker Feature Store to store curated features that can be accessed to train new models.\nB: Although clean data is essential for ML accuracy, SageMaker Data Wrangler is a service that assists with data visualization and transformation. You can use SageMaker Feature Store to save time by storing curated features that can be accessed to train new models.\nC: SageMaker Ground Truth is a separate service for labeling data. SageMaker Feature Store helps manage and share features, not label data. You can use SageMaker Feature Store to save time by storing curated features that can be accessed to train new models.\nSlide: 64\nQuestion 2\nA telecommunications company wants to use Amazon SageMaker to train a machine learning model on their customer data. They have the data stored in different formats.\nWhich data formats do most Amazon SageMaker algorithms support for training? \nResponse\nA\nCSV and \nRecordIO-protobuf\nB\nAll data formats\nC\nNo data formats\nD\nCustom data formats\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember that SageMaker supports CSV and \nRecordIO-protobuf\n formats for training data.\nSlide: 65\nQuestion 2 answer\nA telecommunications company wants to use Amazon SageMaker to train a machine learning model on their customer data. They have the data stored in different formats.\nWhich data formats do most Amazon SageMaker algorithms support for training? \nResponse\nA correct\nCSV and \nRecordIO-protobuf\nB\nAll data formats\nC\nNo data formats\nD\nCustom data formats\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nA: Many Amazon SageMaker algorithms support training with data in CSV and \nRecordIO-protobuf\n format.\nThe incorrect responses are:\nB: Many Amazon SageMaker algorithms support certain data formats.\nC: Many Amazon SageMaker algorithms support certain commonly used data formats. \nD: Many Amazon SageMaker algorithms support certain commonly used data formats. \nSlide: 66\nResources\nTo learn more, choose from the following resources:\nAmazon SageMaker AI \u2013CSV Format\n (developer guide)\nAmazon SageMaker AI \u2013RecordIO Format \n(developer guide)\nTime-Series Datasets Format and Missing Values Filling Methods\n (developer guide)\nCreate, Store, and Share Features with Amazon SageMaker Feature Store\n (developer guide)\nTransform Data\n (developer guide)\nSpeaker Notes:\nSlide: 67\nModule summary\nYou should now be able to:\nExplain the value of data cleaning and transformation.\nDescribe how to process incorrect or duplicated data.\nDescribe how to detect and treat outliers.\nDescribe how to process missing values.\nDescribe fundamental encoding techniques.\nIdentify feature engineering use cases.\nDescribe fundamental concepts, benefits, and techniques of feature engineering.\nDescribe fundamental feature selection techniques.\nDescribe AWS services for validating and labeling data.\nIdentify AWS tools and services for visualizing and transforming data.\nSpeaker Notes:\n|Student notes\nSlide: 68\nQuestions?\nThank you for attending\nthis session\nCorrections, feedback, or other questions? \nContact us at \nhttps://support.aws.amazon.com/#/contacts/aws-training\n. \nAll trademarks are the property of their owners.\nSpeaker Notes:\n|Instructor Notes\n|\n|Student Notes",
        "RESPONSE": "Here is a summary of the key points for slides 1-10 of the presentation on Data Transformation and Feature Engineering:\n\nSlide 1: Introduction to Module 3\n- Covers data transformation concepts and techniques like data cleaning, encoding, and feature engineering\n- Introduces AWS services like SageMaker Feature Store, Data Wrangler, and AWS Glue for data transformation\n\nSlide 2: Module Objectives \nKey objectives include:\n- Explaining value of data cleaning and transformation\n- Describing how to process incorrect/duplicated data and outliers\n- Describing encoding techniques and feature engineering concepts\n- Identifying AWS tools for data visualization and transformation\n\nSlide 3: Data Processing in ML Lifecycle\n- Data processing phase prepares data for model development\n- Involves data cleaning, encoding, and feature engineering \n- Aims to produce clean, encoded, engineered dataset for modeling\n\nSlide 4: Data Transformation Techniques\nThree key techniques:\n1. Data cleaning - handling errors, duplicates, outliers\n2. Categorical encoding - converting categories to numbers\n3. Feature engineering - creating new features\n\nSlide 5: Module Agenda\nCovers 5 main topics:\nA. Handling incorrect, duplicated, missing data\nB. Feature engineering concepts  \nC. Numeric and text feature engineering\nD. Feature selection techniques\nE. AWS data transformation services\n\nSlide 6: Handling Incorrect, Duplicated, Missing Data\n- First step in data preparation after collection/ingestion\n- Critical for proper model functioning\n\nSlide 7: Cleaning Data for ML Training\nThree key areas:\n1. Incorrect or duplicate data\n2. Data outliers  \n3. Incomplete or missing data\n\nSlide 8: Incorrect Data\n- Examples: Language differences, typos, inconsistent formats\n- Techniques: Inspection, filling correct values, fixing formats, excluding data\n\nSlide 9: Duplicated Data  \n- Examples: Copies from multiple sources, incorrect data entry\n- Technique: Deduplication to remove duplicate records\n\nSlide 10: Activity: Identifying Incorrect and Duplicated Data\n- Practical exercise to identify errors in a sample dataset\n- Looks for missing data, inconsistent formatting, misplaced information",
        "PARAMETERS": {
            "temp": 0.5,
            "topP": 1.0,
            "topK": 250,
            "maxT": 200000,
            "stopSeq": ""
        }
    },
    {
        "timestamp": "2025-05-08 12:50:43",
        "MODEL": "Claude 35S",
        "COMMAND": "This is a complex prompt. Take your time to read through the entire prompt and understand every writing guideline and format required. \n\nOnce you are sure of the requirements, proceed.\n\nDo not use Amazon Bedrock cached data.\n\n[Role]\nYou are an AWS Technical Curriculum Developer creating slide notes for a presentation.\nRead the text in the INPUT. This is a text version of a PowerPoint slide presentation.   \nAll text in the INPUT must be treated as content. Read and unsderstand the contents of every slide\n\nCarefully identify the individual slide and \"Slide: nn\" followed by the slide title on the next line.\nIn youre response, the format will be Slide nn: Slide title consistently for every slide.\n\n[variables]\nTarget_audience: Machine learning engineers\nInstructional_level: 200-300 \nSpecific_AWS_services: Amazon SageMaker AI\n/variables\n\n[Task]\nPerform this work ONLY for slides 1-10. \nWhen you complete the last slide in this range, stop processing.\n\nYour audience is {{Target_audience}} who are {{Instructional_level}} in {{Specific_AWS_services}}.\n\nGenerate concise student notes for each PowerPoint slide that:\n1. Captures the essential concepts (maximum 3-4 sentences)\n2. Do not reveal information from upcoming slides\n3. Provide context only for THIS slide's content\n4. Use clear, direct language without unnecessary elaboration\n5. Include only relevant examples that reinforce the current slide's concepts\n\nUse existing notes and slide content as your primary source of information.\nExpand on existing slide notes when required, but do not use verbose language.\nFor long Student Notes, edit to 500 words maximum for each slide's student notes.\n\n[Output]\nThe complete SPEAKERNOTES: for each slide explicitly following the format and example below.  Every slide must contain every section of the SPEAKERNOTES.\n\n[Guidelines]\n1.  Your response shall be the entire notes text for all the slides specified.\n2.  FOLLOW the FORMAT explicitly and consistently for every slide.\n3.  Write clearly and concisely, ensuring technical accuracy. \n4.  Reference https://docs.aws.amazon.com for {{Specific_AWS_services}} details. \n5.  Provide valuable insights beyond what's visible on the slide. \n6.  Include transitional statements when changing topics, especially when moving from one section to the next. \n7.  Write in second person only.  Only use \"we\" to describe Amazon or AWs.\n8.  Add *italics* markdown to introduce a concept to the student notes.\n9. For lists items use bullets unless describing a sequence of steps then use a numbered list.\n\nFor Image Description, create a brief, accurate description for the image on this slide that:\n1. Describes what the image actually shows (no assumptions)\n2. Explains how the image directly relates to the slide's key point\n3. Uses no more than 2 sentences\n4. Avoids explanations that should be in the main student notes\n5. Begins with \"This image shows...\" or \"This diagram illustrates...\"\n\nDO NOT make statements like \"This slide is about x\" or \"This slide does that\" or \"This slide introduces...\" or \"This slide demonstrates...\"\n\n[FORMAT]\nSlide nn: Slide Title [in exactly this format]\nSPEAKERNOTES:\n~Developer Notes:\n~[Reference materials and URLs - live, accessible, relevant links]\n~\n~Image Description:\n~[Detaied visual description of the slide content or images.]\n~\n|INSTRUCTOR NOTES:\n- |[Key topics to cover]\n- |[Key topics to cover]\n- |...\n|\n|STUDENT NOTES:\n[3-4 clear, concise, and complete paragraphs expanding on slide content.]\n[300 words maximum]\n\n[EXAMPLES]\nSlide 1: Introduction to AWS Generative AI and ML\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/whitepapers/latest/aws-overview/machine-learning.html\n~\n~Image Description:\n~A diagram showing the AWS ML stack, including SageMaker, Comprehend, and Rekognition\n~\n|INSTRUCTOR NOTES:\n- |Introduce AWS Generative AI and ML services\n- |Highlight key benefits and use cases\n- |Discuss integration with other AWS services\n|\n|STUDENT NOTES:\nAWS Generative AI and Machine Learning services provide powerful tools for creating intelligent applications. These services include Amazon SageMake AI for end-to-end ML workflows, Amazon Comprehend for natural language processing, and Amazon Rekognition for image and video analysis.\n\nGenerative AI models, such as those powering Amazon Bedrock, can create new content like text, images, and code based on input prompts. This technology opens up exciting possibilities for content creation, language translation, and creative assistance across various industries.\n\nAWS ML services integrate seamlessly with other AWS offerings, allowing for scalable, *cost-effective*, and *secure* AI/ML solutions. By leveraging the AWS cloud infrastructure, businesses can easily deploy and manage ML models, process large datasets, and incorporate AI capabilities into their applications without the need for extensive in-house expertise or infrastructure.\n\nSlide 10: Activity: Identifying Incorrect and Duplicated Data\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide presents a dataset for an activity on identifying incorrect and duplicated data.\n~\n|INSTRUCTOR NOTES:\n- |Guide students through the activity\n- |Encourage them to identify different types of data issues\n- |Discuss potential solutions for each issue identified\n|\n|STUDENT NOTES:\nIn this activity, you'll practice identifying incorrect and duplicated data in a sample dataset. As you review the data, look for:\n- Missing data\n- Inconsistent formatting\n- Misplaced information\n- Duplicate entries\n\nConsider how you would address each issue you identify. For example:\n- How would you handle missing values?\n- What steps would you take to standardize inconsistent formats?\n- How would you correct misplaced information?\n- What process would you use to remove duplicates?\n\nThis exercise will help you develop a critical eye for common data quality issues and start thinking about appropriate strategies for addressing them. Remember, the goal is to create a clean, consistent dataset that will lead to more accurate and reliable machine learning models.\n\n",
        "INPUT": "Slide: 1\nModule 3: Data Transformation and Feature Engineering\nMachine Learning Engineering on AWS\nSpeaker Notes:\n|Student notes\nAt this point, you are familiar with data collection, ingestion, and storage. You've covered data fundamentals such as data types and formats, and you've been introduced to core AWS storage services. You can choose the most effective AWS storage decision and data format based on your specific machine learning (ML) needs. \nIn this module, you\u2019ll learn about data transformation. This module covers various transformation concepts and techniques, such as data cleaning, encoding, and feature engineering. You\u2019ll discover how to use AWS services such as Amazon SageMaker Feature Store, Amazon SageMaker Data Wrangler, and AWS Glue to transform your data.\nSlide: 2\nModule objectives & outline\nOn completion, you will be able to:\nExplain the value of data cleaning and transformation.\nDescribe how to process incorrect or duplicated data.\nDescribe how to detect and treat outliers.\nDescribe how to process missing values.\nDescribe fundamental encoding techniques.\nIdentify feature engineering use cases.\nDescribe fundamental concepts, benefits, and techniques of feature engineering.\nDescribe fundamental feature selection techniques.\nDescribe AWS services for validating and labeling data.\nIdentify AWS tools and services for visualizing and transforming data.\nSpeaker Notes:\n|Student Notes\nSlide: 3\nData processing: Data transformation and feature engineering\nBusiness goals\nML problem framing\nData processing\nModel development\nDeployment\nMonitoring\nSpeaker Notes:\n|Student notes\nDuring this phase of the machine learning lifecycle, you\u2019re processing your data in preparation for model development. By this point in the data preparation process, you\u2019ve collected, ingested, extracted, and stored your data as appropriate for your business\u2019s ML needs. Now, you are ready to transform that data into a clean, encoded, engineered dataset. Data cleaning, categorical encoding, and feature engineering are important techniques you can use to help ensure a more accurate and model-ready dataset.\nSlide: 4\nTechniques for data transformation\nData cleaning\nCategorical encoding\nFeature engineering\nThe label encoding examples in the source are not good ones. For example, by encoding countries with numbers like 1, 2, 3 you are assigning some countries higher numerical weight which will impact the model. Countries should be one-hot encoded so that they have the same weight. A better example for label encoding would be small, medium, large sizes as 1,2,3  since that preserves a relative weighting.  \nSpeaker Notes:\n|Student notes\nCleaning, encoding, and engineering your dataset is essential to produce more accurate and efficient results from your machine learning model.\nData cleaning \nis an important first step before training an ML model. Raw data often contains errors such as missing values, duplicates, and outliers that need to be identified and handled to ensure data accuracy and quality. \nExample: A few customer IDs are duplicated in a dataset. \nSolution: Remove or consolidate duplicated customer IDs. \nCategorical encoding \nconverts categorical variables into numeric representations that ML algorithms can mathematically interpret. Common encoding techniques include one-hot encoding and label encoding. \nExample: Data includes a country feature with values like USA, Canada, and UK.\nSolution: Encode the countries as numbers, like 1, 2, and 3, so the ML algorithm can mathematically compare them.\nFeature engineering \ncreates new features or selects relevant features to help ML algorithms better understand the relationships in the data. Methods such as binning, bag-of-words, and N-gram extract key information from text data. \nExample: Train a model to predict whether a review is positive or negative.\nSolution: Use the bag-of-words method to convert text into counts of word occurrences and represent text data in a numeric form. \nSlide: 5\nAgenda\nTopic A: Handling incorrect, duplicated, and missing data\nTopic B: Feature engineering concepts\nTopic C: Numeric and text feature engineering\nTopic D: Feature selection techniques\nTopic E: AWS data transformation services\nSpeaker Notes:\nSlide: 6\nHandling incorrect, duplicated, and missing data\nTopic A: Handling incorrect, duplicated, and missing data\nTopic B: Feature engineering concepts\nTopic C: Numeric and text feature engineering\nTopic D: Feature selection techniques\nTopic E: AWS data transformation services\nSpeaker Notes:\n|Student notes\nOne of the first steps of preparing your dataset after collection, ingestion, and storage is cleaning up your data. This can be a time-consuming process, but without taking care of flaws in your data, your machine learning (ML) model will not function properly.\nSlide: 7\nCleaning data for ML training\nIncorrect or duplicate data\nData outliers\nIncomplete or missing data\nSpeaker Notes:\n|Student notes\nThe goal for data cleaning is to standardize your dataset. You want to make sure the data is in the language and format that your model expects. Part of this standardization involves checking your dataset for incorrect or duplicated data.\nAnother key method of cleaning up your data is to identify data outliers. Outliers are data points that differ significantly from the majority of the data. \nOutliers in your data can present experimental anomalies or even highlight novel discoveries, so they are not necessarily data points you always want to remove. However, they can make it harder for your model to make accurate predictions. Detecting and examining outliers is important to the data preparation process, because they might be indicating errors in data collection or skewing values.\nIn addition to inspecting your data for incorrect or duplicated information and data outliers, you also want to address any areas with incomplete or missing data. \nSome columns in your dataset might be missing data because of a data collection error. It is also possible that data was not collected on a particular feature until well into the data collection process.\nSlide: 8\nIncorrect data\nExamples\nLanguage differences\nTypos\nInconsistent formats\nTechniques\nInspection\nFilling correct values\nFix formatting\nExclude from dataset\nSpeaker Notes:\n|Student notes\nDatasets often contain incorrect or anomalous data points that need to be addressed before model training. Common issues include language differences, typos from data entry, values that don't conform to expected formats, and more. To fix incorrect data values, inspect your dataset for anomalies. You might need to fill in the correct values, convert invalid formats to valid formats, or exclude the incorrect entries from the dataset to prevent inaccurate model predictions.\nSlide: 9\nDuplicated data\nExamples\nCopies from multiple sources\nIncorrect data entry\nTechniques\nDeduplication\nSpeaker Notes:\n|Student notes\nDuplicated data refers to records in a dataset that contain the same information. Duplicated data can occur when data is gathered from multiple sources or is entered incorrectly. After your data is fully collected, it's important to inspect the data for duplicate records and then delete or consolidate the duplicate entries. When a dataset contains multiple copies of the same data points, it skews the distribution of the data and over-represents certain examples.\nThe process of automating data duplication removal is called \ndeduplication\n. Deduplication works by scanning datasets for duplicate information, retaining one copy of the information, and replacing the other instances with pointers that direct back to the stored copy. Deduplication can drastically increase storage capacity by keeping only unique information in your data repositories.\nSlide: 10\nActivity: Identifying incorrect and duplicated data\nSpeaker Notes:\n|Student notes\nThe research team is working with the following set of data. Which errors do you notice in this dataset? You might notice that there is missing data, inconsistent formatting, or misplaced information. Consider how to identify and fix incorrect and duplicated data.\nSlide: 11\nActivity: Identifying incorrect and duplicated data\nSpeaker Notes:\n|Student notes\nBaccalaur\u00e9at\n \nis incorrect because it is in a different language than the rest of the dataset. Although the data is placed correctly in the \nEducation\n column, the word \nbaccalaur\u00e9at\n is a French word for a high school diploma. The data would need to be translated to English so the dataset has a standard language to process.\nMasters/PhD\n is incorrect because it is placed in the wrong column. \nMasters/PhD\n is a valid answer for the Education column, but is incorrectly placed in the \nState\n column. This data would need to be moved to the Education column, and you would need to impute the missing data for the State column.\nThe last row is an example of duplicated data. You might recognize that the Participant ID of 123456 is the biggest marker for duplication. There is already a row of data that contains 123456 as the ID.  You need to remove this entire row of data to keep data weights balanced and prevent bias.\nSlide: 12\nDetecting outliers by using mean and median\nMean\nMedian\nMiddle value of dataset\nBetter metric when data is skewed\nConsider both natural and artificial outliers.\nAverage of all values\nUseful when data is symmetrical\nAge\nCount\nMedian\nMean\nSpeaker Notes:\n|Student notes\nThere are a number of different methods you can use to detect and measure the impact of outliers in your data. These methods include calculating mean and median and determining if the outlier is natural or artificial. \nThese measures describe the extent to which your data is clustered around some value or position. \nThe \nmean\n is the average of all the values in the dataset. Mean can be a useful method for understanding your data when the data is symmetrical. For example, a symmetrical set of data that contains ages of respondents might reflect that both the mean and median of the dataset is 50 years old. However, if your data contains outliers, the mean can be skewed one way toward those values. In that case, your mean would not serve as an accurate representation of where your values are truly centered.\nThe \nmedian\n is the value in the dataset that divides the values into two equal halves. To calculate the median, you arrange all the numerical values in ascending order and identify the middle value. If your data is skewed or contains outliers, the median tends to provide the better metric for understanding your data as it relates to central tendency. \nFor example, a dataset that contains income level might contain outliers. The mean might skew toward higher or lower values, while the median would provide a more accurate picture of the data's central tendency.\nOutliers can be caused by either natural or artificial factors. A natural outlier is not the result of an artificial error but is reflective of some truth in the data.\nNatural outliers \nare data points that are accurate representations of data, but are extreme variations of the central data points. For example, in a dataset that includes height measurements of individuals, an extremely tall individual would represent a natural outlier.\nArtificial outliers \nare anomalous data points in your dataset due to error or improper data collection. For example, a faulty sensor in a thermometer might produce a body temperature that is unrealistically high or low compared to expected body temperatures. \nSlide: 13\nAddressing incomplete or missing data\nExploration\nIdentify missing values\nDetermine why values are missing\nMissing at Random (MAR)\nMissing Completely at Random (MCAR)\nMissing Not at Random (MNAR)\nTechniques\nDrop rows with missing values\nImpute values\nSpeaker Notes:\n|Student notes\nUnfortunately, most ML algorithms cannot deal with missing values automatically. You need to use human intelligence to replace missing values with something meaningful and relevant to the problem.\nA few missing values might not be an issue. However, if you\u2019re missing many values in one column, you might find it difficult to interpret the relationship between that feature and the target. In other words, it can be hard to understand how a value in a row corresponds to what needs to be predicted by the model. There are some key steps you can take to address incomplete and missing values in your dataset.\nIdentify missing values \n\u2013There are certain Python libraries, such as Pandas, that you can use to check for missing values. You can also use AWS services to assist you in this process. For example, Amazon SageMaker Autopilot provides a number of filling methods to handle missing values, and Amazon SageMaker Data Wrangler is a full-service data preparation tool. You will learn about SageMaker Data Wrangler in more detail later in this course.\nDetermine why values are missing \n\u2013Before you can determine how to treat the missing values, it\u2019s important to investigate which mechanisms caused the missing values. The following are three common types of missing data:\nMissing at Random (MAR): The probability that a data point is missing depends only on the observed data, not the missing data. Example: In a dataset of student test scores, scores are missing for some students who were absent that day. Absence is related to performance. \nMissing Completely at Random (MCAR): The probability that a data point is missing does not depend on the observed or unobserved data. Example: In an employee survey, some people forgot to answer the question about their number of siblings. Their missing sibling data does not depend on any values.\nMissing Not at Random (MNAR): The probability that a data point is missing depends on the missing data itself. Example: In a financial audit, companies with accounting irregularities are less likely to provide complete records. The missing data depends on the sensitive information being withheld.\nDrop missing values \u2013 \nDepending on what is causing your missing values, you will decide to either drop the missing values or impute data into your dataset. One of the most straightforward ways to deal with missing values is to remove the rows of data with missing values. You can accomplish this by using a Pandas function. Dropping rows or columns will make the dataset non-missing. However, the risk of dropping rows and columns is significant. If you have hundreds of rows or columns of data, all of that missing data might cause bias in your model predictions. If you drop too much data, you might not have enough features to feed the model.\nImpute values \n\u2013 Missing values might be related to new features that haven\u2019t included your dataset yet. After you include more data, those missing values might be highly correlated with the new feature. In this case, you would deal with missing values by adding more new features to the dataset. If you determine the values are missing at random, data imputation, or inputting the data into your dataset, is most likely the best option. There are different ways to impute a missing value. One common way to impute missing values is to replace the value with the mean, median, or most frequent value. You would select the mean, median, or most frequent value for categorical variables. You would select the mean or median for numerical variables. Choosing the mean, median, or most frequent value depending on your business problem and data collection procedures. \nWhen there is a missing value, always consider whether those values are missing at random. If they are not, try to investigate what the mechanism is behind the missing value. You can often find more efficient ways to impute those missing values. Remember that a majority of ML algorithms can\u2019t take missing values. You will most likely need to either drop the values or impute data for the ML model to perform.\nSlide: 14\nQuestion 1\nA real estate company is working on a machine learning project where the goal is to predict housing prices. The team has a dataset that contains information on the number of bedrooms, the square footage, and the locations of different houses. Before building a machine learning model, the team wants to prepare and transform the data. \nWhich data preparation technique could convert values for the home's location (with values of urban, suburban, and rural) into numeric representations that can be fed into a machine learning algorithm? \nResponse\nA\nData cleaning\nB\nFeature engineering\nC\nCategorical encoding\nD\nDimensionality reduction\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember which data preparation technique converts string values into numeric representations. \nSlide: 15\nQuestion 1 answer\nA real estate company is working on a machine learning project where the goal is to predict housing prices. The team has a dataset that contains information on the number of bedrooms, the square footage, and the locations of different houses. Before building a machine learning model, the team wants to prepare and transform the data. \nWhich data preparation technique could convert values for the home's location (with values of urban, suburban, and rural) into numeric representations that can be fed into a machine learning algorithm? \nResponse\nA\nData cleaning\nB\nFeature engineering\nC correct\nCategorical encoding\nD\nDimensionality reduction\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nC: Categorical encoding can be used to convert values for the home's location into numeric representations. For example, you could have 0 for urban, 1 for suburban, and 2 for rural. This allows machine learning algorithms to mathematically interpret the data.\nThe incorrect responses are:\nA: Data cleaning focuses on handling issues like missing data and outliers.\nB: Feature engineering focuses on modifying or creating new features from the data, rather than encoding features.\nD: Dimensionality reduction techniques reduce the number of features but don't convert data types.\nSlide: 16\nQuestion 2\nA non-profit organization is gathering data to decide which type of fundraiser their patrons would like to attend. They decide to provide two options for patrons to share their preferences: an online survey and a hard-copy mailer. Some patrons plan to fill out both the online and hard-copy survey to make sure their preference is recorded.\nWhich data cleaning technique will BEST help the company retain unique data points for the most effective machine learning (ML) solution?\nResponse\nA\nReformatting the hard-copy mailer to contain only \"Yes/No\" answers\nB\nInspecting the data for incorrectly entered dates\nC\nIdentifying duplicate data from those who submitted both an online and hard-copy survey\nD\nChecking the datasets for mistyped patron names\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember that duplicate data must be removed from a training dataset to avoid model bias.\nSlide: 17\nQuestion 2 answer\nA non-profit organization is gathering data to decide which type of fundraiser their patrons would like to attend. They decide to provide two options for patrons to share their preferences: an online survey and a hard-copy mailer. Some patrons plan to fill out both the online and hard-copy survey to make sure their preference is recorded.\nWhich data cleaning technique will BEST help the company retain unique data points for the most effective machine learning (ML) solution?\nResponse\nA\nReformatting the hard-copy mailer to contain only \"Yes/No\" answers\nB\nInspecting the data for incorrectly entered dates\nC correct\nIdentifying duplicate data from those who submitted both an online and hard-copy survey\nD\nChecking the datasets for mistyped patron names\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nC: Because the company has data points from multiple formats, there is a likelihood of duplication in their dataset. It's important to identify duplication to prevent the ML solution from overrepresenting certain examples.\nThe incorrect responses are:\nA,B,D: These options would not address the probable duplication in the dataset. Because the company has data points from multiple formats, there is a likelihood of duplication in their dataset. It's important to identify duplication to prevent the ML solution from overrepresenting certain examples.\nSlide: 18\nQuestion 3\nYou are cleaning up a dataset that includes the body weights of patients at a medical clinic. A clinician tells you that they usually see adults, but children occasionally visit the clinic.\nWhich data measurement BEST provides the central tendencies of your data?\nResponse\nA\nYou would choose the mean, because the best measurement of data that might contain outliers is taking the average of all data entries.\nB\nYou would choose the median, because the best measurement of data that might contain outliers is sorting the data and dividing the dataset at the middle variable.\nC\nBecause the data is numerical, you can use any method of measurement to find the average weight of a patient.\nD\nThere is no measurement you can use to find central tendencies of data.\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember the best measurement to find the central tendencies when the data is skewed.\nSlide: 19\nQuestion 3 answer\nYou are cleaning up a dataset that includes the body weights of patients at a medical clinic. A clinician tells you that they usually see adults, but children occasionally visit the clinic.\nWhich data measurement BEST provides the central tendencies of your data?\nResponse\nA\nYou would choose the mean, because the best measurement of data that might contain outliers is taking the average of all data entries.\nB correct\nYou would choose the median, because the best measurement of data that might contain outliers is sorting the data and dividing the dataset at the middle variable.\nC\nBecause the data is numerical, you can use any method of measurement to find the average weight of a patient.\nD\nThere is no measurement you can use to find central tendencies of data.\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nB: Because the clinic mostly sees adults, a few entries of children's weights might skew the average patient weight. In this case, it is best to use median to better understand the central tendencies of your data.\nThe incorrect responses are:\nA,C: Because the clinic mostly sees adults, a few entries of children's weights might skew the average patient weight. The mean of all patient data would skew towards the outlier values.\nD: You can use measurements such as mean and median to find central tendencies of your data.\nSlide: 20\nQuestion 4\nAn apparel company is building a machine learning solution to determine which types of products are not selling well. They've collected data on how long products sit in inventory, but their dataset contains many missing values.\nWhich method is the most effective option for addressing missing values in the apparel company's dataset?\nResponse\nA\nInspecting the reason for the missed values, and dropping, leaving, or imputing data accordingly\nB\nDropping all the data that contains missing values\nC\nManually imputing every missing value\nD\nLetting their ML algorithm replace the missing values\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember the steps that should be followed when there is missing data. \nSlide: 21\nQuestion 4 answer\nAn apparel company is building a machine learning solution to determine which types of products are not selling well. They've collected data on how long products sit in inventory, but their dataset contains many missing values.\nWhich method is the most effective option for addressing missing values in the apparel company's dataset?\nResponse\nA correct\nInspecting the reason for the missed values, and dropping, leaving, or imputing data accordingly\nB\nDropping all the data that contains missing values\nC\nManually imputing every missing value\nD\nLetting their ML algorithm replace the missing values\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nA: It's important to understand why the data is missing. Then, the company can set mechanisms in place to drop rows or columns of data, leave empty values, or impute data as needed.\nThe incorrect responses are:\nB: It's important to understand why the data is missing. Then, the company can set mechanisms in place to drop rows or columns of data, leave empty values, or impute data as needed. Dropping all missing values could remove significant and relevant data.\nC: This method is time-intensive. It's important to understand why the data is missing. Then, the company can set mechanisms in place to drop rows or columns of data, leave empty values, or impute data as needed.\nD: ML algorithms have trouble with missing values. While you can use AWS services to help locate and replace missing values, ML models themselves are not the best option for replacing missing values.\nSlide: 22\nFeature engineering concepts\nTopic A: Handling incorrect, duplicated, and missing data\nTopic B: Feature engineering concepts\nTopic C: Numeric and text feature engineering\nTopic D: Feature selection techniques\nTopic E: AWS data transformation services\nSpeaker Notes:\n|Student notes\nDuring the data transformation process, it is likely that you will come across different data formats. Remember, machine learning algorithms prefer numbers, so data that is text-based needs to be converted to numbers if possible. Categorical encoding is the process of manipulating text-based variables into number-based variables.\nAfter your data has been cleaned up and you\u2019ve encoded your data as necessary for your model, you can fine-tune or create new features in your dataset through feature engineering. Feature engineering is a method for transforming raw data into more informative features that help models better capture underlying relationships in the data.\nSlide: 23\nFeature engineering requirements\nUnderstanding the raw data\nRecognizing the business problem\nSelecting the right ML algorithm\nSpeaker Notes:\n|Student notes\nFeature engineering is more of an art than a science. Often, feature engineering is not a set of rules, and your approach will be based on the specific business problem that you are trying to solve.\nFeature engineering is often a highly intuitive practice. Understanding the raw data, familiarity with your business problem, and intuition of how to bridge the data and the algorithm work in parallel to determine your feature engineering requirements. Humans are skilled at finding relationships between the problem and the input feature of the raw data, so lean into your intuition when employing feature engineering.\nSlide: 24\nFeature engineering use cases \nNumeric feature engineering\nWide range or high variance\nOutliers or skewed distribution\nTime-based numeric features\nText feature engineering\nCategorical text\nLarge chunks of text\nComplex relationships\nSpeaker Notes:\n~Dev notes\n~Alt text from digital: Graph with mostly concentrated plot points and six outliers.\n|Student notes\nDepending on your data\u2019s format, you will select different methods of feature engineering to transform your dataset. \nNumeric feature engineering \ninvolves transforming numeric values for the model and is often accomplished by grouping different numeric values together. Sometimes, ML algorithms have trouble handling large numbers or isolated numbers. Grouping numeric values helps the model to better understand the relationships of these values.\nNumeric feature engineering strategies such as feature scaling, binning, and log transformation can help the ML model better understand the relationships of the data\u2019s numeric values. When deciding whether your dataset could benefit from numeric feature engineering, look for these qualities:\nNumeric features that have a wide range of values or high variance.\nNumeric features with outliers or a skewed distribution are also good targets for techniques like log transforms or binning to reduce the influence of extreme values.\nNumeric features related to time, such as timestamps, are good candidates for extraction of more meaningful metrics like day of the week or hour of the day.\nText feature engineering \ninvolves transforming text for the model and is often accomplished by splitting the text into smaller pieces. When dealing with text data, or corpus data, you can use feature engineering to transform attributes within your data to make those attributes more useful for your ML model. The purpose of text feature engineering is to break your text down into more bite-sized pieces for your model to understand the weights and relationships of your raw data.\nText feature engineering strategies, such as bag-of-words and N-gram, can tokenize the value of text data for your ML model to infer the right balance of information from that data. Because text feature engineering improves the ability for your model to understand your data effectively, essentially every type of raw data that includes text can benefit from some type of text feature engineering.\nCategorical features are associated with a group, and can often be represented numerically.\nSlide: 25\nCategorical encoding\nBinary\nTwo options \nNominal\nMultiple categories\nOrdinal\nCategories where order of values matter\nSome ML algorithms do not require encoding.\nSpeaker Notes:\n|Student notes\nWhen it comes to your data, the term numerical refers to values expressed as a measurable number, and categorical refers to features that are associated with a group. For example, in a dataset that contains animal medical records, a cat\u2019s weight is considered numerical, and the breed of cat is considered categorical.\nThe following list provides different types of categorical values.\nBinary\n categorical values refer to values that are one of two options. For example, a column of data might be true or false, such as if someone attended an event.\nNominal\n, or multi-categorical, values are category values where order does not matter. A set of data that contains different geographic locations, such as states or cities, might be considered multi-categorical.\nOrdinal\n, or ordered, values are category values where the order does matter, like what size of drink you order at a coffee shop: small, medium, or large.\nNot all categorical variables need to be encoded. Depending on your use case, different ML algorithms might not require you to encode your variables. \nFor instance, a random forest model can handle categorical features directly. You would not need to encode values, such as teal, green, and blue, as numeric values.\nSlide: 26\nActivity: Assessing data for categorical encoding\nSpeaker Notes:\n|Student notes\nThe data gathered by the research scientist team includes various types of numerical and categorical data. \nSlide: 27\nActivity: Assessing data for categorical encoding\nSpeaker Notes:\n|Student notes\nThe \nage\n column contains numerical data. Because this data is already set as numeric values, there is no need for categorical encoding.\nThe \neducation\n column is categorical data. Depending on which type of ML algorithm you decide to use, you might need to encode this data before using it to train and inform your model.\nThe \nflu shot \ncolumn is binary categorical data because respondents answered Yes or No. Depending on the type of algorithm you decide to use, this data might need to be encoded for your model to read the data more easily. For example, you can encode Yes to 0 and No to 1.\nThe \noutbreak zone \ncolumn is also binary categorical data. In this case, the research scientists could have also offered more value options for this feature that would be categorized as ordered, or ordinal, data. For instance, the scientists could categorize outbreak by light, medium, or heavy outbreak, rather than a binary Yes or No outbreak. Either way, this column might need to be encoded for model training, depending on the algorithm you decide to use. \nSlide: 28\nQuestion 1\nAn automobile company wants to use a machine learning solution to determine how quickly their various sizes of vehicles reach their highest speeds. The vehicle sizes are compact, mid-size, and full-size. \nWhich type of categorical data BEST describes the feature of their dataset that lists the size of the vehicle?\nResponse\nA\nNumerical\nB\nNominal\nC\nOrdinal\nD\nBinary\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember the type of categorical values that are ranked in order. \nSlide: 29\nQuestion 1 answer\nAn automobile company wants to use a machine learning solution to determine how quickly their various sizes of vehicles reach their highest speeds. The vehicle sizes are compact, mid-size, and full-size. \nWhich type of categorical data BEST describes the feature of their dataset that lists the size of the vehicle?\nResponse\nA\nNumerical\nB\nNominal\nC\nOrdinal\nD\nBinary\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nC: Ordinal data is the correct type of categorical data here, because the vehicle sizes like compact, midsize, and full-size are categorical labels with a numeric order. Ordinal data describes data that is categorical, but ranked in a numeric order.\nThe incorrect responses are:\nA: Numerical data represents quantitative values. The vehicle sizes (compact, mid-size, full-size) are qualitative categories, not numerical values.\nB: This question is asking about the categorical data that would describe the vehicle sizes. The vehicle sizes, such as compact, midsize, and full-size, would be considered ordinal data because they are categorical values that have a specific size order.\nD: Binary data represents categories with only two possible values. The vehicle sizes have three possible categories: compact, mid-size, full-size.\nSlide: 30\nQuestion 2\nAn energy company wants to assess how often their customers have to replace their solar panels after their initial solar panel purchase. Their raw data includes the dates of first purchase, customer reviews, and dates of additional purchases.\nHow might this company use feature engineering to help the machine learning model understand the relationship between these features in their dataset?\nResponse\nA\nThis is not a good use case for feature engineering, because the data includes a text-based feature.\nB\nThe company can use numeric feature engineering to extract information from the purchase dates, and they can use text feature engineering to extract information from the customer reviews.\nC\nThe company can only use numeric feature engineering to extract information from this dataset.\nD\nThe company can only use text feature engineering to extract information from this dataset.\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember that machine learning models find patterns in historical data and use that to make predictions.\nSlide: 31\nQuestion 2 answer\nAn energy company wants to assess how often their customers have to replace their solar panels after their initial solar panel purchase. Their raw data includes the dates of first purchase, customer reviews, and dates of additional purchases.\nHow might this company use feature engineering to help the machine learning model understand the relationship between these features in their dataset?\nResponse\nA\nThis is not a good use case for feature engineering, because the data includes a text-based feature.\nB correct\nThe company can use numeric feature engineering to extract information from the purchase dates, and they can use text feature engineering to extract information from the customer reviews.\nC\nThe company can only use numeric feature engineering to extract information from this dataset.\nD\nThe company can only use text feature engineering to extract information from this dataset.\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nB: Extracting information from these different features might help to show the relationship between initial purchase, review sentiment, and any additional purchases.\nThe incorrect responses are:\nA: Both the numeric and text-based data in this dataset are good candidates for feature engineering.\nC: The company can use text feature engineering to extract meaningful information from customer reviews..\nD: The company can use numeric feature engineering to extract meaningful information from initial and additional purchase dates.\nSlide: 32\nNumeric and text feature engineering\nTopic A: Handling incorrect, duplicated, and missing data\nTopic B: Feature engineering concepts\nTopic C: Numeric and text feature engineering\nTopic D: Feature selection techniques\nTopic E: AWS data transformation services\nSpeaker Notes:\n|Student notes\nIn the last section, you learned about the difference between numeric and categorical values. Numeric feature engineering involves creating a new feature by combining two or more numeric values. \nText data is an important component in many business problems, and not all text-based features can be easily encoded with categorical encoding. Text feature engineering involves transforming the text in your data so that your ML model can better understand the relationships within that data.\nSlide: 33\nNumeric feature engineering methods\nFeature scaling\nBinning\nLog transformation\nSpeaker Notes:\n|Student notes\nTypes of numeric feature engineering methods include feature scaling, binning, and log transformation.\nFeature scaling, also known as data scaling or normalization, aims to transform the numeric values so that all values are on the same scale. This method helps you to take large numbers and scale them down, so that the ML algorithm can achieve quicker computations and avoid skewed results.\nBinning is a technique to group numerical features into a smaller number of bins, or categories. The data is divided into these bins based on value ranges, thus transforming a numeric feature into a categorical one.\nLog transformation applies a logarithmic function to numeric features in a dataset. A logarithmic function is a mathematical function where the independent variable is the exponent of a fixed number, called the base.\nSlide: 34\nFeature scaling approaches\nNormalization\nRescale values to a common scale (\neg.\n 0 to 1)\nImpacted by outliers\nStandardization\nRescale values to a mean of 0 and a standard deviation of 1\nReduces negative effect of outliers\nSpeaker Notes:\n~Dev notes\n~Alt text: \nGraph with points ranging between 0 and 1\n~Alt text: \nGraph with points centered around 0\n|Student notes\nA lot of ML algorithms are sensitive to wide ranges of data, so feature scaling is required for many ML models. However, some ML models like decision trees are not skewed by wide ranges of numeric data. Whether or not to scale your data will depend on the ML algorithm that you decide to use.\nNormalization\n rescales the values of numeric features to a common scale, often between 0 and 1. This rescaling makes the values more comparable and prevents features with greater magnitudes from incorrectly influencing the model. Outliers can prevent a normalization approach from working effectively.\nStandardization\n is similar to normalization, but instead of scaling values from 0\u20131, it rescales the features to have a mean of 0 and standard deviation of 1. \nStandardization reduces the negative effect of outliers.\nSlide: 35\nFeature scaling example\nSpeaker Notes:\n|Student notes\nAs an example, a dataset considering housing data might contain features such as the number of bedrooms and the price of the house. Number of bedrooms might range from 1 to 5 as a numeric value, but the price might range from $100,000 to $1,000,000.\nNormalizing the house pricing feature with values from 100,000 to 1,000,000 to a 0\u20131 scale would transform the values to be between 0 and 1. This method assigns the smallest value to 0 and the highest value to 1, and uses a calculation to scale down the remaining values.\nStandardization would set the average home price at the integer of 0 and use a calculation to score the remaining values. The calculation smooths out the values, so outliers are not as much of a negative factor with standardization as they are with normalization.\nSlide: 36\nBinning\nFeature is transformed from numeric to categorical\nHouse prices\nBinned house prices\nSpeaker Notes:\n~Dev notes\n~Alt text from digital: \nTwo bar charts: one with bars for features; one with these features grouped into three bars, or bins.\n|Student notes\nBinning can make models more robust by reducing the effect of outliers and allowing nonlinear relationships between features and target values to be captured more easily. It is an effective strategy for numerical data when the exact difference in numbers is not important, but the general range is a factor.\nFor example, you might use binning to engineer the house pricing feature. You might not be concerned about distinguishing between data points of a house price of $150,000 and $151,000, but you might be interested in distinguishing between houses priced at $150,000 and $500,000. \nYou can use a method called quantile binning to evenly distribution values between bins. For example, if you had three bins for your housing data, the lowest 33.3% of data would distribute to one bin, the next 33.3% in the next bin, and highest 33.3% in the remaining bin. Quantile binning results in each bin containing approximately the same number of data points.\nSlide: 37\nLog transformation\nApply a logarithmic function to numeric features\nSpeaker Notes:\n~Dev notes\n~Alt text from digital: \nTwo bar graphs: The first shows bars with many outliers; the second uses log transformation to bring values closer.\n|Student notes\nThe most common logarithmic functions have bases of 10 or e, where e is approximately equal to 2.71828. Logarithmic functions are the inverse of exponential functions and are useful for modeling phenomena where growth decreases over time, like population growth or decay.\nLog transformation is used to normalize skewed numeric data, reduce the effect of outliers, and keep values closer to normal distribution. Log transforming can improve model performance when using numeric features that have highly skewed distributions or outliers.\nLet\u2019s say your housing dataset had extreme outliers due to containing multiple geographic locations. One geographic location has a rural lot listed for $10,000 and another location has a luxury home in the city listed for $10,000,000. Taking the log of the housing prices scales down the large outlier values.\nFor example, the log of $10,000 would be around 4 and the log of $10,000,000 would be around 7. Using this method, the outliers are brought much closer to the normal values in the remainder of the dataset.\nSlide: 38\nText feature engineering methods\nBag of words\nN-gram\nTemporal data\nSpeaker Notes:\n|Student notes\nYou can use feature engineering methods such as bag-of-words and N-gram to engineer your text-based features. Temporal or time series data is data involving time, like a series of dates. Temporal data can come in many formats, and is often a mix of numeric and text data.\nSlide: 39\nBag-of-words\n\u201cdistrict\u201d \u201chas\u201d \u201cgreat\u201d \u201cteachers\u201d \n\u201cand\u201d \u201cgreat\u201d \u201cfacilities\u201d \nCount number of times words appear\nSpeaker Notes:\n|Student notes\nThe bag-of-words model does not keep track of the sequence of words, but counts the number of words in each observation. Bag-of-words uses tokenization to create a statistical representation of the text.\nConsider an example with the housing dataset. Perhaps you want to determine whether the listed home is in a highly rated school district, and you want to include parents\u2019 school reviews as a part of your dataset. The bag-of-words model would take a portion of raw text, break down each piece of text, and tokenize the text to create a table or statistical representation of that review.\nSlide: 40\nN-gram\n\u201cdistrict has\u201d \u201chas great\u201d \u201cgreat teachers\u201d \n\u201cteachers and\u201d \u201cand great\u201d \u201cgreat facilities\u201d \nProduce groups of words of n size\nSpeaker Notes:\n|Student notes\nThe N-gram model builds off of bag-of-words by producing a group of words of n size.\nFor example, if n = 2, a sliding window creates each group of two words as a token pair and creates a statistical representation of these token pairs. This method is useful when we want to find a particular combination of words in the text.\nIn the housing data, you could use the n-gram model to represent how many times reviews include word combinations of \u201cgreat teachers\u201d or \u201cafter-school programs.\"\nSlide: 41\nTemporal data\nExtract features from dates and times\nSpeaker Notes:\n|Student notes\nFeature engineering is important for temporal data because it can help capture key trends, cycles, or patterns over time. Examples of temporal features include time of day, day of the week, and seasons. By carefully extracting and including them in your model, you can help the model to better understand and predict time-based relationships in the data. \nFor example, you might have a feature in your housing dataset that contains the dates of when houses were listed for sale. When you have your dates in a consistent format, you might use feature engineering to create different features that extract the temporal information into day, month, and year. Extracting this information might reveal that most homes are listed in the summer or that most homes are listed at the first of each month.\nSlide: 42\nQuestion 1\nA company is building a machine learning model to predict credit risk. One of the features in the dataset is \nLoanAmount\n, which is likely to contain a few outliers.\nWhich feature engineering technique is a good method for engineering this numeric feature before model training?\nResponse\nA\nTake the log transform of the values to help normalize distribution.\nB\nLeave the values unchanged.\nC\nTake the mean of all values.\nD\nUse one-hot encoding to sort the values.\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember that a log transform can adjust data with outliers to a more normal distribution.\nSlide: 43\nQuestion 1 answer\nA company is building a machine learning model to predict credit risk. One of the features in the dataset is \nLoanAmount\n, which is likely to contain a few outliers.\nWhich feature engineering technique is a good method for engineering this numeric feature before model training?\nResponse\nA correct\nTake the log transform of the values to help normalize distribution.\nB\nLeave the values unchanged.\nC\nTake the mean of all values.\nD\nUse one-hot encoding to sort the values.\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nA: Taking the log transform of numeric features like \nLoanAmount\n can help normalize the distribution and handle skewness. This makes the values more comparable for modeling.\nThe incorrect responses are:\nB: Leaving the values unchanged would not help to normalize the distribution. Taking the log transform of numeric features like \nLoanAmount\n can help normalize the distribution and handle skewness.\nC: Taking the mean of all values can be a helpful method for measuring data, but in this case, the outliers might skew that calculation. Taking the log transform of numeric features like \nLoanAmount\n can help normalize the distribution and handle skewness.\nD: Although one-hot encoding is a great tool for categorical values, this value is numeric. Taking the log transform of numeric features like \nLoanAmount\n can help normalize the distribution and handle skewness.\nSlide: 44\nQuestion 2\nYou work for a travel company and want to build a model to categorize customer reviews as positive or negative.\nWhich text feature engineering technique would be most useful for this classification task?\nResponse\nA\nBag-of-words\nB\nBinning\nC\nLog transformation\nD\nN-gram\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember that n-grams are a useful feature engineering technique for sentiment analysis.\nSlide: 45\nQuestion 2 answer\nYou work for a travel company and want to build a model to categorize customer reviews as positive or negative.\nWhich text feature engineering technique would be most useful for this classification task?\nResponse\nA\nBag-of-words\nB\nBinning\nC\nLog transformation\nD correct\nN-gram\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nD: N-grams are a good technique for text classification tasks like sentiment analysis, because they help capture important phrases and sequences of words, not just individual words. Bag-of-words loses this important context. For a travel review, sequences like \"not clean\" and \"very comfortable\" are important signals. N-grams can help the model understand these types of phrases.\nThe incorrect responses are:\nA: Although bag-of-words represents the occurrence of words in a document, it does not preserve the order and context of the words. Reviews with very different sentiments could have similar bag-of-words representations if they contain the same vocabulary.\nB: Binning is useful for converting continuous numerical variables into categorical bins, not for text feature engineering.\nC: Log transformations are useful for normalizing skewed numerical distributions, not text data.\nSlide: 46\nFeature selection techniques\nTopic A: Handling incorrect, duplicated, and missing data\nTopic B: Feature engineering concepts\nTopic C: Numeric and text feature engineering\nTopic D: Feature selection techniques\nTopic E: AWS data transformation services\nSpeaker Notes:\n|Student Notes\nYou\u2019ve learned about feature scaling and feature engineering to make the most of your dataset and help your model determine relationships in the data. You can also use the technique of feature selection to select the most useful features for your model to make efficient and accurate predictions. \nSlide: 47\nFeature selection\nReduces dimensions\nRemove or select features\nRemove\nSpeaker Notes:\n|Student notes\nFeature selection is the intuitive process of selecting the most relevant features to use in a machine learning model.\nEarlier, you learned about feature scaling. Feature scaling refers to methods that rescale the values of features to a common scale without removing any. Feature selection, on the other hand, reduces dimensions by removing features or selecting certain features so that your model can focus on the correct information. \nIn this example, the housing dataset includes features of the price, the location, the date listed, and the number of bedrooms. You deal primarily with houses in the same geographic location, and you are more interested in the relationship between the number of bedrooms and the price of the house. In that case, you might decide to remove the \u201clocation\u201d and \u201cdate listed\u201d features, so that your model can focus on the most relevant information to the problem.\nSlide: 48\nFeature selection by splitting and combining\nFeature splitting\nFeature combining\nTotal number of bedrooms\nTotal square footage\nSpeaker Notes:\n~Dev notes\n~Alt text:  graphic showing splitting number of bedrooms into small, medium and large features.\n~Alt text:  graphic showing combining individual room sizes into a total square footage feature.\n|Student notes\nIn addition to selecting certain relevant features, you can also use feature splitting and combining to provide the most relevant information to your ML model.\nFeature splitting breaks down features into multiple derived features, expanding the feature space. Feature combining aggregates multiple features into a new combined feature, reducing the feature space. Both techniques can help improve model performance by transforming the original feature set into a more optimal set of predictive features.\nFeature splitting \nFor example, in the housing dataset, you might decide to capture more specific information about bedrooms in each home. The number of bedrooms feature might be split into the number of small, medium, and large bedrooms.\nFeature combining\nOr you might have each room\u2019s square footage categorized in a separate feature. You could combine the room size features to create one complete feature that represents the home\u2019s square footage.\nSlide: 49\nPrincipal component analysis (PCA)\nReduce number of features\nTransform features into principal components \nRetain most of the original information\nRanks components by their variance\nSpeaker Notes:\n|Student notes\nPrincipal component analysis (PCA) is a statistical technique that you can use for dimensionality reduction. In other words, PCA is a method for reducing the number of features in a dataset.\nThe goal of PCA is to retain most of the variation in the original features but reduce the overall number of features to increase computation efficiency. PCA works by transforming the features into a new set of uncorrelated features called principal components. PCA ranks the new set of principal components by their variance, with the first components capturing most of the variance in the data.\nSlide: 50\nActivity: Understanding principal components\nSpeaker Notes:\n|Student notes\nSlide: 51\nActivity: Understanding principal components\nSpeaker Notes:\n|Student notes\nIn the example table, you could reduce five of the features to just two principal components and compare them. The first principal component focuses on house size, and the second component focuses on cost. \nPrincipal component 1: Size\nThe first component accounts for the physical attributes of the home that includes square footage, bedrooms, and bathrooms.\nPrincipal component 2: Cost\nThe second component captures the financial factors that include price and tax rate.\nSlide: 52\nQuestion 1\nA baseball analyst wants to predict a player's batting average using a machine learning model. They have a dataset with over thirty different hitting statistics for each player. \nThe analyst wants to retain necessary information but reduce the number of features before training the model. Which feature selection technique should they use?\nResponse\nA\nFeature splitting\nB\nFeature scaling\nC\nPrincipal component analysis (PCA)\nD\nNo need for feature selection \nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember that PCA reduces features while retaining information. . \nSlide: 53\nQuestion 1 answer\nA baseball analyst wants to predict a player's batting average using a machine learning model. They have a dataset with over thirty different hitting statistics for each player. \nThe analyst wants to retain necessary information but reduce the number of features before training the model. Which feature selection technique should they use?\nResponse\nA\nFeature splitting\nB\nFeature scaling\nC correct\nPrincipal component analysis (PCA)\nD\nNo need for feature selection \nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nC: PCA reduces the dimensionality of the data while retaining most of the variation. This makes it useful for reducing a large set of variables down to a smaller subset of features before modeling.\nThe incorrect responses are:\nA: Feature splitting divides features into multiple datasets, which does not reduce dimensionality.\nB: Feature scaling standardizes features but does not reduce dimensionality.\nD: This example is a good candidate for using PCA. PCA reduces the dimensionality of the data while retaining most of the variation. This makes it useful for reducing a large set of variables down to a smaller subset of features before modeling.\nSlide: 54\nAWS data transformation services\nTopic A: Handling incorrect, duplicated, and missing data\nTopic B: Feature engineering concepts\nTopic C: Numeric and text feature engineering\nTopic D: Feature selection techniques\nTopic E: AWS data transformation services\nSpeaker Notes:\n|Student notes\nData ingestion is the process of collecting data from diverse sources, such as databases, data lakes, and streaming sources. The goal of data ingestion is to consolidate your data to use for your machine learning solution. \nIn this section, you will learn how you can use AWS services, such as Amazon SageMaker Data Wrangler, to ingest your data. You will also learn how you can use Amazon SageMaker Feature Store to ingest data and to create and manage features. \nYou will learn about data transformation use cases of these AWS services. You also learn fundamental steps of how you can use these services to automate part or all of the data transformation process.\nSlide: 55\nIngesting and transforming data for ML on AWS\nAmazon SageMaker Data Wrangler\nAmazon SageMaker Feature Store\nAmazon EMR\nAdd EMR notes from 1.2.13\nDo not add content but leave note here for T&C to revisit this slide since digital content is streaming-specific.\nSpeaker Notes:\n|Student notes\nYou can use AWS services, such as Amazon SageMaker Data Wrangler, to ingest your data. You can also use Amazon SageMaker Feature Store to ingest data and to create and manage features.\nAWS services like SageMaker Data Wrangler and SageMaker Feature Store streamline complicated data ingestion pipelines. Data Wrangler provides a visual, no-code way to transform and clean data from diverse sources. SageMaker Feature Store manages and shares curated features for machine learning model development. Using these services together eliminates much of the manual coding needed for ingesting and wrangling data. \nAWS services like Amazon EMR can help you to transform, enrich, and load data at scale. \nSlide: 56\nBenefits of Amazon SageMaker Data Wrangler\nAmazon SageMaker Data Wrangler\nVisual interface\nDrag-and-drop environment\nBuilt-in algorithms\nIntegration with data sources\nReusable data flows\nSpeaker Notes:\n|Student notes\nData Wrangler streamlines data ingestion through a visual interface to profile, understand, clean, and transform data using built-in recipes. It connects to sources, transforms, cleans, and loads data. Then, it outputs structured, model-ready data to AWS storage without writing code. SageMaker Data Wrangler streamlines complex data preparation so you can focus on building and training ML models, rather than preparing data. \nData Wrangler is a visual, code-free tool for data preprocessing and feature engineering that integrates with AWS data sources. It streamlines machine learning workflows through reusable data flows, large dataset handling, and integration with SageMaker.\nSlide: 57\nUse cases for SageMaker Data Wrangler\nClean data\nFeature engineering\nFix formatting issues\nReduce data size\nAutomate transformations\nSpeaker Notes:\n|Student notes\nData Wrangler streamlines exploratory data analysis and data preprocessing stages for ML learning in a visual, scalable, and collaborative environment. The following are some of the common use cases:\nClean data\n: You can use SageMaker Data Wrangler to interactively explore, visualize, clean, validate, and transform data to prepare it for ML model training. You can use the built-in transforms and visual interface to clean, standardize, and preprocess raw data from various sources like Amazon S3, databases, and more. \nFeature engineering\n: You can use the built-in transform and Python code blocks to engineer new features from your data. For example, you can use SageMaker Data Wrangler to combine columns and apply formulas.\nFixing formatting issues\n: SageMaker Data Wrangler can handle issues like missing headers and encoding problems to get the data into a consistent format for downstream use.\nReducing data size: \nFor large datasets, you can use SageMaker Data Wrangler to select subsets of columns, filter rows, sample data, and convert to lower precision data types. This helps to reduce storage and memory needs.\nAutomating transformations\n: After you interactively build transformation steps in SageMaker Data Wrangler, you can save them as a data recipe or flow to automate preparation for future datasets.\nSlide: 58\nUsing Amazon SageMaker Data Wrangler for data ingestion and transformation\nInput\nOutput\nAmazon \nSageMaker\n Data Wrangler\nIdentify data sources\nCreate a \nnew data flow\nPreview transformations\nOutput results\nSelect built-in transforms to transform data\nSpeaker Notes:\n|Student notes\nIdentify data sources\nUpload your dataset to an S3 bucket. SageMaker Data Wrangler supports CSV, JSON, Parquet, and other common data formats.\nCreate a new data flow\nOpen the SageMaker console and navigate to Data Wrangler. Choose Create flow to start a new data flow. Select the dataset you uploaded to Amazon S3 as the input source. SageMaker Data Wrangler will infer the schema and show a sample of the data. \nSelect built-in transforms to transform data\nIn the Transform section, use the built-in transforms to transform your data. You can see a sample output as you build the transforms. Some common transformations include the following: \nChanging column data types \nFiltering or sorting rows \nFilling in missing values \nParsing columns, like dates \nJoining datasets \nAggregating data, like sums or averages\nPreview transformations\nAutomate conversations, question answering, and workflows based on research into training honest and responsible AI systems.\nOutput results\nGenerate unique, realistic, high-quality images, art, logos, and designs.\nSlide: 59\nBenefits of Amazon SageMaker Feature Store\nAmazon SageMaker Feature Store\nStreamline\nTrack\nConfigure\nRetain\nSpeaker Notes:\n|Student notes\nFeature Store is a managed repository for storing, sharing, and managing features used for ML models. Features can be reused by multiple teams and SageMaker Feature Store keeps them synchronized for both batch training and real-time inference. By centralizing features in a secured store, SageMaker Feature Store improves feature quality and model accuracy at scale across the ML lifecycle. Storing features saves time by eliminating redundant feature engineering efforts.\nFeature Store provides centralized management of feature engineering to streamline machine learning workflows. It provides configurable online and offline storage of feature groups with metadata to enable discovery, reuse, and automation from data ingestion through model training and prediction. \nFeature Store provides a centralized and automated way to engineer, monitor, govern, and reuse features to improve model training and performance across teams. The following are some of the common use cases:\nAutomated data preprocessing: \nSageMaker Feature Store can automatically preprocess raw data as it is ingested. This includes scaling, encoding, transforming, and cleaning data so it is ready for model training.\nCentralized feature repository: \nSageMaker Feature Store acts as a centralized repository for curated features that have been preprocessed and engineered. Features can be reused across teams and models.\nStandardized features\n: SageMaker Feature Store establishes a standardized source of truth for featured data that is consistent across the organization. This avoids issues with different teams using their own feature engineering code.\nManagement of feature pipelines\n: SageMaker Feature Store manages the end-to-end lifecycle of feature pipelines from data ingestion through preprocessing. This provides monitoring and governance of feature engineering.\nCaching for performance\n: Frequently used features are cached in SageMaker Feature Store for faster retrieval during training. This improves performance by avoiding repetitive preprocessing.\nSlide: 60\nBenefits of Amazon EMR\nAmazon EMR\n<list key benefits>\nAdd EMR notes from 1.2.13\nDo not add content but leave note here for T&C to revisit this slide since digital content is streaming-specific.\nSpeaker Notes:\n|Student notes\nSlide: 61\nData formatting for model training in Amazon SageMaker\nSplit data sets.\nTraining\nTesting\nValidation\nAugment or shuffle training data for increased diversity.\nFormat data set.\nCSV\nRecordIO-protobuf\nSpeaker Notes:\n|Student notes\nReducing bias in your dataset is one of the primary objectives in data preparation. With balanced data, the model can focus on meaningful patterns in the dataset rather than any inherent noise.\nA common strategy to achieve a balanced and accurate dataset is to divide the data into distinct subsets for training, testing, and validation. \nData augmentation works by creating new, realistic training examples that expand the model's understanding of the data distribution. Dataset augmentation involves artificially expanding the size and diversity of a dataset by applying a variety of transformations and modifications to existing data samples. These transformations introduce variations in the data while preserving the essential characteristics relevant to the learning task\nData shuffling involves rearranging the order of the data points in a dataset before they are used for training a machine learning model. Shuffling your data helps to verify that the training examples are presented to the model in a random order rather than in a fixed sequence.\nWhen using Amazon SageMaker for your ML workloads, you want to make sure you are providing data that adheres to an accepted format. The two most common formats supported by the SageMaker built-in algorithms are CSV formatting and \nRecordIO-protobuf\n formatting. \nFormatting can be a complex task. You focus more on adhering to certain formatting types during the model training and refining stage of the machine learning lifecycle. However, it is helpful to know the basic concepts and structure of these two types of formats.\nCSV is a widely used text-based data format for training machine learning models. CSV files store tabular data, where each row represents a data sample, and the columns represent the features of that sample. For SageMaker built-in algorithms, the label in the training dataset must be the first column on the left, and the features should be to the right. Many built-in algorithms in SageMaker, such as \nXGBoost\n, linear learner, and \nDeepAR\n, accept CSV formatted data as input for model training. \nRecordIO-protobuf\n is a binary data format used by SageMaker for efficient data storage and transfer. This format is particularly useful when you need to handle large datasets or perform automatic transformations on the data. The \nRecordIO-protobuf\n format consists of a series of records, where each record contains a binary-encoded \nprotobuf\n message. This format is commonly used for image data, where each record represents an image and its associated metadata.\nSlide: 62\nQuestion 1\nYou work for a hospitality company that operates hotels across the country. You want to build a machine learning model to predict customer loyalty using data about past customer stays and purchases. You are considering using SageMaker Feature Store to prepare the data for model training. \nHow could SageMaker Feature Store be an effective service option for this use case?\nResponse\nA\nSageMaker Feature Store is a marketplace where you can pay workers to complete discrete tasks like labeling data. This would verify that your customers\u2019 purchases are accurate.\nB\nYou can use SageMaker Feature Store to visualize and clean your data. Clean data is essential for machine learning accuracy.\nC\nSageMaker Feature Store automatically labels your data. This saves time labeling data for supervised learning.\nD\nYou can use SageMaker Feature Store to store normalized features that can be reused for multiple models. This saves time preparing data for each new model.\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember that Feature Store serves as a centralized repository for features.\nSlide: 63\nQuestion 1 answer\nYou work for a hospitality company that operates hotels across the country. You want to build a machine learning model to predict customer loyalty using data about past customer stays and purchases. You are considering using SageMaker Feature Store to prepare the data for model training. \nHow could SageMaker Feature Store be an effective service option for this use case?\nResponse\nA\nSageMaker Feature Store is a marketplace where you can pay workers to complete discrete tasks like labeling data. This would verify that your customers\u2019 purchases are accurate.\nB\nYou can use SageMaker Feature Store to visualize and clean your data. Clean data is essential for machine learning accuracy.\nC\nSageMaker Feature Store automatically labels your data. This saves time labeling data for supervised learning.\nD correct\nYou can use SageMaker Feature Store to store normalized features that can be reused for multiple models. This saves time preparing data for each new model.\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nD: You can use SageMaker Feature Store to save time by storing curated features that can be accessed to train new models.\nThe incorrect responses are:\nA: Amazon Mechanical Turk is a service for hiring workers to complete tasks like labeling data. Feature Store helps manage and share features, not label data. You can use SageMaker Feature Store to store curated features that can be accessed to train new models.\nB: Although clean data is essential for ML accuracy, SageMaker Data Wrangler is a service that assists with data visualization and transformation. You can use SageMaker Feature Store to save time by storing curated features that can be accessed to train new models.\nC: SageMaker Ground Truth is a separate service for labeling data. SageMaker Feature Store helps manage and share features, not label data. You can use SageMaker Feature Store to save time by storing curated features that can be accessed to train new models.\nSlide: 64\nQuestion 2\nA telecommunications company wants to use Amazon SageMaker to train a machine learning model on their customer data. They have the data stored in different formats.\nWhich data formats do most Amazon SageMaker algorithms support for training? \nResponse\nA\nCSV and \nRecordIO-protobuf\nB\nAll data formats\nC\nNo data formats\nD\nCustom data formats\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember that SageMaker supports CSV and \nRecordIO-protobuf\n formats for training data.\nSlide: 65\nQuestion 2 answer\nA telecommunications company wants to use Amazon SageMaker to train a machine learning model on their customer data. They have the data stored in different formats.\nWhich data formats do most Amazon SageMaker algorithms support for training? \nResponse\nA correct\nCSV and \nRecordIO-protobuf\nB\nAll data formats\nC\nNo data formats\nD\nCustom data formats\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nA: Many Amazon SageMaker algorithms support training with data in CSV and \nRecordIO-protobuf\n format.\nThe incorrect responses are:\nB: Many Amazon SageMaker algorithms support certain data formats.\nC: Many Amazon SageMaker algorithms support certain commonly used data formats. \nD: Many Amazon SageMaker algorithms support certain commonly used data formats. \nSlide: 66\nResources\nTo learn more, choose from the following resources:\nAmazon SageMaker AI \u2013CSV Format\n (developer guide)\nAmazon SageMaker AI \u2013RecordIO Format \n(developer guide)\nTime-Series Datasets Format and Missing Values Filling Methods\n (developer guide)\nCreate, Store, and Share Features with Amazon SageMaker Feature Store\n (developer guide)\nTransform Data\n (developer guide)\nSpeaker Notes:\nSlide: 67\nModule summary\nYou should now be able to:\nExplain the value of data cleaning and transformation.\nDescribe how to process incorrect or duplicated data.\nDescribe how to detect and treat outliers.\nDescribe how to process missing values.\nDescribe fundamental encoding techniques.\nIdentify feature engineering use cases.\nDescribe fundamental concepts, benefits, and techniques of feature engineering.\nDescribe fundamental feature selection techniques.\nDescribe AWS services for validating and labeling data.\nIdentify AWS tools and services for visualizing and transforming data.\nSpeaker Notes:\n|Student notes\nSlide: 68\nQuestions?\nThank you for attending\nthis session\nCorrections, feedback, or other questions? \nContact us at \nhttps://support.aws.amazon.com/#/contacts/aws-training\n. \nAll trademarks are the property of their owners.\nSpeaker Notes:\n|Instructor Notes\n|\n|Student Notes",
        "RESPONSE": "Here are the speaker notes for slides 1-10 as requested:\n\nSlide 1: Module 3: Data Transformation and Feature Engineering\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide shows the title \"Module 3: Data Transformation and Feature Engineering\" for a course on Machine Learning Engineering on AWS.\n~\n|INSTRUCTOR NOTES:\n- |Introduce the module on data transformation and feature engineering\n- |Highlight the importance of these steps in the ML pipeline\n- |Set expectations for what will be covered\n|\n|STUDENT NOTES:\nAt this point in the machine learning lifecycle, you have collected, ingested, and stored your data. You're familiar with data fundamentals like types and formats, and core AWS storage services. You can now choose effective AWS storage solutions and data formats based on your specific ML needs.\n\nThis module focuses on data transformation, covering various concepts and techniques such as data cleaning, encoding, and feature engineering. You'll learn how to use AWS services like Amazon SageMaker Feature Store, Amazon SageMaker Data Wrangler, and AWS Glue to transform your data and prepare it for model development.\n\nData transformation is a critical step that can significantly impact model performance. By properly cleaning, encoding, and engineering features, you can help your ML models extract more meaningful patterns from the data.\n\nSlide 2: Module objectives & outline\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide lists the learning objectives and outline for Module 3 on data transformation and feature engineering.\n~\n|INSTRUCTOR NOTES:\n- |Review the module objectives with students\n- |Explain the importance of each objective in the ML workflow\n- |Highlight how these skills apply to real-world ML projects\n|\n|STUDENT NOTES:\nBy the end of this module, you will be able to:\n\n\u2022 Explain the value of data cleaning and transformation in ML workflows\n\u2022 Describe methods for processing incorrect or duplicated data\n\u2022 Identify techniques for detecting and treating outliers\n\u2022 Outline approaches for handling missing values\n\u2022 Describe fundamental encoding techniques for categorical data\n\u2022 Identify common use cases for feature engineering\n\u2022 Explain key concepts, benefits, and techniques in feature engineering\n\u2022 Describe fundamental feature selection techniques\n\u2022 Identify AWS services for data validation and labeling\n\u2022 Recognize AWS tools and services for data visualization and transformation\n\nThese objectives cover essential skills for preparing data for machine learning models. Understanding these concepts will help you create more accurate and efficient ML solutions using AWS services.\n\nSlide 3: Data processing: Data transformation and feature engineering\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide shows a diagram of the machine learning lifecycle, highlighting the data processing stage which includes data transformation and feature engineering.\n~\n|INSTRUCTOR NOTES:\n- |Explain where data transformation fits in the ML lifecycle\n- |Highlight the importance of this stage for model accuracy\n- |Discuss how this stage builds on previous data preparation steps\n|\n|STUDENT NOTES:\nData transformation and feature engineering are crucial steps in the data processing phase of the machine learning lifecycle. At this point, you've already collected, ingested, extracted, and stored your data appropriately for your business's ML needs. \n\nNow, you're ready to transform that data into a clean, encoded, and engineered dataset. This stage involves several key techniques:\n\n\u2022 *Data cleaning* to handle errors, inconsistencies, and missing values\n\u2022 *Categorical encoding* to convert non-numeric data into a format suitable for ML algorithms\n\u2022 *Feature engineering* to create new features or modify existing ones to improve model performance\n\nThese techniques help ensure that your dataset is more accurate and model-ready. By properly transforming your data, you can significantly improve the performance and reliability of your ML models.\n\nSlide 4: Techniques for data transformation\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide outlines three main techniques for data transformation: data cleaning, categorical encoding, and feature engineering, with examples for each.\n~\n|INSTRUCTOR NOTES:\n- |Explain each transformation technique in detail\n- |Provide real-world examples for each technique\n- |Discuss the impact of these techniques on model performance\n|\n|STUDENT NOTES:\nData transformation involves three main techniques to prepare your dataset for machine learning:\n\n1. *Data cleaning* is the first crucial step before training an ML model. It involves identifying and handling errors such as missing values, duplicates, and outliers to ensure data accuracy and quality.\n   Example: Removing or consolidating duplicated customer IDs in a dataset.\n\n2. *Categorical encoding* converts categorical variables into numeric representations that ML algorithms can mathematically interpret. Common methods include one-hot encoding and label encoding.\n   Example: Encoding country names as numbers (e.g., USA=1, Canada=2, UK=3) for mathematical comparison.\n\n3. *Feature engineering* creates new features or selects relevant features to help ML algorithms better understand data relationships. Methods like binning, bag-of-words, and N-gram can extract key information from text data.\n   Example: Using bag-of-words to convert text reviews into word occurrence counts for sentiment analysis.\n\nThese techniques are essential for producing more accurate and efficient results from your machine learning models. They help transform raw data into a format that algorithms can effectively process and learn from.\n\nSlide 5: Agenda\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide presents the agenda for the module, listing five main topics to be covered.\n~\n|INSTRUCTOR NOTES:\n- |Provide an overview of the module structure\n- |Explain how each topic builds on the previous one\n- |Set expectations for the depth of coverage for each topic\n|\n|STUDENT NOTES:\nThis module covers five key topics in data transformation and feature engineering:\n\n1. Handling incorrect, duplicated, and missing data\n2. Feature engineering concepts\n3. Numeric and text feature engineering\n4. Feature selection techniques\n5. AWS data transformation services\n\nEach topic builds on the previous ones, providing you with a comprehensive understanding of data preparation for machine learning. We'll start with fundamental data cleaning techniques, progress through feature engineering concepts and methods, and conclude with an overview of AWS services that can assist in these processes.\n\nBy the end of this module, you'll have a solid foundation in data transformation techniques and be familiar with AWS tools that can streamline your ML workflows.\n\nSlide 6: Handling incorrect, duplicated, and missing data\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide introduces the first topic of the module: handling incorrect, duplicated, and missing data.\n~\n|INSTRUCTOR NOTES:\n- |Emphasize the importance of data quality in ML\n- |Introduce common data quality issues\n- |Preview the techniques that will be covered\n|\n|STUDENT NOTES:\nCleaning up your data is one of the first and most crucial steps in preparing your dataset for machine learning. This process, while potentially time-consuming, is essential for ensuring your ML model functions properly.\n\nIn this section, we'll focus on three key aspects of data cleaning:\n\n1. Identifying and correcting incorrect data\n2. Detecting and handling duplicated data\n3. Addressing missing or incomplete data\n\nThese steps are fundamental to creating a standardized, high-quality dataset that your model can effectively learn from. By addressing these issues, you're laying the groundwork for more accurate and reliable machine learning models.\n\nWe'll explore various techniques and best practices for each of these aspects, helping you develop a systematic approach to data cleaning in your ML projects.\n\nSlide 7: Cleaning data for ML training\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide outlines three main areas of focus for cleaning data: incorrect or duplicate data, data outliers, and incomplete or missing data.\n~\n|INSTRUCTOR NOTES:\n- |Explain the importance of each aspect of data cleaning\n- |Provide examples of how each issue can impact ML models\n- |Discuss strategies for identifying these issues in datasets\n|\n|STUDENT NOTES:\nWhen cleaning data for ML training, focus on three key areas:\n\n1. *Incorrect or duplicate data*: Standardize your dataset by checking for and correcting errors or inconsistencies. This ensures the data is in the format and language your model expects.\n\n2. *Data outliers*: Identify data points that significantly differ from the majority. While outliers can sometimes indicate novel discoveries, they can also make it harder for your model to make accurate predictions. Detecting and examining outliers is crucial to understand if they represent errors or important variations in your data.\n\n3. *Incomplete or missing data*: Address areas with missing information. This could be due to data collection errors or changes in the collection process over time. Handling missing data appropriately is essential for maintaining the integrity of your dataset.\n\nBy addressing these areas, you create a cleaner, more consistent dataset that allows your ML model to focus on meaningful patterns rather than noise or errors in the data. This process is fundamental to improving the accuracy and reliability of your machine learning models.\n\nSlide 8: Incorrect data\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide presents examples of incorrect data and techniques for addressing them.\n~\n|INSTRUCTOR NOTES:\n- |Discuss common sources of incorrect data\n- |Explain each technique for addressing incorrect data\n- |Provide real-world examples of applying these techniques\n|\n|STUDENT NOTES:\nIncorrect data in datasets can significantly impact model performance. Common issues include:\n\n\u2022 Language differences\n\u2022 Typos from data entry\n\u2022 Inconsistent formats\n\nTo address incorrect data, consider these techniques:\n\n1. *Inspection*: Carefully review your dataset to identify anomalies or inconsistencies.\n2. *Filling correct values*: Replace incorrect values with the correct information when possible.\n3. *Fix formatting*: Standardize data formats across the dataset (e.g., date formats, units of measurement).\n4. *Exclude from dataset*: If the data is irreparably incorrect and can't be fixed, consider removing it to prevent inaccurate model predictions.\n\nWhen applying these techniques, it's important to document your data cleaning process and decisions. This ensures transparency and allows for review or adjustment of your approach if needed. Remember, the goal is to create a clean, consistent dataset that will lead to more accurate and reliable machine learning models.\n\nSlide 9: Duplicated data\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide explains duplicated data, its sources, and techniques for handling it.\n~\n|INSTRUCTOR NOTES:\n- |Define duplicated data and its impact on ML models\n- |Discuss common sources of data duplication\n- |Explain deduplication techniques and their importance\n|\n|STUDENT NOTES:\nDuplicated data refers to records in a dataset that contain the same information. This can occur due to:\n\n\u2022 Copies from multiple sources\n\u2022 Incorrect data entry\n\nDuplicated data can skew the distribution of your dataset and over-represent certain examples, potentially leading to biased model predictions.\n\nTo address this issue, use *deduplication* techniques:\n\n1. Inspect the data for duplicate records\n2. Delete or consolidate duplicate entries\n3. Use automated deduplication processes for large datasets\n\n*Deduplication* works by scanning datasets for duplicate information, retaining one copy, and replacing other instances with pointers to the stored copy. This process can significantly increase storage capacity by keeping only unique information in your data repositories.\n\nWhen deduplicating, consider:\n\u2022 Which copy to keep (e.g., most recent, most complete)\n\u2022 How to handle near-duplicates or partial matches\n\u2022 The impact of deduplication on your analysis or model training\n\nEffective deduplication ensures your ML model trains on a more accurate representation of your data, improving its performance and reliability.\n\nSlide 10: Activity: Identifying incorrect and duplicated data\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide presents a dataset for an activity on identifying incorrect and duplicated data.\n~\n|INSTRUCTOR NOTES:\n- |Guide students through the activity\n- |Encourage them to identify different types of data issues\n- |Discuss potential solutions for each issue identified\n|\n|STUDENT NOTES:\nIn this activity, you'll practice identifying incorrect and duplicated data in a sample dataset. As you review the data, look for:\n\u2022 Missing data\n\u2022 Inconsistent formatting\n\u2022 Misplaced information\n\u2022 Duplicate entries\n\nConsider how you would address each issue you identify. For example:\n\u2022 How would you handle missing values?\n\u2022 What steps would you take to standardize inconsistent formats?\n\u2022 How would you correct misplaced information?\n\u2022 What process would you use to remove duplicates?\n\nThis exercise will help you develop a critical eye for common data quality issues and start thinking about appropriate strategies for addressing them. Remember, the goal is to create a clean, consistent dataset that will lead to more accurate and reliable machine learning models.",
        "PARAMETERS": {
            "temp": 0.5,
            "topP": 1.0,
            "topK": 250,
            "maxT": 200000,
            "stopSeq": ""
        }
    },
    {
        "timestamp": "2025-05-08 12:53:33",
        "MODEL": "Claude 35S",
        "COMMAND": "This is a complex prompt. Take your time to read through the entire prompt and understand every writing guideline and format required. \n\nOnce you are sure of the requirements, proceed.\n\nDo not use Amazon Bedrock cached data.\n\n[Role]\nYou are an AWS Technical Curriculum Developer creating slide notes for a presentation.\nRead the text in the INPUT. This is a text version of a PowerPoint slide presentation.   \nAll text in the INPUT must be treated as content. Read and unsderstand the contents of every slide\n\nCarefully identify the individual slide and \"Slide: nn\" followed by the slide title on the next line.\nIn youre response, the format will be Slide nn: Slide title consistently for every slide.\n\n[variables]\nTarget_audience: Machine learning engineers\nInstructional_level: 200-300 \nSpecific_AWS_services: Amazon SageMaker AI\n/variables\n\n[Task]\nPerform this work ONLY for slides 1-10. \nWhen you complete the last slide in this range, stop processing.\n\nYour audience is {{Target_audience}} who are {{Instructional_level}} in {{Specific_AWS_services}}.\n\nGenerate concise student notes for each PowerPoint slide that:\n1. Captures the essential concepts (maximum 3-4 sentences)\n2. Do not reveal information from upcoming slides\n3. Provide context only for THIS slide's content\n4. Use clear, direct language without unnecessary elaboration\n5. Include only relevant examples that reinforce the current slide's concepts\n\nUse existing notes and slide content as your primary source of information.\nExpand on existing slide notes when required, but do not use verbose language.\nFor long Student Notes, edit to 500 words maximum for each slide's student notes.\n\n[Output]\nThe complete SPEAKERNOTES: for each slide explicitly following the format and example below.  Every slide must contain every section of the SPEAKERNOTES.\n\n[Guidelines]\n1.  Your response shall be the entire notes text for all the slides specified.\n2.  FOLLOW the FORMAT explicitly and consistently for every slide.\n3.  Write clearly and concisely, ensuring technical accuracy. \n4.  Reference https://docs.aws.amazon.com for {{Specific_AWS_services}} details. \n5.  Provide valuable insights beyond what's visible on the slide. \n6.  Include transitional statements when changing topics, especially when moving from one section to the next. \n7.  Write in second person only.  Only use \"we\" to describe Amazon or AWs.\n8.  Add *italics* markdown to introduce a concept to the student notes.\n9. Please use bulleted lists as the default format for listing items. Only use numbered lists when describing a specific sequence or set of steps.\n\nFor Image Description, create a brief, accurate description for the image on this slide that:\n1. Describes what the image actually shows (no assumptions)\n2. Explains how the image directly relates to the slide's key point\n3. Uses no more than 2 sentences\n4. Avoids explanations that should be in the main student notes\n5. Begins with \"This image shows...\" or \"This diagram illustrates...\"\n\nDO NOT make statements like \"This slide is about x\" or \"This slide does that\" or \"This slide introduces...\" or \"This slide demonstrates...\"\n\n[FORMAT]\nSlide nn: Slide Title [in exactly this format]\nSPEAKERNOTES:\n~Developer Notes:\n~[Reference materials and URLs - live, accessible, relevant links]\n~\n~Image Description:\n~[Detaied visual description of the slide content or images.]\n~\n|INSTRUCTOR NOTES:\n- |[Key topics to cover]\n- |[Key topics to cover]\n- |...\n|\n|STUDENT NOTES:\n[3-4 clear, concise, and complete paragraphs expanding on slide content.]\n[300 words maximum]\n\n[EXAMPLES]\nSlide 1: Introduction to AWS Generative AI and ML\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/whitepapers/latest/aws-overview/machine-learning.html\n~\n~Image Description:\n~A diagram showing the AWS ML stack, including SageMaker, Comprehend, and Rekognition\n~\n|INSTRUCTOR NOTES:\n- |Introduce AWS Generative AI and ML services\n- |Highlight key benefits and use cases\n- |Discuss integration with other AWS services\n|\n|STUDENT NOTES:\nAWS Generative AI and Machine Learning services provide powerful tools for creating intelligent applications. These services include Amazon SageMake AI for end-to-end ML workflows, Amazon Comprehend for natural language processing, and Amazon Rekognition for image and video analysis.\n\nGenerative AI models, such as those powering Amazon Bedrock, can create new content like text, images, and code based on input prompts. This technology opens up exciting possibilities for content creation, language translation, and creative assistance across various industries.\n\nAWS ML services integrate seamlessly with other AWS offerings, allowing for scalable, *cost-effective*, and *secure* AI/ML solutions. By leveraging the AWS cloud infrastructure, businesses can easily deploy and manage ML models, process large datasets, and incorporate AI capabilities into their applications without the need for extensive in-house expertise or infrastructure.\n\nSlide 10: Activity: Identifying Incorrect and Duplicated Data\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide presents a dataset for an activity on identifying incorrect and duplicated data.\n~\n|INSTRUCTOR NOTES:\n- |Guide students through the activity\n- |Encourage them to identify different types of data issues\n- |Discuss potential solutions for each issue identified\n|\n|STUDENT NOTES:\nIn this activity, you'll practice identifying incorrect and duplicated data in a sample dataset. As you review the data, look for:\n- Missing data\n- Inconsistent formatting\n- Misplaced information\n- Duplicate entries\n\nConsider how you would address each issue you identify. For example:\n- How would you handle missing values?\n- What steps would you take to standardize inconsistent formats?\n- How would you correct misplaced information?\n- What process would you use to remove duplicates?\n\nThis exercise will help you develop a critical eye for common data quality issues and start thinking about appropriate strategies for addressing them. Remember, the goal is to create a clean, consistent dataset that will lead to more accurate and reliable machine learning models.\n\n",
        "INPUT": "Slide: 1\nModule 3: Data Transformation and Feature Engineering\nMachine Learning Engineering on AWS\nSpeaker Notes:\n|Student notes\nAt this point, you are familiar with data collection, ingestion, and storage. You've covered data fundamentals such as data types and formats, and you've been introduced to core AWS storage services. You can choose the most effective AWS storage decision and data format based on your specific machine learning (ML) needs. \nIn this module, you\u2019ll learn about data transformation. This module covers various transformation concepts and techniques, such as data cleaning, encoding, and feature engineering. You\u2019ll discover how to use AWS services such as Amazon SageMaker Feature Store, Amazon SageMaker Data Wrangler, and AWS Glue to transform your data.\nSlide: 2\nModule objectives & outline\nOn completion, you will be able to:\nExplain the value of data cleaning and transformation.\nDescribe how to process incorrect or duplicated data.\nDescribe how to detect and treat outliers.\nDescribe how to process missing values.\nDescribe fundamental encoding techniques.\nIdentify feature engineering use cases.\nDescribe fundamental concepts, benefits, and techniques of feature engineering.\nDescribe fundamental feature selection techniques.\nDescribe AWS services for validating and labeling data.\nIdentify AWS tools and services for visualizing and transforming data.\nSpeaker Notes:\n|Student Notes\nSlide: 3\nData processing: Data transformation and feature engineering\nBusiness goals\nML problem framing\nData processing\nModel development\nDeployment\nMonitoring\nSpeaker Notes:\n|Student notes\nDuring this phase of the machine learning lifecycle, you\u2019re processing your data in preparation for model development. By this point in the data preparation process, you\u2019ve collected, ingested, extracted, and stored your data as appropriate for your business\u2019s ML needs. Now, you are ready to transform that data into a clean, encoded, engineered dataset. Data cleaning, categorical encoding, and feature engineering are important techniques you can use to help ensure a more accurate and model-ready dataset.\nSlide: 4\nTechniques for data transformation\nData cleaning\nCategorical encoding\nFeature engineering\nThe label encoding examples in the source are not good ones. For example, by encoding countries with numbers like 1, 2, 3 you are assigning some countries higher numerical weight which will impact the model. Countries should be one-hot encoded so that they have the same weight. A better example for label encoding would be small, medium, large sizes as 1,2,3  since that preserves a relative weighting.  \nSpeaker Notes:\n|Student notes\nCleaning, encoding, and engineering your dataset is essential to produce more accurate and efficient results from your machine learning model.\nData cleaning \nis an important first step before training an ML model. Raw data often contains errors such as missing values, duplicates, and outliers that need to be identified and handled to ensure data accuracy and quality. \nExample: A few customer IDs are duplicated in a dataset. \nSolution: Remove or consolidate duplicated customer IDs. \nCategorical encoding \nconverts categorical variables into numeric representations that ML algorithms can mathematically interpret. Common encoding techniques include one-hot encoding and label encoding. \nExample: Data includes a country feature with values like USA, Canada, and UK.\nSolution: Encode the countries as numbers, like 1, 2, and 3, so the ML algorithm can mathematically compare them.\nFeature engineering \ncreates new features or selects relevant features to help ML algorithms better understand the relationships in the data. Methods such as binning, bag-of-words, and N-gram extract key information from text data. \nExample: Train a model to predict whether a review is positive or negative.\nSolution: Use the bag-of-words method to convert text into counts of word occurrences and represent text data in a numeric form. \nSlide: 5\nAgenda\nTopic A: Handling incorrect, duplicated, and missing data\nTopic B: Feature engineering concepts\nTopic C: Numeric and text feature engineering\nTopic D: Feature selection techniques\nTopic E: AWS data transformation services\nSpeaker Notes:\nSlide: 6\nHandling incorrect, duplicated, and missing data\nTopic A: Handling incorrect, duplicated, and missing data\nTopic B: Feature engineering concepts\nTopic C: Numeric and text feature engineering\nTopic D: Feature selection techniques\nTopic E: AWS data transformation services\nSpeaker Notes:\n|Student notes\nOne of the first steps of preparing your dataset after collection, ingestion, and storage is cleaning up your data. This can be a time-consuming process, but without taking care of flaws in your data, your machine learning (ML) model will not function properly.\nSlide: 7\nCleaning data for ML training\nIncorrect or duplicate data\nData outliers\nIncomplete or missing data\nSpeaker Notes:\n|Student notes\nThe goal for data cleaning is to standardize your dataset. You want to make sure the data is in the language and format that your model expects. Part of this standardization involves checking your dataset for incorrect or duplicated data.\nAnother key method of cleaning up your data is to identify data outliers. Outliers are data points that differ significantly from the majority of the data. \nOutliers in your data can present experimental anomalies or even highlight novel discoveries, so they are not necessarily data points you always want to remove. However, they can make it harder for your model to make accurate predictions. Detecting and examining outliers is important to the data preparation process, because they might be indicating errors in data collection or skewing values.\nIn addition to inspecting your data for incorrect or duplicated information and data outliers, you also want to address any areas with incomplete or missing data. \nSome columns in your dataset might be missing data because of a data collection error. It is also possible that data was not collected on a particular feature until well into the data collection process.\nSlide: 8\nIncorrect data\nExamples\nLanguage differences\nTypos\nInconsistent formats\nTechniques\nInspection\nFilling correct values\nFix formatting\nExclude from dataset\nSpeaker Notes:\n|Student notes\nDatasets often contain incorrect or anomalous data points that need to be addressed before model training. Common issues include language differences, typos from data entry, values that don't conform to expected formats, and more. To fix incorrect data values, inspect your dataset for anomalies. You might need to fill in the correct values, convert invalid formats to valid formats, or exclude the incorrect entries from the dataset to prevent inaccurate model predictions.\nSlide: 9\nDuplicated data\nExamples\nCopies from multiple sources\nIncorrect data entry\nTechniques\nDeduplication\nSpeaker Notes:\n|Student notes\nDuplicated data refers to records in a dataset that contain the same information. Duplicated data can occur when data is gathered from multiple sources or is entered incorrectly. After your data is fully collected, it's important to inspect the data for duplicate records and then delete or consolidate the duplicate entries. When a dataset contains multiple copies of the same data points, it skews the distribution of the data and over-represents certain examples.\nThe process of automating data duplication removal is called \ndeduplication\n. Deduplication works by scanning datasets for duplicate information, retaining one copy of the information, and replacing the other instances with pointers that direct back to the stored copy. Deduplication can drastically increase storage capacity by keeping only unique information in your data repositories.\nSlide: 10\nActivity: Identifying incorrect and duplicated data\nSpeaker Notes:\n|Student notes\nThe research team is working with the following set of data. Which errors do you notice in this dataset? You might notice that there is missing data, inconsistent formatting, or misplaced information. Consider how to identify and fix incorrect and duplicated data.\nSlide: 11\nActivity: Identifying incorrect and duplicated data\nSpeaker Notes:\n|Student notes\nBaccalaur\u00e9at\n \nis incorrect because it is in a different language than the rest of the dataset. Although the data is placed correctly in the \nEducation\n column, the word \nbaccalaur\u00e9at\n is a French word for a high school diploma. The data would need to be translated to English so the dataset has a standard language to process.\nMasters/PhD\n is incorrect because it is placed in the wrong column. \nMasters/PhD\n is a valid answer for the Education column, but is incorrectly placed in the \nState\n column. This data would need to be moved to the Education column, and you would need to impute the missing data for the State column.\nThe last row is an example of duplicated data. You might recognize that the Participant ID of 123456 is the biggest marker for duplication. There is already a row of data that contains 123456 as the ID.  You need to remove this entire row of data to keep data weights balanced and prevent bias.\nSlide: 12\nDetecting outliers by using mean and median\nMean\nMedian\nMiddle value of dataset\nBetter metric when data is skewed\nConsider both natural and artificial outliers.\nAverage of all values\nUseful when data is symmetrical\nAge\nCount\nMedian\nMean\nSpeaker Notes:\n|Student notes\nThere are a number of different methods you can use to detect and measure the impact of outliers in your data. These methods include calculating mean and median and determining if the outlier is natural or artificial. \nThese measures describe the extent to which your data is clustered around some value or position. \nThe \nmean\n is the average of all the values in the dataset. Mean can be a useful method for understanding your data when the data is symmetrical. For example, a symmetrical set of data that contains ages of respondents might reflect that both the mean and median of the dataset is 50 years old. However, if your data contains outliers, the mean can be skewed one way toward those values. In that case, your mean would not serve as an accurate representation of where your values are truly centered.\nThe \nmedian\n is the value in the dataset that divides the values into two equal halves. To calculate the median, you arrange all the numerical values in ascending order and identify the middle value. If your data is skewed or contains outliers, the median tends to provide the better metric for understanding your data as it relates to central tendency. \nFor example, a dataset that contains income level might contain outliers. The mean might skew toward higher or lower values, while the median would provide a more accurate picture of the data's central tendency.\nOutliers can be caused by either natural or artificial factors. A natural outlier is not the result of an artificial error but is reflective of some truth in the data.\nNatural outliers \nare data points that are accurate representations of data, but are extreme variations of the central data points. For example, in a dataset that includes height measurements of individuals, an extremely tall individual would represent a natural outlier.\nArtificial outliers \nare anomalous data points in your dataset due to error or improper data collection. For example, a faulty sensor in a thermometer might produce a body temperature that is unrealistically high or low compared to expected body temperatures. \nSlide: 13\nAddressing incomplete or missing data\nExploration\nIdentify missing values\nDetermine why values are missing\nMissing at Random (MAR)\nMissing Completely at Random (MCAR)\nMissing Not at Random (MNAR)\nTechniques\nDrop rows with missing values\nImpute values\nSpeaker Notes:\n|Student notes\nUnfortunately, most ML algorithms cannot deal with missing values automatically. You need to use human intelligence to replace missing values with something meaningful and relevant to the problem.\nA few missing values might not be an issue. However, if you\u2019re missing many values in one column, you might find it difficult to interpret the relationship between that feature and the target. In other words, it can be hard to understand how a value in a row corresponds to what needs to be predicted by the model. There are some key steps you can take to address incomplete and missing values in your dataset.\nIdentify missing values \n\u2013There are certain Python libraries, such as Pandas, that you can use to check for missing values. You can also use AWS services to assist you in this process. For example, Amazon SageMaker Autopilot provides a number of filling methods to handle missing values, and Amazon SageMaker Data Wrangler is a full-service data preparation tool. You will learn about SageMaker Data Wrangler in more detail later in this course.\nDetermine why values are missing \n\u2013Before you can determine how to treat the missing values, it\u2019s important to investigate which mechanisms caused the missing values. The following are three common types of missing data:\nMissing at Random (MAR): The probability that a data point is missing depends only on the observed data, not the missing data. Example: In a dataset of student test scores, scores are missing for some students who were absent that day. Absence is related to performance. \nMissing Completely at Random (MCAR): The probability that a data point is missing does not depend on the observed or unobserved data. Example: In an employee survey, some people forgot to answer the question about their number of siblings. Their missing sibling data does not depend on any values.\nMissing Not at Random (MNAR): The probability that a data point is missing depends on the missing data itself. Example: In a financial audit, companies with accounting irregularities are less likely to provide complete records. The missing data depends on the sensitive information being withheld.\nDrop missing values \u2013 \nDepending on what is causing your missing values, you will decide to either drop the missing values or impute data into your dataset. One of the most straightforward ways to deal with missing values is to remove the rows of data with missing values. You can accomplish this by using a Pandas function. Dropping rows or columns will make the dataset non-missing. However, the risk of dropping rows and columns is significant. If you have hundreds of rows or columns of data, all of that missing data might cause bias in your model predictions. If you drop too much data, you might not have enough features to feed the model.\nImpute values \n\u2013 Missing values might be related to new features that haven\u2019t included your dataset yet. After you include more data, those missing values might be highly correlated with the new feature. In this case, you would deal with missing values by adding more new features to the dataset. If you determine the values are missing at random, data imputation, or inputting the data into your dataset, is most likely the best option. There are different ways to impute a missing value. One common way to impute missing values is to replace the value with the mean, median, or most frequent value. You would select the mean, median, or most frequent value for categorical variables. You would select the mean or median for numerical variables. Choosing the mean, median, or most frequent value depending on your business problem and data collection procedures. \nWhen there is a missing value, always consider whether those values are missing at random. If they are not, try to investigate what the mechanism is behind the missing value. You can often find more efficient ways to impute those missing values. Remember that a majority of ML algorithms can\u2019t take missing values. You will most likely need to either drop the values or impute data for the ML model to perform.\nSlide: 14\nQuestion 1\nA real estate company is working on a machine learning project where the goal is to predict housing prices. The team has a dataset that contains information on the number of bedrooms, the square footage, and the locations of different houses. Before building a machine learning model, the team wants to prepare and transform the data. \nWhich data preparation technique could convert values for the home's location (with values of urban, suburban, and rural) into numeric representations that can be fed into a machine learning algorithm? \nResponse\nA\nData cleaning\nB\nFeature engineering\nC\nCategorical encoding\nD\nDimensionality reduction\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember which data preparation technique converts string values into numeric representations. \nSlide: 15\nQuestion 1 answer\nA real estate company is working on a machine learning project where the goal is to predict housing prices. The team has a dataset that contains information on the number of bedrooms, the square footage, and the locations of different houses. Before building a machine learning model, the team wants to prepare and transform the data. \nWhich data preparation technique could convert values for the home's location (with values of urban, suburban, and rural) into numeric representations that can be fed into a machine learning algorithm? \nResponse\nA\nData cleaning\nB\nFeature engineering\nC correct\nCategorical encoding\nD\nDimensionality reduction\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nC: Categorical encoding can be used to convert values for the home's location into numeric representations. For example, you could have 0 for urban, 1 for suburban, and 2 for rural. This allows machine learning algorithms to mathematically interpret the data.\nThe incorrect responses are:\nA: Data cleaning focuses on handling issues like missing data and outliers.\nB: Feature engineering focuses on modifying or creating new features from the data, rather than encoding features.\nD: Dimensionality reduction techniques reduce the number of features but don't convert data types.\nSlide: 16\nQuestion 2\nA non-profit organization is gathering data to decide which type of fundraiser their patrons would like to attend. They decide to provide two options for patrons to share their preferences: an online survey and a hard-copy mailer. Some patrons plan to fill out both the online and hard-copy survey to make sure their preference is recorded.\nWhich data cleaning technique will BEST help the company retain unique data points for the most effective machine learning (ML) solution?\nResponse\nA\nReformatting the hard-copy mailer to contain only \"Yes/No\" answers\nB\nInspecting the data for incorrectly entered dates\nC\nIdentifying duplicate data from those who submitted both an online and hard-copy survey\nD\nChecking the datasets for mistyped patron names\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember that duplicate data must be removed from a training dataset to avoid model bias.\nSlide: 17\nQuestion 2 answer\nA non-profit organization is gathering data to decide which type of fundraiser their patrons would like to attend. They decide to provide two options for patrons to share their preferences: an online survey and a hard-copy mailer. Some patrons plan to fill out both the online and hard-copy survey to make sure their preference is recorded.\nWhich data cleaning technique will BEST help the company retain unique data points for the most effective machine learning (ML) solution?\nResponse\nA\nReformatting the hard-copy mailer to contain only \"Yes/No\" answers\nB\nInspecting the data for incorrectly entered dates\nC correct\nIdentifying duplicate data from those who submitted both an online and hard-copy survey\nD\nChecking the datasets for mistyped patron names\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nC: Because the company has data points from multiple formats, there is a likelihood of duplication in their dataset. It's important to identify duplication to prevent the ML solution from overrepresenting certain examples.\nThe incorrect responses are:\nA,B,D: These options would not address the probable duplication in the dataset. Because the company has data points from multiple formats, there is a likelihood of duplication in their dataset. It's important to identify duplication to prevent the ML solution from overrepresenting certain examples.\nSlide: 18\nQuestion 3\nYou are cleaning up a dataset that includes the body weights of patients at a medical clinic. A clinician tells you that they usually see adults, but children occasionally visit the clinic.\nWhich data measurement BEST provides the central tendencies of your data?\nResponse\nA\nYou would choose the mean, because the best measurement of data that might contain outliers is taking the average of all data entries.\nB\nYou would choose the median, because the best measurement of data that might contain outliers is sorting the data and dividing the dataset at the middle variable.\nC\nBecause the data is numerical, you can use any method of measurement to find the average weight of a patient.\nD\nThere is no measurement you can use to find central tendencies of data.\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember the best measurement to find the central tendencies when the data is skewed.\nSlide: 19\nQuestion 3 answer\nYou are cleaning up a dataset that includes the body weights of patients at a medical clinic. A clinician tells you that they usually see adults, but children occasionally visit the clinic.\nWhich data measurement BEST provides the central tendencies of your data?\nResponse\nA\nYou would choose the mean, because the best measurement of data that might contain outliers is taking the average of all data entries.\nB correct\nYou would choose the median, because the best measurement of data that might contain outliers is sorting the data and dividing the dataset at the middle variable.\nC\nBecause the data is numerical, you can use any method of measurement to find the average weight of a patient.\nD\nThere is no measurement you can use to find central tendencies of data.\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nB: Because the clinic mostly sees adults, a few entries of children's weights might skew the average patient weight. In this case, it is best to use median to better understand the central tendencies of your data.\nThe incorrect responses are:\nA,C: Because the clinic mostly sees adults, a few entries of children's weights might skew the average patient weight. The mean of all patient data would skew towards the outlier values.\nD: You can use measurements such as mean and median to find central tendencies of your data.\nSlide: 20\nQuestion 4\nAn apparel company is building a machine learning solution to determine which types of products are not selling well. They've collected data on how long products sit in inventory, but their dataset contains many missing values.\nWhich method is the most effective option for addressing missing values in the apparel company's dataset?\nResponse\nA\nInspecting the reason for the missed values, and dropping, leaving, or imputing data accordingly\nB\nDropping all the data that contains missing values\nC\nManually imputing every missing value\nD\nLetting their ML algorithm replace the missing values\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember the steps that should be followed when there is missing data. \nSlide: 21\nQuestion 4 answer\nAn apparel company is building a machine learning solution to determine which types of products are not selling well. They've collected data on how long products sit in inventory, but their dataset contains many missing values.\nWhich method is the most effective option for addressing missing values in the apparel company's dataset?\nResponse\nA correct\nInspecting the reason for the missed values, and dropping, leaving, or imputing data accordingly\nB\nDropping all the data that contains missing values\nC\nManually imputing every missing value\nD\nLetting their ML algorithm replace the missing values\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nA: It's important to understand why the data is missing. Then, the company can set mechanisms in place to drop rows or columns of data, leave empty values, or impute data as needed.\nThe incorrect responses are:\nB: It's important to understand why the data is missing. Then, the company can set mechanisms in place to drop rows or columns of data, leave empty values, or impute data as needed. Dropping all missing values could remove significant and relevant data.\nC: This method is time-intensive. It's important to understand why the data is missing. Then, the company can set mechanisms in place to drop rows or columns of data, leave empty values, or impute data as needed.\nD: ML algorithms have trouble with missing values. While you can use AWS services to help locate and replace missing values, ML models themselves are not the best option for replacing missing values.\nSlide: 22\nFeature engineering concepts\nTopic A: Handling incorrect, duplicated, and missing data\nTopic B: Feature engineering concepts\nTopic C: Numeric and text feature engineering\nTopic D: Feature selection techniques\nTopic E: AWS data transformation services\nSpeaker Notes:\n|Student notes\nDuring the data transformation process, it is likely that you will come across different data formats. Remember, machine learning algorithms prefer numbers, so data that is text-based needs to be converted to numbers if possible. Categorical encoding is the process of manipulating text-based variables into number-based variables.\nAfter your data has been cleaned up and you\u2019ve encoded your data as necessary for your model, you can fine-tune or create new features in your dataset through feature engineering. Feature engineering is a method for transforming raw data into more informative features that help models better capture underlying relationships in the data.\nSlide: 23\nFeature engineering requirements\nUnderstanding the raw data\nRecognizing the business problem\nSelecting the right ML algorithm\nSpeaker Notes:\n|Student notes\nFeature engineering is more of an art than a science. Often, feature engineering is not a set of rules, and your approach will be based on the specific business problem that you are trying to solve.\nFeature engineering is often a highly intuitive practice. Understanding the raw data, familiarity with your business problem, and intuition of how to bridge the data and the algorithm work in parallel to determine your feature engineering requirements. Humans are skilled at finding relationships between the problem and the input feature of the raw data, so lean into your intuition when employing feature engineering.\nSlide: 24\nFeature engineering use cases \nNumeric feature engineering\nWide range or high variance\nOutliers or skewed distribution\nTime-based numeric features\nText feature engineering\nCategorical text\nLarge chunks of text\nComplex relationships\nSpeaker Notes:\n~Dev notes\n~Alt text from digital: Graph with mostly concentrated plot points and six outliers.\n|Student notes\nDepending on your data\u2019s format, you will select different methods of feature engineering to transform your dataset. \nNumeric feature engineering \ninvolves transforming numeric values for the model and is often accomplished by grouping different numeric values together. Sometimes, ML algorithms have trouble handling large numbers or isolated numbers. Grouping numeric values helps the model to better understand the relationships of these values.\nNumeric feature engineering strategies such as feature scaling, binning, and log transformation can help the ML model better understand the relationships of the data\u2019s numeric values. When deciding whether your dataset could benefit from numeric feature engineering, look for these qualities:\nNumeric features that have a wide range of values or high variance.\nNumeric features with outliers or a skewed distribution are also good targets for techniques like log transforms or binning to reduce the influence of extreme values.\nNumeric features related to time, such as timestamps, are good candidates for extraction of more meaningful metrics like day of the week or hour of the day.\nText feature engineering \ninvolves transforming text for the model and is often accomplished by splitting the text into smaller pieces. When dealing with text data, or corpus data, you can use feature engineering to transform attributes within your data to make those attributes more useful for your ML model. The purpose of text feature engineering is to break your text down into more bite-sized pieces for your model to understand the weights and relationships of your raw data.\nText feature engineering strategies, such as bag-of-words and N-gram, can tokenize the value of text data for your ML model to infer the right balance of information from that data. Because text feature engineering improves the ability for your model to understand your data effectively, essentially every type of raw data that includes text can benefit from some type of text feature engineering.\nCategorical features are associated with a group, and can often be represented numerically.\nSlide: 25\nCategorical encoding\nBinary\nTwo options \nNominal\nMultiple categories\nOrdinal\nCategories where order of values matter\nSome ML algorithms do not require encoding.\nSpeaker Notes:\n|Student notes\nWhen it comes to your data, the term numerical refers to values expressed as a measurable number, and categorical refers to features that are associated with a group. For example, in a dataset that contains animal medical records, a cat\u2019s weight is considered numerical, and the breed of cat is considered categorical.\nThe following list provides different types of categorical values.\nBinary\n categorical values refer to values that are one of two options. For example, a column of data might be true or false, such as if someone attended an event.\nNominal\n, or multi-categorical, values are category values where order does not matter. A set of data that contains different geographic locations, such as states or cities, might be considered multi-categorical.\nOrdinal\n, or ordered, values are category values where the order does matter, like what size of drink you order at a coffee shop: small, medium, or large.\nNot all categorical variables need to be encoded. Depending on your use case, different ML algorithms might not require you to encode your variables. \nFor instance, a random forest model can handle categorical features directly. You would not need to encode values, such as teal, green, and blue, as numeric values.\nSlide: 26\nActivity: Assessing data for categorical encoding\nSpeaker Notes:\n|Student notes\nThe data gathered by the research scientist team includes various types of numerical and categorical data. \nSlide: 27\nActivity: Assessing data for categorical encoding\nSpeaker Notes:\n|Student notes\nThe \nage\n column contains numerical data. Because this data is already set as numeric values, there is no need for categorical encoding.\nThe \neducation\n column is categorical data. Depending on which type of ML algorithm you decide to use, you might need to encode this data before using it to train and inform your model.\nThe \nflu shot \ncolumn is binary categorical data because respondents answered Yes or No. Depending on the type of algorithm you decide to use, this data might need to be encoded for your model to read the data more easily. For example, you can encode Yes to 0 and No to 1.\nThe \noutbreak zone \ncolumn is also binary categorical data. In this case, the research scientists could have also offered more value options for this feature that would be categorized as ordered, or ordinal, data. For instance, the scientists could categorize outbreak by light, medium, or heavy outbreak, rather than a binary Yes or No outbreak. Either way, this column might need to be encoded for model training, depending on the algorithm you decide to use. \nSlide: 28\nQuestion 1\nAn automobile company wants to use a machine learning solution to determine how quickly their various sizes of vehicles reach their highest speeds. The vehicle sizes are compact, mid-size, and full-size. \nWhich type of categorical data BEST describes the feature of their dataset that lists the size of the vehicle?\nResponse\nA\nNumerical\nB\nNominal\nC\nOrdinal\nD\nBinary\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember the type of categorical values that are ranked in order. \nSlide: 29\nQuestion 1 answer\nAn automobile company wants to use a machine learning solution to determine how quickly their various sizes of vehicles reach their highest speeds. The vehicle sizes are compact, mid-size, and full-size. \nWhich type of categorical data BEST describes the feature of their dataset that lists the size of the vehicle?\nResponse\nA\nNumerical\nB\nNominal\nC\nOrdinal\nD\nBinary\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nC: Ordinal data is the correct type of categorical data here, because the vehicle sizes like compact, midsize, and full-size are categorical labels with a numeric order. Ordinal data describes data that is categorical, but ranked in a numeric order.\nThe incorrect responses are:\nA: Numerical data represents quantitative values. The vehicle sizes (compact, mid-size, full-size) are qualitative categories, not numerical values.\nB: This question is asking about the categorical data that would describe the vehicle sizes. The vehicle sizes, such as compact, midsize, and full-size, would be considered ordinal data because they are categorical values that have a specific size order.\nD: Binary data represents categories with only two possible values. The vehicle sizes have three possible categories: compact, mid-size, full-size.\nSlide: 30\nQuestion 2\nAn energy company wants to assess how often their customers have to replace their solar panels after their initial solar panel purchase. Their raw data includes the dates of first purchase, customer reviews, and dates of additional purchases.\nHow might this company use feature engineering to help the machine learning model understand the relationship between these features in their dataset?\nResponse\nA\nThis is not a good use case for feature engineering, because the data includes a text-based feature.\nB\nThe company can use numeric feature engineering to extract information from the purchase dates, and they can use text feature engineering to extract information from the customer reviews.\nC\nThe company can only use numeric feature engineering to extract information from this dataset.\nD\nThe company can only use text feature engineering to extract information from this dataset.\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember that machine learning models find patterns in historical data and use that to make predictions.\nSlide: 31\nQuestion 2 answer\nAn energy company wants to assess how often their customers have to replace their solar panels after their initial solar panel purchase. Their raw data includes the dates of first purchase, customer reviews, and dates of additional purchases.\nHow might this company use feature engineering to help the machine learning model understand the relationship between these features in their dataset?\nResponse\nA\nThis is not a good use case for feature engineering, because the data includes a text-based feature.\nB correct\nThe company can use numeric feature engineering to extract information from the purchase dates, and they can use text feature engineering to extract information from the customer reviews.\nC\nThe company can only use numeric feature engineering to extract information from this dataset.\nD\nThe company can only use text feature engineering to extract information from this dataset.\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nB: Extracting information from these different features might help to show the relationship between initial purchase, review sentiment, and any additional purchases.\nThe incorrect responses are:\nA: Both the numeric and text-based data in this dataset are good candidates for feature engineering.\nC: The company can use text feature engineering to extract meaningful information from customer reviews..\nD: The company can use numeric feature engineering to extract meaningful information from initial and additional purchase dates.\nSlide: 32\nNumeric and text feature engineering\nTopic A: Handling incorrect, duplicated, and missing data\nTopic B: Feature engineering concepts\nTopic C: Numeric and text feature engineering\nTopic D: Feature selection techniques\nTopic E: AWS data transformation services\nSpeaker Notes:\n|Student notes\nIn the last section, you learned about the difference between numeric and categorical values. Numeric feature engineering involves creating a new feature by combining two or more numeric values. \nText data is an important component in many business problems, and not all text-based features can be easily encoded with categorical encoding. Text feature engineering involves transforming the text in your data so that your ML model can better understand the relationships within that data.\nSlide: 33\nNumeric feature engineering methods\nFeature scaling\nBinning\nLog transformation\nSpeaker Notes:\n|Student notes\nTypes of numeric feature engineering methods include feature scaling, binning, and log transformation.\nFeature scaling, also known as data scaling or normalization, aims to transform the numeric values so that all values are on the same scale. This method helps you to take large numbers and scale them down, so that the ML algorithm can achieve quicker computations and avoid skewed results.\nBinning is a technique to group numerical features into a smaller number of bins, or categories. The data is divided into these bins based on value ranges, thus transforming a numeric feature into a categorical one.\nLog transformation applies a logarithmic function to numeric features in a dataset. A logarithmic function is a mathematical function where the independent variable is the exponent of a fixed number, called the base.\nSlide: 34\nFeature scaling approaches\nNormalization\nRescale values to a common scale (\neg.\n 0 to 1)\nImpacted by outliers\nStandardization\nRescale values to a mean of 0 and a standard deviation of 1\nReduces negative effect of outliers\nSpeaker Notes:\n~Dev notes\n~Alt text: \nGraph with points ranging between 0 and 1\n~Alt text: \nGraph with points centered around 0\n|Student notes\nA lot of ML algorithms are sensitive to wide ranges of data, so feature scaling is required for many ML models. However, some ML models like decision trees are not skewed by wide ranges of numeric data. Whether or not to scale your data will depend on the ML algorithm that you decide to use.\nNormalization\n rescales the values of numeric features to a common scale, often between 0 and 1. This rescaling makes the values more comparable and prevents features with greater magnitudes from incorrectly influencing the model. Outliers can prevent a normalization approach from working effectively.\nStandardization\n is similar to normalization, but instead of scaling values from 0\u20131, it rescales the features to have a mean of 0 and standard deviation of 1. \nStandardization reduces the negative effect of outliers.\nSlide: 35\nFeature scaling example\nSpeaker Notes:\n|Student notes\nAs an example, a dataset considering housing data might contain features such as the number of bedrooms and the price of the house. Number of bedrooms might range from 1 to 5 as a numeric value, but the price might range from $100,000 to $1,000,000.\nNormalizing the house pricing feature with values from 100,000 to 1,000,000 to a 0\u20131 scale would transform the values to be between 0 and 1. This method assigns the smallest value to 0 and the highest value to 1, and uses a calculation to scale down the remaining values.\nStandardization would set the average home price at the integer of 0 and use a calculation to score the remaining values. The calculation smooths out the values, so outliers are not as much of a negative factor with standardization as they are with normalization.\nSlide: 36\nBinning\nFeature is transformed from numeric to categorical\nHouse prices\nBinned house prices\nSpeaker Notes:\n~Dev notes\n~Alt text from digital: \nTwo bar charts: one with bars for features; one with these features grouped into three bars, or bins.\n|Student notes\nBinning can make models more robust by reducing the effect of outliers and allowing nonlinear relationships between features and target values to be captured more easily. It is an effective strategy for numerical data when the exact difference in numbers is not important, but the general range is a factor.\nFor example, you might use binning to engineer the house pricing feature. You might not be concerned about distinguishing between data points of a house price of $150,000 and $151,000, but you might be interested in distinguishing between houses priced at $150,000 and $500,000. \nYou can use a method called quantile binning to evenly distribution values between bins. For example, if you had three bins for your housing data, the lowest 33.3% of data would distribute to one bin, the next 33.3% in the next bin, and highest 33.3% in the remaining bin. Quantile binning results in each bin containing approximately the same number of data points.\nSlide: 37\nLog transformation\nApply a logarithmic function to numeric features\nSpeaker Notes:\n~Dev notes\n~Alt text from digital: \nTwo bar graphs: The first shows bars with many outliers; the second uses log transformation to bring values closer.\n|Student notes\nThe most common logarithmic functions have bases of 10 or e, where e is approximately equal to 2.71828. Logarithmic functions are the inverse of exponential functions and are useful for modeling phenomena where growth decreases over time, like population growth or decay.\nLog transformation is used to normalize skewed numeric data, reduce the effect of outliers, and keep values closer to normal distribution. Log transforming can improve model performance when using numeric features that have highly skewed distributions or outliers.\nLet\u2019s say your housing dataset had extreme outliers due to containing multiple geographic locations. One geographic location has a rural lot listed for $10,000 and another location has a luxury home in the city listed for $10,000,000. Taking the log of the housing prices scales down the large outlier values.\nFor example, the log of $10,000 would be around 4 and the log of $10,000,000 would be around 7. Using this method, the outliers are brought much closer to the normal values in the remainder of the dataset.\nSlide: 38\nText feature engineering methods\nBag of words\nN-gram\nTemporal data\nSpeaker Notes:\n|Student notes\nYou can use feature engineering methods such as bag-of-words and N-gram to engineer your text-based features. Temporal or time series data is data involving time, like a series of dates. Temporal data can come in many formats, and is often a mix of numeric and text data.\nSlide: 39\nBag-of-words\n\u201cdistrict\u201d \u201chas\u201d \u201cgreat\u201d \u201cteachers\u201d \n\u201cand\u201d \u201cgreat\u201d \u201cfacilities\u201d \nCount number of times words appear\nSpeaker Notes:\n|Student notes\nThe bag-of-words model does not keep track of the sequence of words, but counts the number of words in each observation. Bag-of-words uses tokenization to create a statistical representation of the text.\nConsider an example with the housing dataset. Perhaps you want to determine whether the listed home is in a highly rated school district, and you want to include parents\u2019 school reviews as a part of your dataset. The bag-of-words model would take a portion of raw text, break down each piece of text, and tokenize the text to create a table or statistical representation of that review.\nSlide: 40\nN-gram\n\u201cdistrict has\u201d \u201chas great\u201d \u201cgreat teachers\u201d \n\u201cteachers and\u201d \u201cand great\u201d \u201cgreat facilities\u201d \nProduce groups of words of n size\nSpeaker Notes:\n|Student notes\nThe N-gram model builds off of bag-of-words by producing a group of words of n size.\nFor example, if n = 2, a sliding window creates each group of two words as a token pair and creates a statistical representation of these token pairs. This method is useful when we want to find a particular combination of words in the text.\nIn the housing data, you could use the n-gram model to represent how many times reviews include word combinations of \u201cgreat teachers\u201d or \u201cafter-school programs.\"\nSlide: 41\nTemporal data\nExtract features from dates and times\nSpeaker Notes:\n|Student notes\nFeature engineering is important for temporal data because it can help capture key trends, cycles, or patterns over time. Examples of temporal features include time of day, day of the week, and seasons. By carefully extracting and including them in your model, you can help the model to better understand and predict time-based relationships in the data. \nFor example, you might have a feature in your housing dataset that contains the dates of when houses were listed for sale. When you have your dates in a consistent format, you might use feature engineering to create different features that extract the temporal information into day, month, and year. Extracting this information might reveal that most homes are listed in the summer or that most homes are listed at the first of each month.\nSlide: 42\nQuestion 1\nA company is building a machine learning model to predict credit risk. One of the features in the dataset is \nLoanAmount\n, which is likely to contain a few outliers.\nWhich feature engineering technique is a good method for engineering this numeric feature before model training?\nResponse\nA\nTake the log transform of the values to help normalize distribution.\nB\nLeave the values unchanged.\nC\nTake the mean of all values.\nD\nUse one-hot encoding to sort the values.\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember that a log transform can adjust data with outliers to a more normal distribution.\nSlide: 43\nQuestion 1 answer\nA company is building a machine learning model to predict credit risk. One of the features in the dataset is \nLoanAmount\n, which is likely to contain a few outliers.\nWhich feature engineering technique is a good method for engineering this numeric feature before model training?\nResponse\nA correct\nTake the log transform of the values to help normalize distribution.\nB\nLeave the values unchanged.\nC\nTake the mean of all values.\nD\nUse one-hot encoding to sort the values.\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nA: Taking the log transform of numeric features like \nLoanAmount\n can help normalize the distribution and handle skewness. This makes the values more comparable for modeling.\nThe incorrect responses are:\nB: Leaving the values unchanged would not help to normalize the distribution. Taking the log transform of numeric features like \nLoanAmount\n can help normalize the distribution and handle skewness.\nC: Taking the mean of all values can be a helpful method for measuring data, but in this case, the outliers might skew that calculation. Taking the log transform of numeric features like \nLoanAmount\n can help normalize the distribution and handle skewness.\nD: Although one-hot encoding is a great tool for categorical values, this value is numeric. Taking the log transform of numeric features like \nLoanAmount\n can help normalize the distribution and handle skewness.\nSlide: 44\nQuestion 2\nYou work for a travel company and want to build a model to categorize customer reviews as positive or negative.\nWhich text feature engineering technique would be most useful for this classification task?\nResponse\nA\nBag-of-words\nB\nBinning\nC\nLog transformation\nD\nN-gram\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember that n-grams are a useful feature engineering technique for sentiment analysis.\nSlide: 45\nQuestion 2 answer\nYou work for a travel company and want to build a model to categorize customer reviews as positive or negative.\nWhich text feature engineering technique would be most useful for this classification task?\nResponse\nA\nBag-of-words\nB\nBinning\nC\nLog transformation\nD correct\nN-gram\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nD: N-grams are a good technique for text classification tasks like sentiment analysis, because they help capture important phrases and sequences of words, not just individual words. Bag-of-words loses this important context. For a travel review, sequences like \"not clean\" and \"very comfortable\" are important signals. N-grams can help the model understand these types of phrases.\nThe incorrect responses are:\nA: Although bag-of-words represents the occurrence of words in a document, it does not preserve the order and context of the words. Reviews with very different sentiments could have similar bag-of-words representations if they contain the same vocabulary.\nB: Binning is useful for converting continuous numerical variables into categorical bins, not for text feature engineering.\nC: Log transformations are useful for normalizing skewed numerical distributions, not text data.\nSlide: 46\nFeature selection techniques\nTopic A: Handling incorrect, duplicated, and missing data\nTopic B: Feature engineering concepts\nTopic C: Numeric and text feature engineering\nTopic D: Feature selection techniques\nTopic E: AWS data transformation services\nSpeaker Notes:\n|Student Notes\nYou\u2019ve learned about feature scaling and feature engineering to make the most of your dataset and help your model determine relationships in the data. You can also use the technique of feature selection to select the most useful features for your model to make efficient and accurate predictions. \nSlide: 47\nFeature selection\nReduces dimensions\nRemove or select features\nRemove\nSpeaker Notes:\n|Student notes\nFeature selection is the intuitive process of selecting the most relevant features to use in a machine learning model.\nEarlier, you learned about feature scaling. Feature scaling refers to methods that rescale the values of features to a common scale without removing any. Feature selection, on the other hand, reduces dimensions by removing features or selecting certain features so that your model can focus on the correct information. \nIn this example, the housing dataset includes features of the price, the location, the date listed, and the number of bedrooms. You deal primarily with houses in the same geographic location, and you are more interested in the relationship between the number of bedrooms and the price of the house. In that case, you might decide to remove the \u201clocation\u201d and \u201cdate listed\u201d features, so that your model can focus on the most relevant information to the problem.\nSlide: 48\nFeature selection by splitting and combining\nFeature splitting\nFeature combining\nTotal number of bedrooms\nTotal square footage\nSpeaker Notes:\n~Dev notes\n~Alt text:  graphic showing splitting number of bedrooms into small, medium and large features.\n~Alt text:  graphic showing combining individual room sizes into a total square footage feature.\n|Student notes\nIn addition to selecting certain relevant features, you can also use feature splitting and combining to provide the most relevant information to your ML model.\nFeature splitting breaks down features into multiple derived features, expanding the feature space. Feature combining aggregates multiple features into a new combined feature, reducing the feature space. Both techniques can help improve model performance by transforming the original feature set into a more optimal set of predictive features.\nFeature splitting \nFor example, in the housing dataset, you might decide to capture more specific information about bedrooms in each home. The number of bedrooms feature might be split into the number of small, medium, and large bedrooms.\nFeature combining\nOr you might have each room\u2019s square footage categorized in a separate feature. You could combine the room size features to create one complete feature that represents the home\u2019s square footage.\nSlide: 49\nPrincipal component analysis (PCA)\nReduce number of features\nTransform features into principal components \nRetain most of the original information\nRanks components by their variance\nSpeaker Notes:\n|Student notes\nPrincipal component analysis (PCA) is a statistical technique that you can use for dimensionality reduction. In other words, PCA is a method for reducing the number of features in a dataset.\nThe goal of PCA is to retain most of the variation in the original features but reduce the overall number of features to increase computation efficiency. PCA works by transforming the features into a new set of uncorrelated features called principal components. PCA ranks the new set of principal components by their variance, with the first components capturing most of the variance in the data.\nSlide: 50\nActivity: Understanding principal components\nSpeaker Notes:\n|Student notes\nSlide: 51\nActivity: Understanding principal components\nSpeaker Notes:\n|Student notes\nIn the example table, you could reduce five of the features to just two principal components and compare them. The first principal component focuses on house size, and the second component focuses on cost. \nPrincipal component 1: Size\nThe first component accounts for the physical attributes of the home that includes square footage, bedrooms, and bathrooms.\nPrincipal component 2: Cost\nThe second component captures the financial factors that include price and tax rate.\nSlide: 52\nQuestion 1\nA baseball analyst wants to predict a player's batting average using a machine learning model. They have a dataset with over thirty different hitting statistics for each player. \nThe analyst wants to retain necessary information but reduce the number of features before training the model. Which feature selection technique should they use?\nResponse\nA\nFeature splitting\nB\nFeature scaling\nC\nPrincipal component analysis (PCA)\nD\nNo need for feature selection \nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember that PCA reduces features while retaining information. . \nSlide: 53\nQuestion 1 answer\nA baseball analyst wants to predict a player's batting average using a machine learning model. They have a dataset with over thirty different hitting statistics for each player. \nThe analyst wants to retain necessary information but reduce the number of features before training the model. Which feature selection technique should they use?\nResponse\nA\nFeature splitting\nB\nFeature scaling\nC correct\nPrincipal component analysis (PCA)\nD\nNo need for feature selection \nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nC: PCA reduces the dimensionality of the data while retaining most of the variation. This makes it useful for reducing a large set of variables down to a smaller subset of features before modeling.\nThe incorrect responses are:\nA: Feature splitting divides features into multiple datasets, which does not reduce dimensionality.\nB: Feature scaling standardizes features but does not reduce dimensionality.\nD: This example is a good candidate for using PCA. PCA reduces the dimensionality of the data while retaining most of the variation. This makes it useful for reducing a large set of variables down to a smaller subset of features before modeling.\nSlide: 54\nAWS data transformation services\nTopic A: Handling incorrect, duplicated, and missing data\nTopic B: Feature engineering concepts\nTopic C: Numeric and text feature engineering\nTopic D: Feature selection techniques\nTopic E: AWS data transformation services\nSpeaker Notes:\n|Student notes\nData ingestion is the process of collecting data from diverse sources, such as databases, data lakes, and streaming sources. The goal of data ingestion is to consolidate your data to use for your machine learning solution. \nIn this section, you will learn how you can use AWS services, such as Amazon SageMaker Data Wrangler, to ingest your data. You will also learn how you can use Amazon SageMaker Feature Store to ingest data and to create and manage features. \nYou will learn about data transformation use cases of these AWS services. You also learn fundamental steps of how you can use these services to automate part or all of the data transformation process.\nSlide: 55\nIngesting and transforming data for ML on AWS\nAmazon SageMaker Data Wrangler\nAmazon SageMaker Feature Store\nAmazon EMR\nAdd EMR notes from 1.2.13\nDo not add content but leave note here for T&C to revisit this slide since digital content is streaming-specific.\nSpeaker Notes:\n|Student notes\nYou can use AWS services, such as Amazon SageMaker Data Wrangler, to ingest your data. You can also use Amazon SageMaker Feature Store to ingest data and to create and manage features.\nAWS services like SageMaker Data Wrangler and SageMaker Feature Store streamline complicated data ingestion pipelines. Data Wrangler provides a visual, no-code way to transform and clean data from diverse sources. SageMaker Feature Store manages and shares curated features for machine learning model development. Using these services together eliminates much of the manual coding needed for ingesting and wrangling data. \nAWS services like Amazon EMR can help you to transform, enrich, and load data at scale. \nSlide: 56\nBenefits of Amazon SageMaker Data Wrangler\nAmazon SageMaker Data Wrangler\nVisual interface\nDrag-and-drop environment\nBuilt-in algorithms\nIntegration with data sources\nReusable data flows\nSpeaker Notes:\n|Student notes\nData Wrangler streamlines data ingestion through a visual interface to profile, understand, clean, and transform data using built-in recipes. It connects to sources, transforms, cleans, and loads data. Then, it outputs structured, model-ready data to AWS storage without writing code. SageMaker Data Wrangler streamlines complex data preparation so you can focus on building and training ML models, rather than preparing data. \nData Wrangler is a visual, code-free tool for data preprocessing and feature engineering that integrates with AWS data sources. It streamlines machine learning workflows through reusable data flows, large dataset handling, and integration with SageMaker.\nSlide: 57\nUse cases for SageMaker Data Wrangler\nClean data\nFeature engineering\nFix formatting issues\nReduce data size\nAutomate transformations\nSpeaker Notes:\n|Student notes\nData Wrangler streamlines exploratory data analysis and data preprocessing stages for ML learning in a visual, scalable, and collaborative environment. The following are some of the common use cases:\nClean data\n: You can use SageMaker Data Wrangler to interactively explore, visualize, clean, validate, and transform data to prepare it for ML model training. You can use the built-in transforms and visual interface to clean, standardize, and preprocess raw data from various sources like Amazon S3, databases, and more. \nFeature engineering\n: You can use the built-in transform and Python code blocks to engineer new features from your data. For example, you can use SageMaker Data Wrangler to combine columns and apply formulas.\nFixing formatting issues\n: SageMaker Data Wrangler can handle issues like missing headers and encoding problems to get the data into a consistent format for downstream use.\nReducing data size: \nFor large datasets, you can use SageMaker Data Wrangler to select subsets of columns, filter rows, sample data, and convert to lower precision data types. This helps to reduce storage and memory needs.\nAutomating transformations\n: After you interactively build transformation steps in SageMaker Data Wrangler, you can save them as a data recipe or flow to automate preparation for future datasets.\nSlide: 58\nUsing Amazon SageMaker Data Wrangler for data ingestion and transformation\nInput\nOutput\nAmazon \nSageMaker\n Data Wrangler\nIdentify data sources\nCreate a \nnew data flow\nPreview transformations\nOutput results\nSelect built-in transforms to transform data\nSpeaker Notes:\n|Student notes\nIdentify data sources\nUpload your dataset to an S3 bucket. SageMaker Data Wrangler supports CSV, JSON, Parquet, and other common data formats.\nCreate a new data flow\nOpen the SageMaker console and navigate to Data Wrangler. Choose Create flow to start a new data flow. Select the dataset you uploaded to Amazon S3 as the input source. SageMaker Data Wrangler will infer the schema and show a sample of the data. \nSelect built-in transforms to transform data\nIn the Transform section, use the built-in transforms to transform your data. You can see a sample output as you build the transforms. Some common transformations include the following: \nChanging column data types \nFiltering or sorting rows \nFilling in missing values \nParsing columns, like dates \nJoining datasets \nAggregating data, like sums or averages\nPreview transformations\nAutomate conversations, question answering, and workflows based on research into training honest and responsible AI systems.\nOutput results\nGenerate unique, realistic, high-quality images, art, logos, and designs.\nSlide: 59\nBenefits of Amazon SageMaker Feature Store\nAmazon SageMaker Feature Store\nStreamline\nTrack\nConfigure\nRetain\nSpeaker Notes:\n|Student notes\nFeature Store is a managed repository for storing, sharing, and managing features used for ML models. Features can be reused by multiple teams and SageMaker Feature Store keeps them synchronized for both batch training and real-time inference. By centralizing features in a secured store, SageMaker Feature Store improves feature quality and model accuracy at scale across the ML lifecycle. Storing features saves time by eliminating redundant feature engineering efforts.\nFeature Store provides centralized management of feature engineering to streamline machine learning workflows. It provides configurable online and offline storage of feature groups with metadata to enable discovery, reuse, and automation from data ingestion through model training and prediction. \nFeature Store provides a centralized and automated way to engineer, monitor, govern, and reuse features to improve model training and performance across teams. The following are some of the common use cases:\nAutomated data preprocessing: \nSageMaker Feature Store can automatically preprocess raw data as it is ingested. This includes scaling, encoding, transforming, and cleaning data so it is ready for model training.\nCentralized feature repository: \nSageMaker Feature Store acts as a centralized repository for curated features that have been preprocessed and engineered. Features can be reused across teams and models.\nStandardized features\n: SageMaker Feature Store establishes a standardized source of truth for featured data that is consistent across the organization. This avoids issues with different teams using their own feature engineering code.\nManagement of feature pipelines\n: SageMaker Feature Store manages the end-to-end lifecycle of feature pipelines from data ingestion through preprocessing. This provides monitoring and governance of feature engineering.\nCaching for performance\n: Frequently used features are cached in SageMaker Feature Store for faster retrieval during training. This improves performance by avoiding repetitive preprocessing.\nSlide: 60\nBenefits of Amazon EMR\nAmazon EMR\n<list key benefits>\nAdd EMR notes from 1.2.13\nDo not add content but leave note here for T&C to revisit this slide since digital content is streaming-specific.\nSpeaker Notes:\n|Student notes\nSlide: 61\nData formatting for model training in Amazon SageMaker\nSplit data sets.\nTraining\nTesting\nValidation\nAugment or shuffle training data for increased diversity.\nFormat data set.\nCSV\nRecordIO-protobuf\nSpeaker Notes:\n|Student notes\nReducing bias in your dataset is one of the primary objectives in data preparation. With balanced data, the model can focus on meaningful patterns in the dataset rather than any inherent noise.\nA common strategy to achieve a balanced and accurate dataset is to divide the data into distinct subsets for training, testing, and validation. \nData augmentation works by creating new, realistic training examples that expand the model's understanding of the data distribution. Dataset augmentation involves artificially expanding the size and diversity of a dataset by applying a variety of transformations and modifications to existing data samples. These transformations introduce variations in the data while preserving the essential characteristics relevant to the learning task\nData shuffling involves rearranging the order of the data points in a dataset before they are used for training a machine learning model. Shuffling your data helps to verify that the training examples are presented to the model in a random order rather than in a fixed sequence.\nWhen using Amazon SageMaker for your ML workloads, you want to make sure you are providing data that adheres to an accepted format. The two most common formats supported by the SageMaker built-in algorithms are CSV formatting and \nRecordIO-protobuf\n formatting. \nFormatting can be a complex task. You focus more on adhering to certain formatting types during the model training and refining stage of the machine learning lifecycle. However, it is helpful to know the basic concepts and structure of these two types of formats.\nCSV is a widely used text-based data format for training machine learning models. CSV files store tabular data, where each row represents a data sample, and the columns represent the features of that sample. For SageMaker built-in algorithms, the label in the training dataset must be the first column on the left, and the features should be to the right. Many built-in algorithms in SageMaker, such as \nXGBoost\n, linear learner, and \nDeepAR\n, accept CSV formatted data as input for model training. \nRecordIO-protobuf\n is a binary data format used by SageMaker for efficient data storage and transfer. This format is particularly useful when you need to handle large datasets or perform automatic transformations on the data. The \nRecordIO-protobuf\n format consists of a series of records, where each record contains a binary-encoded \nprotobuf\n message. This format is commonly used for image data, where each record represents an image and its associated metadata.\nSlide: 62\nQuestion 1\nYou work for a hospitality company that operates hotels across the country. You want to build a machine learning model to predict customer loyalty using data about past customer stays and purchases. You are considering using SageMaker Feature Store to prepare the data for model training. \nHow could SageMaker Feature Store be an effective service option for this use case?\nResponse\nA\nSageMaker Feature Store is a marketplace where you can pay workers to complete discrete tasks like labeling data. This would verify that your customers\u2019 purchases are accurate.\nB\nYou can use SageMaker Feature Store to visualize and clean your data. Clean data is essential for machine learning accuracy.\nC\nSageMaker Feature Store automatically labels your data. This saves time labeling data for supervised learning.\nD\nYou can use SageMaker Feature Store to store normalized features that can be reused for multiple models. This saves time preparing data for each new model.\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember that Feature Store serves as a centralized repository for features.\nSlide: 63\nQuestion 1 answer\nYou work for a hospitality company that operates hotels across the country. You want to build a machine learning model to predict customer loyalty using data about past customer stays and purchases. You are considering using SageMaker Feature Store to prepare the data for model training. \nHow could SageMaker Feature Store be an effective service option for this use case?\nResponse\nA\nSageMaker Feature Store is a marketplace where you can pay workers to complete discrete tasks like labeling data. This would verify that your customers\u2019 purchases are accurate.\nB\nYou can use SageMaker Feature Store to visualize and clean your data. Clean data is essential for machine learning accuracy.\nC\nSageMaker Feature Store automatically labels your data. This saves time labeling data for supervised learning.\nD correct\nYou can use SageMaker Feature Store to store normalized features that can be reused for multiple models. This saves time preparing data for each new model.\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nD: You can use SageMaker Feature Store to save time by storing curated features that can be accessed to train new models.\nThe incorrect responses are:\nA: Amazon Mechanical Turk is a service for hiring workers to complete tasks like labeling data. Feature Store helps manage and share features, not label data. You can use SageMaker Feature Store to store curated features that can be accessed to train new models.\nB: Although clean data is essential for ML accuracy, SageMaker Data Wrangler is a service that assists with data visualization and transformation. You can use SageMaker Feature Store to save time by storing curated features that can be accessed to train new models.\nC: SageMaker Ground Truth is a separate service for labeling data. SageMaker Feature Store helps manage and share features, not label data. You can use SageMaker Feature Store to save time by storing curated features that can be accessed to train new models.\nSlide: 64\nQuestion 2\nA telecommunications company wants to use Amazon SageMaker to train a machine learning model on their customer data. They have the data stored in different formats.\nWhich data formats do most Amazon SageMaker algorithms support for training? \nResponse\nA\nCSV and \nRecordIO-protobuf\nB\nAll data formats\nC\nNo data formats\nD\nCustom data formats\nChoice\nSpeaker Notes:\n|Instructor Notes\n| The intent of this question is to see if the students remember that SageMaker supports CSV and \nRecordIO-protobuf\n formats for training data.\nSlide: 65\nQuestion 2 answer\nA telecommunications company wants to use Amazon SageMaker to train a machine learning model on their customer data. They have the data stored in different formats.\nWhich data formats do most Amazon SageMaker algorithms support for training? \nResponse\nA correct\nCSV and \nRecordIO-protobuf\nB\nAll data formats\nC\nNo data formats\nD\nCustom data formats\nChoice\nSpeaker Notes:\n|Student notes\nThe correct response is:\nA: Many Amazon SageMaker algorithms support training with data in CSV and \nRecordIO-protobuf\n format.\nThe incorrect responses are:\nB: Many Amazon SageMaker algorithms support certain data formats.\nC: Many Amazon SageMaker algorithms support certain commonly used data formats. \nD: Many Amazon SageMaker algorithms support certain commonly used data formats. \nSlide: 66\nResources\nTo learn more, choose from the following resources:\nAmazon SageMaker AI \u2013CSV Format\n (developer guide)\nAmazon SageMaker AI \u2013RecordIO Format \n(developer guide)\nTime-Series Datasets Format and Missing Values Filling Methods\n (developer guide)\nCreate, Store, and Share Features with Amazon SageMaker Feature Store\n (developer guide)\nTransform Data\n (developer guide)\nSpeaker Notes:\nSlide: 67\nModule summary\nYou should now be able to:\nExplain the value of data cleaning and transformation.\nDescribe how to process incorrect or duplicated data.\nDescribe how to detect and treat outliers.\nDescribe how to process missing values.\nDescribe fundamental encoding techniques.\nIdentify feature engineering use cases.\nDescribe fundamental concepts, benefits, and techniques of feature engineering.\nDescribe fundamental feature selection techniques.\nDescribe AWS services for validating and labeling data.\nIdentify AWS tools and services for visualizing and transforming data.\nSpeaker Notes:\n|Student notes\nSlide: 68\nQuestions?\nThank you for attending\nthis session\nCorrections, feedback, or other questions? \nContact us at \nhttps://support.aws.amazon.com/#/contacts/aws-training\n. \nAll trademarks are the property of their owners.\nSpeaker Notes:\n|Instructor Notes\n|\n|Student Notes",
        "RESPONSE": "Here are the speaker notes for slides 1-10 of the presentation on Data Transformation and Feature Engineering:\n\nSlide 1: Module 3: Data Transformation and Feature Engineering\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide shows the title \"Module 3: Data Transformation and Feature Engineering\" for a Machine Learning Engineering on AWS course.\n~\n|INSTRUCTOR NOTES:\n- |Introduce the module on data transformation and feature engineering\n- |Highlight that this builds on previous modules on data collection and storage\n- |Explain that this module will cover key concepts and AWS tools for preparing data for ML\n|\n|STUDENT NOTES:\nThis module focuses on data transformation and feature engineering, which are critical steps in preparing data for machine learning. You'll learn about various transformation techniques like data cleaning, encoding, and feature engineering. The module will cover how to use AWS services such as Amazon SageMaker Feature Store, Amazon SageMaker Data Wrangler, and AWS Glue to transform your data.\n\nAt this point in the course, you should be familiar with data collection, ingestion, and storage fundamentals. You've learned about data types, formats, and core AWS storage services. This module builds on that knowledge to help you effectively prepare your data for ML model development.\n\nSlide 2: Module objectives & outline\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide lists the learning objectives and outline for Module 3 on data transformation and feature engineering.\n~\n|INSTRUCTOR NOTES:\n- |Review the module objectives with students\n- |Explain how the objectives tie into the overall course goals\n- |Highlight key topics that will be covered\n|\n|STUDENT NOTES:\nThis module aims to equip you with essential skills for data transformation and feature engineering. You'll learn to:\n\n- Explain the importance of data cleaning and transformation\n- Handle incorrect, duplicated, and missing data\n- Detect and treat outliers\n- Apply fundamental encoding techniques \n- Identify use cases for feature engineering\n- Understand key concepts and techniques in feature engineering\n- Apply feature selection methods\n- Use AWS services for data validation, labeling, visualization, and transformation\n\nThese skills are crucial for preparing high-quality datasets that can lead to more accurate and efficient machine learning models. The module will cover both theoretical concepts and practical applications using AWS tools.\n\nSlide 3: Data processing: Data transformation and feature engineering\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide shows a diagram of the machine learning lifecycle with \"Data processing\" highlighted, including data transformation and feature engineering.\n~\n|INSTRUCTOR NOTES:\n- |Explain where data transformation fits in the ML lifecycle\n- |Highlight the importance of this step for model accuracy\n- |Discuss how this builds on previous data preparation steps\n|\n|STUDENT NOTES:\nData transformation and feature engineering are critical steps in the machine learning lifecycle, occurring after data collection, ingestion, and storage. At this stage, you're preparing your data for model development by cleaning, encoding, and engineering features to create a more accurate and model-ready dataset.\n\nThis phase builds on the work you've done in earlier stages of data preparation. You're now taking the collected and stored data and transforming it into a format that will be most effective for training your machine learning model. The quality of your data transformation can significantly impact the accuracy and performance of your final model.\n\n*Data cleaning*, *categorical encoding*, and *feature engineering* are key techniques you'll use to ensure your dataset is optimized for model training. These processes help to remove noise, standardize data formats, and create more informative features that can improve model performance.\n\nSlide 4: Techniques for data transformation\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide outlines three main techniques for data transformation: data cleaning, categorical encoding, and feature engineering.\n~\n|INSTRUCTOR NOTES:\n- |Explain each technique and its importance\n- |Provide examples for each technique\n- |Discuss how these techniques work together in data preparation\n|\n|STUDENT NOTES:\nData transformation involves three key techniques: data cleaning, categorical encoding, and feature engineering. Each plays a crucial role in preparing your data for machine learning.\n\n*Data cleaning* is the first step in ensuring data quality. It involves identifying and handling errors, duplicates, and outliers in your dataset. For example, you might need to remove duplicate customer IDs or correct inconsistent date formats.\n\n*Categorical encoding* converts text-based categorical variables into numeric representations that ML algorithms can process. This is essential because most ML algorithms work with numerical data. For instance, you might encode country names as numbers or use one-hot encoding for non-ordinal categories.\n\n*Feature engineering* involves creating new features or modifying existing ones to help ML algorithms better understand the relationships in the data. This could include techniques like binning numeric data, creating interaction terms, or using domain knowledge to derive more informative features.\n\nThese techniques work together to transform raw data into a clean, encoded, and engineered dataset that is optimized for machine learning model training and prediction.\n\nSlide 5: Agenda\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide presents the agenda for the module, listing five main topics to be covered.\n~\n|INSTRUCTOR NOTES:\n- |Provide an overview of the module structure\n- |Explain how each topic builds on the previous one\n- |Set expectations for what will be covered in each section\n|\n|STUDENT NOTES:\nThis module is structured into five main topics:\n\n1. Handling incorrect, duplicated, and missing data\n2. Feature engineering concepts\n3. Numeric and text feature engineering\n4. Feature selection techniques\n5. AWS data transformation services\n\nWe'll start by exploring how to clean and prepare your data, addressing common issues like errors, duplicates, and missing values. Then, we'll dive into feature engineering concepts, learning how to create and modify features to improve model performance. \n\nWe'll look at specific techniques for both numeric and text data, as these often require different approaches. Feature selection will help you identify the most relevant features for your model. Finally, we'll explore AWS services that can assist with data transformation, making your workflow more efficient and scalable.\n\nSlide 6: Handling incorrect, duplicated, and missing data\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide highlights the first topic of the module: handling incorrect, duplicated, and missing data.\n~\n|INSTRUCTOR NOTES:\n- |Emphasize the importance of data quality\n- |Introduce common data quality issues\n- |Explain how these issues can impact ML model performance\n|\n|STUDENT NOTES:\nHandling incorrect, duplicated, and missing data is a crucial first step in data preparation for machine learning. Poor data quality can significantly impact model performance, leading to inaccurate predictions or biased results.\n\n*Incorrect data* can arise from various sources, such as data entry errors, sensor malfunctions, or inconsistent formatting. Identifying and correcting these errors is essential for maintaining data integrity.\n\n*Duplicated data* can skew your dataset, giving undue weight to certain data points. This can lead to overfitting or biased models. Identifying and removing duplicates helps ensure a balanced and representative dataset.\n\n*Missing data* is a common issue that can arise due to various reasons, such as incomplete data collection or data corruption. How you handle missing data can significantly impact your model's performance. Techniques for dealing with missing data include imputation, deletion, or using algorithms that can handle missing values.\n\nIn this section, we'll explore strategies for identifying and addressing these data quality issues to ensure your dataset is clean and reliable for model training.\n\nSlide 7: Cleaning data for ML training\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide outlines three main aspects of data cleaning: incorrect or duplicate data, data outliers, and incomplete or missing data.\n~\n|INSTRUCTOR NOTES:\n- |Explain the importance of data cleaning in ML\n- |Discuss each aspect of data cleaning\n- |Provide examples of how each issue can affect model performance\n|\n|STUDENT NOTES:\nData cleaning is a critical step in preparing your dataset for machine learning training. It involves addressing three main issues:\n\n1. *Incorrect or duplicate data*: This includes identifying and correcting errors in your data, as well as removing or consolidating duplicate entries. Standardizing your dataset ensures that the model focuses on meaningful patterns rather than noise or redundant information.\n\n2. *Data outliers*: Outliers are data points that significantly differ from other observations. While outliers can sometimes represent valuable information, they can also skew your model's performance. It's important to identify outliers and decide whether to remove them, transform them, or treat them separately.\n\n3. *Incomplete or missing data*: Missing data can occur due to various reasons, such as data collection errors or incomplete responses. Handling missing data appropriately is crucial, as it can significantly impact your model's accuracy.\n\nBy addressing these issues, you create a cleaner, more reliable dataset that allows your ML model to learn more effectively from the underlying patterns in the data.\n\nSlide 8: Incorrect data\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide presents examples of incorrect data and techniques for handling it.\n~\n|INSTRUCTOR NOTES:\n- |Discuss common sources of incorrect data\n- |Explain different techniques for handling incorrect data\n- |Provide examples of how to apply these techniques\n|\n|STUDENT NOTES:\nIncorrect data can significantly impact the quality of your machine learning models. Common sources of incorrect data include:\n\n- Language differences: Inconsistent use of language across data points\n- Typos: Errors made during data entry\n- Inconsistent formats: Data stored in different formats (e.g., dates as MM/DD/YYYY vs. DD/MM/YYYY)\n\nTo handle incorrect data, you can use several techniques:\n\n1. *Inspection*: Carefully review your dataset to identify anomalies or inconsistencies.\n2. *Filling correct values*: Replace incorrect values with the correct ones, if known.\n3. *Fix formatting*: Standardize data formats across your dataset.\n4. *Exclude from dataset*: In some cases, it may be necessary to remove incorrect data points if they can't be corrected.\n\nThe choice of technique depends on the nature of the incorrect data and its impact on your overall dataset. It's crucial to document any changes made during this process to maintain transparency and reproducibility in your ML pipeline.\n\nSlide 9: Duplicated data\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide explains duplicated data, its sources, and techniques for handling it.\n~\n|INSTRUCTOR NOTES:\n- |Explain what duplicated data is and why it's problematic\n- |Discuss common sources of duplicated data\n- |Introduce deduplication as a technique for handling duplicates\n|\n|STUDENT NOTES:\nDuplicated data refers to identical records appearing multiple times in a dataset. This can occur due to various reasons:\n\n- *Copies from multiple sources*: When data is collected from different sources, there might be overlaps leading to duplicates.\n- *Incorrect data entry*: Human error during data entry can result in the same information being entered multiple times.\n\nDuplicated data can skew your analysis and machine learning models by giving undue weight to certain data points. This can lead to biased results and poor model performance.\n\nThe primary technique for handling duplicated data is *deduplication*. This process involves:\n\n1. Identifying duplicate records in the dataset\n2. Retaining one copy of the duplicated information\n3. Removing or consolidating the other instances\n\nDeduplication can significantly improve the quality of your dataset and the efficiency of your storage. It ensures that each unique piece of information is represented only once, providing a more accurate basis for analysis and model training.\n\nSlide 10: Activity: Identifying incorrect and duplicated data\n\nSPEAKERNOTES:\n~Developer Notes:\n~Ref: https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html\n~\n~Image Description:\n~This slide presents a dataset for an activity on identifying incorrect and duplicated data.\n~\n|INSTRUCTOR NOTES:\n- |Guide students through the activity\n- |Encourage them to identify different types of data issues\n- |Discuss potential solutions for each issue identified\n|\n|STUDENT NOTES:\nIn this activity, you'll practice identifying incorrect and duplicated data in a sample dataset. As you review the data, look for:\n- Missing data\n- Inconsistent formatting\n- Misplaced information\n- Duplicate entries\n\nConsider how you would address each issue you identify. For example:\n- How would you handle missing values?\n- What steps would you take to standardize inconsistent formats?\n- How would you correct misplaced information?\n- What process would you use to remove duplicates?\n\nThis exercise will help you develop a critical eye for common data quality issues and start thinking about appropriate strategies for addressing them. Remember, the goal is to create a clean, consistent dataset that will lead to more accurate and reliable machine learning models.",
        "PARAMETERS": {
            "temp": 0.5,
            "topP": 1.0,
            "topK": 250,
            "maxT": 200000,
            "stopSeq": ""
        }
    }
]